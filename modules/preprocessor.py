import streamlit as st
import nltk
import re
from collections import Counter
import pandas as pd

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì•ˆì •í™” ë²„ì „)
@st.cache_resource
def download_nltk_data():
    try:
        import ssl
        try:
            _create_unverified_https_context = ssl._create_unverified_context
        except AttributeError:
            pass
        else:
            ssl._create_default_https_context = _create_unverified_https_context
        
        # í•„ìˆ˜ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt', quiet=True)
        
        try:
            nltk.data.find('tokenizers/punkt_tab')
        except LookupError:
            nltk.download('punkt_tab', quiet=True)
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords', quiet=True)
        
        try:
            nltk.data.find('taggers/averaged_perceptron_tagger')
        except LookupError:
            nltk.download('averaged_perceptron_tagger', quiet=True)
        
        # ğŸ“ ìƒˆë¡œ ì¶”ê°€! ğŸ“
        try:
            nltk.data.find('taggers/averaged_perceptron_tagger_eng')
        except LookupError:
            nltk.download('averaged_perceptron_tagger_eng', quiet=True)
        
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            nltk.download('wordnet', quiet=True)
        
        try:
            nltk.data.find('vader_lexicon')
        except LookupError:
            nltk.download('vader_lexicon', quiet=True)
            
        return True
    except Exception as e:
        st.error(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False

class TextPreprocessor:
    def __init__(self):
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        download_nltk_data()
        
        try:
            from nltk.corpus import stopwords
            self.stop_words = set(stopwords.words('english'))
        except:
            # ê¸°ë³¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
            self.stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}
    
    def extract_essay_content(self, text):
        """ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ (í‰ê°€ ê²°ê³¼ ë¶€ë¶„ ì œì™¸)"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text)
        
        # **EVALUATION RESULTS** ë¶€ë¶„ ì œê±°
        text = re.sub(r'\*\*EVALUATION RESULTS\*\*.*$', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'\*\*.*?\*\*', '', text)
        
        # ì ìˆ˜ íŒ¨í„´ ì œê±° (ì˜ˆ: "ì ìˆ˜: 85ì ")
        text = re.sub(r'ì ìˆ˜\s*:\s*\d+ì ?', '', text, flags=re.IGNORECASE)
        text = re.sub(r'score\s*:\s*\d+', '', text, flags=re.IGNORECASE)
        
        # ê¸°ë³¸ ì •ì œ
        text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸
        text = re.sub(r'\s+', ' ', text)     # ì—°ì† ê³µë°±
        
        return text.strip()
    
    def basic_cleaning(self, text):
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # **EVALUATION RESULTS** ê°™ì€ í‰ê°€ ê²°ê³¼ ë¶€ë¶„ ì œê±°
        text = re.sub(r'\*\*.*?\*\*', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€)
        text = re.sub(r'[^\w\s.,!?;:\'-]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def step1_basic_cleaning(self, text):
        """1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ (HTML, íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì •ë¦¬)"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # **EVALUATION RESULTS** ê°™ì€ í‰ê°€ ê²°ê³¼ ë¶€ë¶„ ì œê±°
        text = re.sub(r'\*\*.*?\*\*', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€)
        text = re.sub(r'[^\w\s.,!?;:\'-]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì–‘ì˜† ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def step2_remove_stopwords(self, text):
        """2ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°"""
        if not text:
            return ""
        
        try:
            words = nltk.word_tokenize(text.lower())
        except:
            words = text.lower().split()
        
        # ë¶ˆìš©ì–´ ì œê±° + ì§§ì€ ë‹¨ì–´ ì œê±°
        filtered_words = []
        for word in words:
            if (word.isalpha() and 
                len(word) > 2 and 
                word not in self.stop_words):
                filtered_words.append(word)
        
        return ' '.join(filtered_words)
    
    def step3_stemming(self, text):
        """3ë‹¨ê³„: ì–´ê°„ ì¶”ì¶œ (Stemming)"""
        if not text:
            return ""
        
        try:
            from nltk.stem import PorterStemmer
            stemmer = PorterStemmer()
            
            words = text.split()
            stemmed_words = [stemmer.stem(word) for word in words]
            return ' '.join(stemmed_words)
        except:
            # NLTKê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    
    def step4_lemmatization(self, text):
        """4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ (Lemmatization) - ë” ì •í™•í•¨"""
        if not text:
            return ""
        
        try:
            from nltk.stem import WordNetLemmatizer
            lemmatizer = WordNetLemmatizer()
            
            words = text.split()
            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
            return ' '.join(lemmatized_words)
        except:
            # NLTKê°€ ì—†ìœ¼ë©´ ì–´ê°„ ì¶”ì¶œë¡œ ëŒ€ì²´
            return self.step3_stemming(text)
    
    def get_preprocessing_steps(self, text):
        """ëª¨ë“  ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê²°ê³¼ ë°˜í™˜"""
        steps = {}
        
        # ì›ë³¸
        steps['ì›ë³¸'] = text
        steps['ì›ë³¸_ë‹¨ì–´ìˆ˜'] = len(text.split()) if text else 0
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ
        step1 = self.step1_basic_cleaning(text)
        steps['1ë‹¨ê³„_ê¸°ë³¸ì •ì œ'] = step1
        steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step1.split()) if step1 else 0
        
        # 2ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°
        step2 = self.step2_remove_stopwords(step1)
        steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'] = step2
        steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step2.split()) if step2 else 0
        
        # 3ë‹¨ê³„: ì–´ê°„ ì¶”ì¶œ
        step3 = self.step3_stemming(step2)
        steps['3ë‹¨ê³„_ì–´ê°„ì¶”ì¶œ'] = step3
        steps['3ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step3.split()) if step3 else 0
        
        # 4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ
        step4 = self.step4_lemmatization(step2)  # step2ì—ì„œ ë°”ë¡œ ì§„í–‰
        steps['4ë‹¨ê³„_í‘œì œì–´ì¶”ì¶œ'] = step4
        steps['4ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step4.split()) if step4 else 0
        
        return steps
    
    # ===========================================
    # ê¸°ë³¸ ê°ì„± ë¶„ì„ (3ê°€ì§€: ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
    # ===========================================
    
    def sentiment_analysis(self, text):
        """ê°œë³„ í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ê°ì„± ë¶„ì„"""
        if not text or pd.isna(text):
            return {'compound': 0, 'positive': 0, 'negative': 0, 'neutral': 0}
        
        try:
            from nltk.sentiment.vader import SentimentIntensityAnalyzer
            analyzer = SentimentIntensityAnalyzer()
            
            # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
            cleaned_text = self.extract_essay_content(text)
            if not cleaned_text:
                return {'compound': 0, 'positive': 0, 'negative': 0, 'neutral': 0}
            
            scores = analyzer.polarity_scores(cleaned_text)
            return {
                'compound': scores['compound'],  # ì „ì²´ ê°ì„± ì ìˆ˜ (-1~1)
                'positive': scores['pos'],       # ê¸ì • ë¹„ìœ¨
                'negative': scores['neg'],       # ë¶€ì • ë¹„ìœ¨  
                'neutral': scores['neu']         # ì¤‘ë¦½ ë¹„ìœ¨
            }
        except Exception as e:
            st.warning(f"ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'compound': 0, 'positive': 0, 'negative': 0, 'neutral': 0}

    def analyze_all_essays_sentiment(self, essay_data):
        """ëª¨ë“  ì—ì„¸ì´ì˜ ê¸°ë³¸ ê°ì„± ë¶„ì„"""
        results = []
        
        for idx, row in essay_data.iterrows():
            essay_text = row.get('essay_text', '')
            topic_name = row.get('topic_name', f'Essay {idx+1}')
            created_at = row.get('created_at', '')
            
            sentiment = self.sentiment_analysis(essay_text)
            
            # ê°ì„± ë ˆì´ë¸” ê²°ì •
            compound = sentiment['compound']
            if compound >= 0.05:
                sentiment_label = "ê¸ì •ì "
                emoji = "ğŸ˜Š"
            elif compound <= -0.05:
                sentiment_label = "ë¶€ì •ì " 
                emoji = "ğŸ˜Ÿ"
            else:
                sentiment_label = "ì¤‘ë¦½ì "
                emoji = "ğŸ˜"
            
            results.append({
                'essay_num': idx + 1,
                'topic_name': topic_name,
                'created_at': created_at,
                'sentiment_label': sentiment_label,
                'emotion_label': sentiment_label,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€
                'emoji': emoji,
                'compound': compound,
                'positive': sentiment['positive'],
                'negative': sentiment['negative'],
                'neutral': sentiment['neutral']
            })
        
        return results

    def get_sentiment_statistics(self, sentiment_results):
        """ê¸°ë³¸ ê°ì„± ë¶„ì„ í†µê³„"""
        if not sentiment_results:
            return {}
        
        total_essays = len(sentiment_results)
        positive_count = sum(1 for r in sentiment_results if r['sentiment_label'] == "ê¸ì •ì ")
        negative_count = sum(1 for r in sentiment_results if r['sentiment_label'] == "ë¶€ì •ì ")
        neutral_count = sum(1 for r in sentiment_results if r['sentiment_label'] == "ì¤‘ë¦½ì ")
        
        avg_compound = sum(r['compound'] for r in sentiment_results) / total_essays
        
        return {
            'total_essays': total_essays,
            'positive_count': positive_count,
            'negative_count': negative_count,
            'neutral_count': neutral_count,
            'positive_ratio': positive_count / total_essays * 100,
            'negative_ratio': negative_count / total_essays * 100,
            'neutral_ratio': neutral_count / total_essays * 100,
            'avg_compound': avg_compound
        }
    
    # ===========================================
    # í–¥ìƒëœ ê°ì„± ë¶„ì„ (8ê°€ì§€ ê°ì •)
    # ===========================================
    
    def enhanced_sentiment_analysis(self, text):
        """í–¥ìƒëœ ê°ì„± ë¶„ì„ - 8ê°€ì§€ ê°ì • ë¶„ë¥˜"""
        if not text or pd.isna(text):
            return {}
        
        try:
            from nltk.sentiment.vader import SentimentIntensityAnalyzer
            analyzer = SentimentIntensityAnalyzer()
            
            # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
            cleaned_text = self.extract_essay_content(text)
            if not cleaned_text:
                return {}
            
            # VADER ê°ì„± ì ìˆ˜
            vader_scores = analyzer.polarity_scores(cleaned_text)
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
            emotion_keywords = {
                'joy': ['happy', 'joy', 'excited', 'wonderful', 'amazing', 'great', 'excellent', 'fantastic', 'love', 'enjoy'],
                'anger': ['angry', 'mad', 'furious', 'annoyed', 'frustrated', 'hate', 'terrible', 'awful', 'worst', 'disgusting'],
                'sadness': ['sad', 'depressed', 'lonely', 'crying', 'disappointed', 'hurt', 'pain', 'sorrow', 'grief', 'unhappy'],
                'fear': ['afraid', 'scared', 'worried', 'anxious', 'nervous', 'terrified', 'panic', 'frightened', 'concerned', 'stress'],
                'surprise': ['surprised', 'shocked', 'amazed', 'astonished', 'unexpected', 'sudden', 'wow', 'incredible', 'unbelievable'],
                'disgust': ['disgusted', 'sick', 'revolting', 'gross', 'horrible', 'disgusting', 'repulsive', 'nasty'],
                'trust': ['trust', 'believe', 'reliable', 'confident', 'faith', 'honest', 'sincere', 'loyal', 'dependable'],
                'anticipation': ['hope', 'expect', 'anticipate', 'looking forward', 'eager', 'excited', 'optimistic', 'future']
            }
            
            # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
            lower_text = cleaned_text.lower()
            
            # ê° ê°ì •ë³„ í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
            emotion_scores = {}
            total_emotion_words = 0
            
            for emotion, keywords in emotion_keywords.items():
                count = sum(lower_text.count(keyword) for keyword in keywords)
                emotion_scores[emotion] = count
                total_emotion_words += count
            
            # ë¹„ìœ¨ ê³„ì‚°
            emotion_ratios = {}
            for emotion, count in emotion_scores.items():
                emotion_ratios[emotion] = (count / total_emotion_words * 100) if total_emotion_words > 0 else 0
            
            # ì£¼ìš” ê°ì • ê²°ì •
            if total_emotion_words > 0:
                primary_emotion = max(emotion_scores, key=emotion_scores.get)
                primary_emotion_score = emotion_scores[primary_emotion]
            else:
                primary_emotion = "neutral"
                primary_emotion_score = 0
            
            # VADERì™€ í‚¤ì›Œë“œ ë¶„ì„ ê²°í•©
            compound = vader_scores['compound']
            
            # ìµœì¢… ê°ì • ë¶„ë¥˜
            if primary_emotion_score > 2:  # ê°•í•œ ê°ì • í‚¤ì›Œë“œê°€ ë§ìœ¼ë©´
                final_emotion = primary_emotion
                confidence = "ë†’ìŒ"
            elif compound >= 0.1:
                final_emotion = "positive"
                confidence = "ì¤‘ê°„"
            elif compound <= -0.1:
                final_emotion = "negative"
                confidence = "ì¤‘ê°„"
            else:
                final_emotion = "neutral"
                confidence = "ë‚®ìŒ"
            
            return {
                'vader_scores': vader_scores,
                'emotion_scores': emotion_scores,
                'emotion_ratios': emotion_ratios,
                'primary_emotion': primary_emotion,
                'final_emotion': final_emotion,
                'confidence': confidence,
                'total_emotion_words': total_emotion_words
            }
            
        except Exception as e:
            st.warning(f"í–¥ìƒëœ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}

    def analyze_all_essays_enhanced_sentiment(self, essay_data):
        """ëª¨ë“  ì—ì„¸ì´ì˜ í–¥ìƒëœ ê°ì„± ë¶„ì„"""
        results = []
        
        for idx, row in essay_data.iterrows():
            essay_text = row.get('essay_text', '')
            topic_name = row.get('topic_name', f'Essay {idx+1}')
            created_at = row.get('created_at', '')
            
            sentiment = self.enhanced_sentiment_analysis(essay_text)
            
            if sentiment:
                # ê°ì„± ë¼ë²¨ê³¼ ì´ëª¨ì§€ ê²°ì •
                emotion_emojis = {
                    'joy': 'ğŸ˜Š', 'anger': 'ğŸ˜ ', 'sadness': 'ğŸ˜¢', 'fear': 'ğŸ˜°',
                    'surprise': 'ğŸ˜²', 'disgust': 'ğŸ¤¢', 'trust': 'ğŸ¤', 'anticipation': 'ğŸ¤—',
                    'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜Ÿ', 'neutral': 'ğŸ˜'
                }
                
                final_emotion = sentiment.get('final_emotion', 'neutral')
                emoji = emotion_emojis.get(final_emotion, 'ğŸ˜')
                
                # í•œêµ­ì–´ ê°ì •ëª…
                emotion_korean = {
                    'joy': 'ê¸°ì¨', 'anger': 'ë¶„ë…¸', 'sadness': 'ìŠ¬í””', 'fear': 'ë‘ë ¤ì›€',
                    'surprise': 'ë†€ë¼ì›€', 'disgust': 'í˜ì˜¤', 'trust': 'ì‹ ë¢°', 'anticipation': 'ê¸°ëŒ€',
                    'positive': 'ê¸ì •ì ', 'negative': 'ë¶€ì •ì ', 'neutral': 'ì¤‘ë¦½ì '
                }
                
                emotion_label = emotion_korean.get(final_emotion, 'ì¤‘ë¦½ì ')
                
                results.append({
                    'essay_num': idx + 1,
                    'topic_name': topic_name,
                    'created_at': created_at,
                    'emotion_label': emotion_label,
                    'emoji': emoji,
                    'final_emotion': final_emotion,
                    'confidence': sentiment.get('confidence', 'ë‚®ìŒ'),
                    'compound': sentiment['vader_scores']['compound'],
                    'emotion_scores': sentiment.get('emotion_scores', {}),
                    'primary_emotion': sentiment.get('primary_emotion', 'neutral')
                })
        
        return results

    def get_enhanced_sentiment_statistics(self, sentiment_results):
        """í–¥ìƒëœ ê°ì„± ë¶„ì„ í†µê³„"""
        if not sentiment_results:
            return {}
        
        total_essays = len(sentiment_results)
        
        # ê°ì •ë³„ ê°œìˆ˜ ê³„ì‚°
        emotion_counts = {}
        for result in sentiment_results:
            emotion = result['final_emotion']
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        # í‰ê·  compound ì ìˆ˜
        avg_compound = sum(r['compound'] for r in sentiment_results) / total_essays
        
        # ê°€ì¥ ë§ì€ ê°ì •
        most_common_emotion = max(emotion_counts, key=emotion_counts.get) if emotion_counts else 'neutral'
        
        return {
            'total_essays': total_essays,
            'emotion_counts': emotion_counts,
            'avg_compound': avg_compound,
            'most_common_emotion': most_common_emotion,
            'emotion_distribution': {k: v/total_essays*100 for k, v in emotion_counts.items()}
        }
    
    # ===========================================
    # í’ˆì‚¬ ë¶„ì„ (ì•ˆì •í™” ë²„ì „)
    # ===========================================
    
    def advanced_pos_analysis(self, text):
        """ê³ ê¸‰ í’ˆì‚¬ ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)"""
        if not text or pd.isna(text):
            return {}
        
        try:
            # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
            cleaned_text = self.extract_essay_content(text)
            if not cleaned_text:
                return {}
            
            # ê¸°ë³¸ í† í°í™” ì‹œë„
            try:
                sentences = nltk.sent_tokenize(cleaned_text)
            except:
                # NLTKê°€ ì‹¤íŒ¨í•˜ë©´ ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬
                sentences = [s.strip() + '.' for s in cleaned_text.split('.') if s.strip()]
            
            all_pos_tags = []
            pos_counts = {}
            
            for sentence in sentences:
                try:
                    # ë‹¨ì–´ í† í°í™” ë° í’ˆì‚¬ íƒœê¹…
                    words = nltk.word_tokenize(sentence)
                    pos_tags = nltk.pos_tag(words)
                except:
                    # NLTKê°€ ì‹¤íŒ¨í•˜ë©´ ê°„ë‹¨í•œ ë‹¨ì–´ ë¶„ë¦¬ ë° ê¸°ë³¸ íƒœê¹…
                    words = sentence.split()
                    pos_tags = [(word, 'NN' if word[0].isupper() else 'VB' if word.endswith('ed') else 'JJ' if word.endswith('ly') else 'NN') for word in words if word.isalpha()]
                
                all_pos_tags.extend(pos_tags)
                
                # í’ˆì‚¬ë³„ ì¹´ìš´íŠ¸
                for word, pos in pos_tags:
                    if pos not in pos_counts:
                        pos_counts[pos] = 0
                    pos_counts[pos] += 1
            
            # ì£¼ìš” í’ˆì‚¬ë³„ ë¶„ë¥˜
            noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']  # ëª…ì‚¬
            verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # ë™ì‚¬
            adjective_tags = ['JJ', 'JJR', 'JJS']  # í˜•ìš©ì‚¬
            adverb_tags = ['RB', 'RBR', 'RBS']  # ë¶€ì‚¬
            
            categorized = {
                'nouns': sum(pos_counts.get(tag, 0) for tag in noun_tags),
                'verbs': sum(pos_counts.get(tag, 0) for tag in verb_tags),
                'adjectives': sum(pos_counts.get(tag, 0) for tag in adjective_tags),
                'adverbs': sum(pos_counts.get(tag, 0) for tag in adverb_tags),
                'total_words': len(all_pos_tags)
            }
            
            # ë¹„ìœ¨ ê³„ì‚°
            total = categorized['total_words']
            if total > 0:
                categorized['noun_ratio'] = categorized['nouns'] / total * 100
                categorized['verb_ratio'] = categorized['verbs'] / total * 100
                categorized['adjective_ratio'] = categorized['adjectives'] / total * 100
                categorized['adverb_ratio'] = categorized['adverbs'] / total * 100
            else:
                categorized['noun_ratio'] = categorized['verb_ratio'] = categorized['adjective_ratio'] = categorized['adverb_ratio'] = 0
            
            return {
                'detailed_pos': pos_counts,
                'categorized': categorized,
                'pos_tags': all_pos_tags
            }
            
        except Exception as e:
            st.warning(f"í’ˆì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}

    def analyze_all_essays_pos(self, essay_data):
        """ëª¨ë“  ì—ì„¸ì´ì˜ í’ˆì‚¬ ë¶„ì„"""
        all_results = []
        
        for idx, row in essay_data.iterrows():
            essay_text = row.get('essay_text', '')
            topic_name = row.get('topic_name', f'Essay {idx+1}')
            
            pos_result = self.advanced_pos_analysis(essay_text)
            
            if pos_result and 'categorized' in pos_result:
                result = {
                    'essay_num': idx + 1,
                    'topic_name': topic_name,
                    **pos_result['categorized']
                }
                all_results.append(result)
        
        return all_results

    def get_pos_statistics(self, pos_results):
        """í’ˆì‚¬ ë¶„ì„ ì¢…í•© í†µê³„"""
        if not pos_results:
            return {}
        
        total_essays = len(pos_results)
        
        # í‰ê·  ê³„ì‚°
        avg_noun_ratio = sum(r['noun_ratio'] for r in pos_results) / total_essays
        avg_verb_ratio = sum(r['verb_ratio'] for r in pos_results) / total_essays
        avg_adjective_ratio = sum(r['adjective_ratio'] for r in pos_results) / total_essays
        avg_adverb_ratio = sum(r['adverb_ratio'] for r in pos_results) / total_essays
        
        return {
            'total_essays': total_essays,
            'avg_noun_ratio': avg_noun_ratio,
            'avg_verb_ratio': avg_verb_ratio,
            'avg_adjective_ratio': avg_adjective_ratio,
            'avg_adverb_ratio': avg_adverb_ratio
        }
    
    # ===========================================
    # ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„ (ì•ˆì „í•œ ë²„ì „)
    # ===========================================
    
    def analyze_sentence_complexity(self, text):
        """ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„ - ì•ˆì „í•œ ë²„ì „"""
        if not text or pd.isna(text):
            return {}
        
        try:
            # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
            cleaned_text = self.extract_essay_content(text)
            if not cleaned_text:
                return {}
            
            # ë¬¸ì¥ ë¶„ë¦¬ (ì•ˆì „í•œ ë°©ë²•)
            try:
                sentences = nltk.sent_tokenize(cleaned_text)
            except:
                sentences = [s.strip() + '.' for s in cleaned_text.split('.') if s.strip()]
            
            sentence_lengths = []
            clause_counts = []
            
            for sentence in sentences:
                # ë‹¨ì–´ í† í°í™” (ì•ˆì „í•œ ë°©ë²•)
                try:
                    words = nltk.word_tokenize(sentence)
                except:
                    words = sentence.split()
                
                sentence_lengths.append(len(words))
                
                # ê°„ë‹¨í•œ ì ˆ ê°œìˆ˜ ì¶”ì • (ì ‘ì†ì‚¬ ê°œìˆ˜ + 1)
                conjunctions = ['and', 'but', 'or', 'because', 'although', 'while', 'when', 'if', 'that', 'which']
                clause_count = 1 + sum(1 for word in words if word.lower() in conjunctions)
                clause_counts.append(clause_count)
            
            if sentence_lengths:
                return {
                    'total_sentences': len(sentences),
                    'avg_sentence_length': sum(sentence_lengths) / len(sentence_lengths),
                    'max_sentence_length': max(sentence_lengths),
                    'min_sentence_length': min(sentence_lengths),
                    'avg_clauses_per_sentence': sum(clause_counts) / len(clause_counts),
                    'complex_sentences': sum(1 for c in clause_counts if c > 2)  # 2ê°œ ì´ìƒ ì ˆ
                }
            else:
                return {}
                
        except Exception as e:
            st.warning(f"ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    # ===========================================
    # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ í•¨ìˆ˜ë“¤
    # ===========================================
    
    def tokenize_sentences(self, text):
        """ë¬¸ì¥ í† í°í™”"""
        try:
            sentences = nltk.sent_tokenize(text)
            return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        except:
            # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬
            sentences = text.split('.')
            return [s.strip() + '.' for s in sentences if s.strip() and len(s.strip()) > 10]
    
    def tokenize_words(self, text):
        """ë‹¨ì–´ í† í°í™”"""
        try:
            words = nltk.word_tokenize(text.lower())
        except:
            # ê°„ë‹¨í•œ ë‹¨ì–´ ë¶„ë¦¬
            words = re.findall(r'\b\w+\b', text.lower())
        
        # ë¶ˆìš©ì–´ ë° ì§§ì€ ë‹¨ì–´ ì œê±°
        words = [w for w in words if w.isalpha() and len(w) > 2 and w not in self.stop_words]
        return words
    
    # ===========================================
    # êµìœ¡ìš© ê°ì„± ë¶„ì„ ë©”ì„œë“œë“¤ (3ë‹¨ê³„ ë¹„êµ)
    # ===========================================

    def educational_sentiment_analysis_step1_lexicon(self, text):
        """1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„ (ì–´íœ˜ ì‚¬ì „ ë°©ì‹) - êµìœ¡ìš©"""
        if not text or pd.isna(text):
            return {}
        
        # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
        cleaned_text = self.extract_essay_content(text)
        if not cleaned_text:
            return {}
        
        # ê°„ë‹¨í•œ ê°ì„± ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
        positive_words = {
            'good': 2, 'great': 3, 'excellent': 3, 'amazing': 3, 'wonderful': 3,
            'fantastic': 3, 'awesome': 3, 'perfect': 3, 'outstanding': 3,
            'love': 2, 'like': 1, 'enjoy': 2, 'happy': 2, 'pleased': 2,
            'satisfied': 2, 'delighted': 3, 'thrilled': 3, 'excited': 2,
            'brilliant': 3, 'superb': 3, 'magnificent': 3, 'beautiful': 2,
            'nice': 1, 'fine': 1, 'well': 1, 'better': 1, 'best': 3,
            'positive': 2, 'successful': 2, 'effective': 2, 'efficient': 2,
            'helpful': 2, 'useful': 2, 'valuable': 2, 'important': 2
        }
        
        negative_words = {
            'bad': -2, 'terrible': -3, 'awful': -3, 'horrible': -3, 'disgusting': -3,
            'hate': -2, 'dislike': -1, 'angry': -2, 'sad': -2, 'disappointed': -2,
            'frustrated': -2, 'annoyed': -1, 'upset': -2, 'worried': -2,
            'concerned': -1, 'problem': -1, 'issue': -1, 'trouble': -2,
            'difficult': -1, 'hard': -1, 'poor': -2, 'weak': -1, 'fail': -2,
            'wrong': -1, 'mistake': -1, 'error': -1, 'worse': -2, 'worst': -3,
            'negative': -2, 'ineffective': -2, 'useless': -2, 'worthless': -3
        }
        
        # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë‹¨ì–´ ë¶„ë¦¬
        words = cleaned_text.lower().split()
        
        # ê° ë‹¨ì–´ì˜ ê°ì„± ì ìˆ˜ ê³„ì‚°
        word_scores = []
        positive_found = []
        negative_found = []
        total_score = 0
        
        for word in words:
            # êµ¬ë‘ì  ì œê±°
            clean_word = re.sub(r'[^\w]', '', word)
            
            if clean_word in positive_words:
                score = positive_words[clean_word]
                word_scores.append((clean_word, score))
                positive_found.append((clean_word, score))
                total_score += score
            elif clean_word in negative_words:
                score = negative_words[clean_word]
                word_scores.append((clean_word, score))
                negative_found.append((clean_word, score))
                total_score += score
        
        # ìµœì¢… ê°ì„± ê²°ì •
        if total_score > 0:
            sentiment = "ê¸ì •"
            emoji = "ğŸ˜Š"
        elif total_score < 0:
            sentiment = "ë¶€ì •"
            emoji = "ğŸ˜Ÿ"
        else:
            sentiment = "ì¤‘ë¦½"
            emoji = "ğŸ˜"
        
        return {
            'method': 'ê·œì¹™ ê¸°ë°˜ (ì–´íœ˜ ì‚¬ì „)',
            'total_score': total_score,
            'sentiment': sentiment,
            'emoji': emoji,
            'positive_words_found': positive_found,
            'negative_words_found': negative_found,
            'all_scored_words': word_scores,
            'explanation': f"ê¸ì • ë‹¨ì–´ë“¤ì˜ ì ìˆ˜ í•©: {sum(score for _, score in positive_found)}, "
                        f"ë¶€ì • ë‹¨ì–´ë“¤ì˜ ì ìˆ˜ í•©: {sum(score for _, score in negative_found)}, "
                        f"ìµœì¢… ì ìˆ˜: {total_score}"
        }

    def educational_sentiment_analysis_step2_tfidf(self, text, all_essays_text):
        """2ë‹¨ê³„: TF-IDF ê¸°ë°˜ ê°ì„± ë¶„ì„ - êµìœ¡ìš©"""
        if not text or pd.isna(text):
            return {}
        
        # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
        cleaned_text = self.extract_essay_content(text)
        if not cleaned_text:
            return {}
        
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.naive_bayes import MultinomialNB
            import numpy as np
            
            # ê°„ë‹¨í•œ í•™ìŠµ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ëŒ€ëŸ‰ì˜ ë°ì´í„°ê°€ í•„ìš”)
            training_texts = [
                "I love this amazing product. It's fantastic and wonderful.",  # ê¸ì •
                "This is terrible and awful. I hate it completely.",  # ë¶€ì •
                "The weather is nice today. I feel great and happy.",  # ê¸ì •
                "This is the worst experience ever. Very disappointing.",  # ë¶€ì •
                "It's okay. Not bad but not great either.",  # ì¤‘ë¦½
                cleaned_text  # í˜„ì¬ ë¶„ì„í•  í…ìŠ¤íŠ¸
            ]
            
            training_labels = [1, 0, 1, 0, 0.5, 0.5]  # 1=ê¸ì •, 0=ë¶€ì •, 0.5=ì¤‘ë¦½
            
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(training_texts)
            
            # í˜„ì¬ í…ìŠ¤íŠ¸ì˜ TF-IDF ë²¡í„°
            current_vector = tfidf_matrix[-1].toarray()[0]
            feature_names = vectorizer.get_feature_names_out()
            
            # ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ (ë†’ì€ TF-IDF ì ìˆ˜)
            important_words = []
            for i, score in enumerate(current_vector):
                if score > 0:
                    important_words.append((feature_names[i], score))
            
            important_words.sort(key=lambda x: x[1], reverse=True)
            top_words = important_words[:10]
            
            # ê°„ë‹¨í•œ ê°ì„± ì˜ˆì¸¡ (TF-IDF ì ìˆ˜ ê¸°ë°˜)
            # ê¸ì •/ë¶€ì • ë‹¨ì–´ì˜ TF-IDF ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
            positive_words = {
                'good', 'great', 'excellent', 'amazing', 'love', 'like', 'best', 'wonderful',
                'fantastic', 'awesome', 'perfect', 'outstanding', 'happy', 'pleased',
                'satisfied', 'delighted', 'thrilled', 'excited', 'brilliant', 'superb',
                'magnificent', 'beautiful', 'nice', 'fine', 'well', 'better', 'positive',
                'successful', 'effective', 'efficient', 'helpful', 'useful', 'valuable',
                'important', 'connect', 'friends', 'family', 'share', 'experiences',
                'discover', 'information', 'ways', 'offer', 'platforms', 'trends'
            }
            negative_words = {
                'bad', 'terrible', 'awful', 'hate', 'worst', 'horrible', 'disappointing',
                'disgusting', 'dislike', 'angry', 'sad', 'disappointed', 'frustrated',
                'annoyed', 'upset', 'worried', 'concerned', 'problem', 'issue', 'trouble',
                'difficult', 'hard', 'poor', 'weak', 'fail', 'wrong', 'mistake', 'error',
                'worse', 'negative', 'ineffective', 'useless', 'worthless', 'without',
                'imagine', 'disadvantages'
            }
            
            positive_score = sum(score for word, score in top_words if word in positive_words)
            negative_score = sum(score for word, score in top_words if word in negative_words)
            
            final_score = positive_score - negative_score
            
            # ì„ê³„ê°’ ì¡°ì • - ë” ë¯¼ê°í•˜ê²Œ
            if final_score > 0.05:  # 0.1 â†’ 0.05ë¡œ ë‚®ì¶¤
                sentiment = "ê¸ì •"
                emoji = "ğŸ˜Š"
            elif final_score < -0.05:  # -0.1 â†’ -0.05ë¡œ ì˜¬ë¦¼
                sentiment = "ë¶€ì •"
                emoji = "ğŸ˜Ÿ"
            else:
                sentiment = "ì¤‘ë¦½"
                emoji = "ğŸ˜"
            
            return {
                'method': 'TF-IDF + ë¨¸ì‹ ëŸ¬ë‹',
                'final_score': final_score,
                'sentiment': sentiment,
                'emoji': emoji,
                'top_tfidf_words': top_words,
                'positive_score': positive_score,
                'negative_score': negative_score,
                'explanation': f"TF-IDFë¡œ ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì„ ì°¾ê³ , ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ê°ì„± ë¶„ì„. "
                            f"ê¸ì • ê°€ì¤‘ì¹˜: {positive_score:.3f}, ë¶€ì • ê°€ì¤‘ì¹˜: {negative_score:.3f}"
            }
            
        except ImportError:
            return {
                'method': 'TF-IDF + ë¨¸ì‹ ëŸ¬ë‹',
                'error': 'scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install scikit-learn ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.',
                'sentiment': 'ë¶„ì„ ë¶ˆê°€',
                'emoji': 'â“'
            }
        except Exception as e:
            return {
                'method': 'TF-IDF + ë¨¸ì‹ ëŸ¬ë‹',
                'error': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'sentiment': 'ë¶„ì„ ë¶ˆê°€',
                'emoji': 'â“'
            }

    def educational_sentiment_analysis_step3_vader(self, text):
        """3ë‹¨ê³„: VADER ê³ ê¸‰ ê°ì„± ë¶„ì„ - êµìœ¡ìš©"""
        if not text or pd.isna(text):
            return {}
        
        try:
            from nltk.sentiment.vader import SentimentIntensityAnalyzer
            
            # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
            cleaned_text = self.extract_essay_content(text)
            if not cleaned_text:
                return {}
            
            analyzer = SentimentIntensityAnalyzer()
            scores = analyzer.polarity_scores(cleaned_text)
            
            # VADERì˜ íŠ¹ë³„í•œ ê¸°ëŠ¥ë“¤ ì„¤ëª…
            compound = scores['compound']
            
            if compound >= 0.05:
                sentiment = "ê¸ì •"
                emoji = "ğŸ˜Š"
            elif compound <= -0.05:
                sentiment = "ë¶€ì •"
                emoji = "ğŸ˜Ÿ"
            else:
                sentiment = "ì¤‘ë¦½"
                emoji = "ğŸ˜"
            
            # ë¬¸ì¥ë³„ ë¶„ì„ (êµìœ¡ì  ëª©ì )
            sentences = cleaned_text.split('.')
            sentence_analysis = []
            
            for i, sentence in enumerate(sentences[:5]):  # ì²˜ìŒ 5ë¬¸ì¥ë§Œ
                if sentence.strip():
                    sent_scores = analyzer.polarity_scores(sentence.strip())
                    sentence_analysis.append({
                        'sentence': sentence.strip()[:100] + "..." if len(sentence.strip()) > 100 else sentence.strip(),
                        'compound': sent_scores['compound'],
                        'positive': sent_scores['pos'],
                        'negative': sent_scores['neg'],
                        'neutral': sent_scores['neu']
                    })
            
            return {
                'method': 'VADER (ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜)',
                'compound': compound,
                'sentiment': sentiment,
                'emoji': emoji,
                'detailed_scores': {
                    'positive': scores['pos'],
                    'negative': scores['neg'],
                    'neutral': scores['neu']
                },
                'sentence_analysis': sentence_analysis,
                'explanation': f"VADERëŠ” ë¬¸ë§¥, ê°•ì¡°, ë¶€ì • ë“±ì„ ê³ ë ¤í•œ ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜ ë°©ë²•ì…ë‹ˆë‹¤. "
                            f"Compound ì ìˆ˜: {compound:.3f} "
                            f"(1ì— ê°€ê¹Œìš°ë©´ ê¸ì •, -1ì— ê°€ê¹Œìš°ë©´ ë¶€ì •)"
            }
            
        except Exception as e:
            return {
                'method': 'VADER (ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜)',
                'error': f'VADER ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'sentiment': 'ë¶„ì„ ë¶ˆê°€',
                'emoji': 'â“'
            }

    def educational_sentiment_comparison(self, essay_data, essay_index=0):
        """ì„¸ ê°€ì§€ ë°©ë²• ë¹„êµ ë¶„ì„ - êµìœ¡ìš©"""
        if essay_data.empty or essay_index >= len(essay_data):
            return {}
        
        # ì„ íƒëœ ì—ì„¸ì´
        selected_essay = essay_data.iloc[essay_index]
        essay_text = selected_essay.get('essay_text', '')
        
        if not essay_text:
            return {}
        
        # ì „ì²´ ì—ì„¸ì´ í…ìŠ¤íŠ¸ (TF-IDFìš©)
        all_essays_text = []
        for _, row in essay_data.iterrows():
            text = row.get('essay_text', '')
            if text:
                all_essays_text.append(self.extract_essay_content(text))
        
        # ì„¸ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë¶„ì„
        result1 = self.educational_sentiment_analysis_step1_lexicon(essay_text)
        result2 = self.educational_sentiment_analysis_step2_tfidf(essay_text, all_essays_text)
        result3 = self.educational_sentiment_analysis_step3_vader(essay_text)
        
        return {
            'essay_info': {
                'essay_num': essay_index + 1,
                'topic': selected_essay.get('topic_name', 'Unknown'),
                'text_preview': self.extract_essay_content(essay_text)[:200] + "..."
            },
            'method1_lexicon': result1,
            'method2_tfidf': result2,
            'method3_vader': result3
        }
    
    # ===========================================
    # êµìœ¡ìš© í’ˆì‚¬ ë¶„ì„ ë©”ì„œë“œë“¤ (3ë‹¨ê³„ ë¹„êµ)
    # ===========================================
    
    def educational_pos_analysis_step1_manual_rules(self, text):
        """1ë‹¨ê³„: ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜ í’ˆì‚¬ íƒœê¹… - êµìœ¡ìš© (ì™„ì „ ê°œì„ ëœ ë²„ì „)"""
        if not text or pd.isna(text):
            return {}
        
        # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
        cleaned_text = self.extract_essay_content(text)
        if not cleaned_text:
            return {}
        
        # ëŒ€í­ í™•ì¥ëœ ê·œì¹™ ê¸°ë°˜ í’ˆì‚¬ ë¶„ë¥˜
        manual_rules = {
            # ëª…ì‚¬ íŒ¨í„´ (1000ê°œ ì´ìƒ)
            'noun_patterns': {
                'endings': ['tion', 'sion', 'ness', 'ment', 'ship', 'hood', 'ity', 'acy', 'ism', 'er', 'or', 'ar', 'ist', 'ant', 'ent', 'ure', 'age', 'ence', 'ance', 'ette', 'dom', 'ty', 'cy'],
                'common_nouns': {
                    # ê¸°ë³¸ ëª…ì‚¬ë“¤
                    'people', 'person', 'time', 'way', 'day', 'man', 'thing', 'woman', 'life', 'child', 'world', 'school', 'state', 'family', 'student', 'group', 'country', 'problem', 'hand', 'part', 'place', 'case', 'week', 'company', 'system', 'program', 'question', 'work', 'government', 'number', 'night', 'point', 'home', 'water', 'room', 'mother', 'area', 'money', 'story', 'fact', 'month', 'lot', 'right', 'study', 'book', 'eye', 'job', 'word', 'business', 'issue', 'side', 'kind', 'head', 'house', 'service', 'friend', 'father', 'power', 'hour', 'game', 'line', 'end', 'member', 'law', 'car', 'city', 'community', 'name', 'president', 'team', 'minute', 'idea', 'kid', 'body', 'information', 'back', 'parent', 'face', 'others', 'level', 'office', 'door', 'health', 'art', 'war', 'history', 'party', 'result', 'change', 'morning', 'reason', 'research', 'girl', 'guy', 'moment', 'air', 'teacher', 'force', 'education', 'food', 'technology', 'media', 'social', 'apps', 'platform', 'experience', 'trend', 'news', 'truth', 'misinformation',
                    
                    # í•™êµ/êµìœ¡ ê´€ë ¨
                    'class', 'lesson', 'homework', 'exam', 'test', 'grade', 'subject', 'course', 'university', 'college', 'library', 'classroom', 'textbook', 'knowledge', 'learning', 'skill', 'talent', 'ability', 'intelligence', 'wisdom', 'understanding', 'concept', 'theory', 'practice', 'method', 'technique', 'strategy', 'approach', 'solution', 'answer', 'explanation', 'definition', 'example', 'exercise', 'assignment', 'project', 'analysis', 'investigation', 'experiment', 'observation', 'discovery', 'invention', 'innovation', 'development', 'progress', 'improvement', 'achievement', 'success', 'failure', 'mistake', 'error', 'correction',
                    
                    # ê¸°ìˆ /ì¸í„°ë„· ê´€ë ¨
                    'internet', 'website', 'computer', 'phone', 'smartphone', 'tablet', 'laptop', 'device', 'screen', 'keyboard', 'mouse', 'software', 'hardware', 'application', 'app', 'program', 'code', 'data', 'database', 'server', 'network', 'wifi', 'bluetooth', 'email', 'message', 'text', 'photo', 'video', 'image', 'file', 'document', 'folder', 'password', 'account', 'profile', 'user', 'username', 'login', 'download', 'upload', 'search', 'result', 'link', 'url', 'browser', 'chrome', 'firefox', 'safari', 'google', 'youtube', 'facebook', 'instagram', 'twitter', 'tiktok', 'snapchat', 'whatsapp', 'zoom', 'skype', 'discord', 'reddit', 'blog', 'post', 'comment', 'like', 'share', 'follow', 'subscriber', 'influencer', 'content', 'creator', 'channel', 'stream', 'podcast', 'gaming', 'game', 'player', 'character', 'level', 'score', 'achievement', 'ranking', 'competition', 'tournament', 'prize', 'reward',
                    
                    # ì¼ìƒìƒí™œ ê´€ë ¨
                    'morning', 'afternoon', 'evening', 'night', 'today', 'yesterday', 'tomorrow', 'weekend', 'holiday', 'vacation', 'trip', 'journey', 'travel', 'destination', 'hotel', 'restaurant', 'cafe', 'shop', 'store', 'market', 'mall', 'cinema', 'theater', 'museum', 'park', 'beach', 'mountain', 'forest', 'river', 'lake', 'ocean', 'sea', 'island', 'bridge', 'road', 'street', 'highway', 'traffic', 'bus', 'train', 'plane', 'airport', 'station', 'ticket', 'passport', 'luggage', 'bag', 'clothes', 'shirt', 'pants', 'dress', 'shoes', 'hat', 'jacket', 'coat', 'jewelry', 'watch', 'ring', 'necklace', 'earring', 'makeup', 'perfume', 'shampoo', 'soap', 'toothbrush', 'toothpaste', 'towel', 'bed', 'pillow', 'blanket', 'mirror', 'lamp', 'chair', 'table', 'desk', 'sofa', 'television', 'radio', 'clock', 'calendar', 'magazine', 'newspaper', 'novel', 'story', 'poem', 'song', 'music', 'instrument', 'piano', 'guitar', 'violin', 'drum', 'band', 'concert', 'performance', 'show', 'movie', 'film', 'actor', 'actress', 'director', 'producer', 'script', 'scene', 'camera', 'microphone', 'stage', 'audience', 'fan', 'celebrity', 'star', 'fame', 'reputation', 'popularity', 'career', 'profession', 'occupation', 'salary', 'income', 'expense', 'budget', 'savings', 'investment', 'bank', 'credit', 'debt', 'loan', 'insurance', 'tax', 'bill', 'receipt', 'purchase', 'sale', 'discount', 'price', 'cost', 'value', 'quality', 'quantity', 'size', 'weight', 'height', 'width', 'length', 'distance', 'speed', 'space', 'location', 'position', 'direction', 'north', 'south', 'east', 'west', 'left', 'right', 'front', 'back', 'top', 'bottom', 'inside', 'outside', 'center', 'corner', 'edge', 'surface', 'ground', 'floor', 'ceiling', 'wall', 'window', 'gate', 'entrance', 'exit', 'path', 'route', 'adventure', 'memory', 'dream', 'hope', 'wish', 'goal', 'plan', 'decision', 'choice', 'option', 'opportunity', 'chance', 'possibility', 'probability', 'risk', 'danger', 'safety', 'security', 'protection', 'defense', 'attack', 'peace', 'conflict', 'agreement', 'contract', 'promise', 'trust', 'faith', 'belief', 'religion', 'god', 'prayer', 'church', 'temple', 'mosque', 'ceremony', 'wedding', 'birthday', 'anniversary', 'celebration', 'festival', 'gift', 'present', 'surprise', 'happiness', 'joy', 'pleasure', 'fun', 'excitement', 'enthusiasm', 'passion', 'love', 'affection', 'friendship', 'relationship', 'marriage', 'divorce', 'partner', 'spouse', 'husband', 'wife', 'boyfriend', 'girlfriend', 'couple', 'date', 'kiss', 'hug', 'smile', 'laugh', 'tear', 'cry', 'sadness', 'depression', 'anger', 'rage', 'frustration', 'stress', 'anxiety', 'worry', 'fear', 'terror', 'horror', 'shock', 'confusion', 'doubt', 'curiosity', 'interest', 'attention', 'focus', 'concentration', 'thought', 'mind', 'brain', 'heart', 'soul', 'spirit', 'emotion', 'feeling', 'sense', 'touch', 'taste', 'smell', 'sight', 'sound', 'voice', 'noise', 'silence', 'music', 'rhythm', 'beat', 'melody', 'harmony', 'tone', 'volume', 'echo', 'whisper', 'shout', 'scream', 'breath', 'wind', 'breeze', 'storm', 'rain', 'snow', 'ice', 'fire', 'flame', 'smoke', 'ash', 'dust', 'dirt', 'mud', 'sand', 'rock', 'stone', 'metal', 'gold', 'silver', 'copper', 'iron', 'steel', 'plastic', 'glass', 'wood', 'paper', 'cloth', 'fabric', 'leather', 'rubber', 'oil', 'gas', 'fuel', 'energy', 'electricity', 'battery', 'cable', 'wire', 'button', 'switch', 'remote', 'control', 'machine', 'engine', 'motor', 'wheel', 'tire', 'brake', 'gear', 'tool', 'hammer', 'screwdriver', 'knife', 'scissors', 'pen', 'pencil', 'eraser', 'ruler', 'calculator', 'compass', 'map', 'globe', 'atlas', 'dictionary', 'encyclopedia', 'manual', 'guide', 'instruction', 'recipe', 'ingredient', 'cooking', 'kitchen', 'stove', 'oven', 'microwave', 'refrigerator', 'freezer', 'dishwasher', 'sink', 'faucet', 'plate', 'bowl', 'cup', 'mug', 'glass', 'bottle', 'can', 'jar', 'box', 'package', 'container', 'basket', 'cart', 'truck', 'van', 'motorcycle', 'bicycle', 'boat', 'ship', 'helicopter', 'rocket', 'satellite', 'planet', 'star', 'moon', 'sun', 'earth', 'sky', 'cloud', 'rainbow', 'lightning', 'thunder', 'earthquake', 'volcano', 'desert', 'jungle', 'valley', 'hill', 'cliff', 'cave', 'tunnel', 'building', 'apartment', 'factory', 'warehouse', 'garage', 'basement', 'attic', 'balcony', 'garden', 'yard', 'fence', 'pool', 'gym', 'playground', 'field', 'court', 'stadium', 'arena', 'track', 'race', 'marathon', 'sport', 'football', 'basketball', 'baseball', 'tennis', 'golf', 'swimming', 'running', 'cycling', 'hiking', 'climbing', 'skiing', 'surfing', 'dancing', 'singing', 'acting', 'drawing', 'painting', 'writing', 'reading', 'studying', 'teaching', 'learning', 'training', 'exercise', 'workout', 'fitness', 'health', 'medicine', 'doctor', 'nurse', 'hospital', 'clinic', 'pharmacy', 'drug', 'pill', 'tablet', 'injection', 'surgery', 'operation', 'treatment', 'therapy', 'recovery', 'healing', 'cure', 'disease', 'illness', 'sickness', 'injury', 'wound', 'pain', 'headache', 'fever', 'cold', 'flu', 'cough', 'sneeze', 'allergy', 'infection', 'virus', 'bacteria', 'cancer', 'diabetes', 'insomnia', 'fatigue', 'weakness', 'strength', 'muscle', 'bone', 'blood', 'skin', 'hair', 'nail', 'tooth', 'tongue', 'lip', 'nose', 'ear', 'eyebrow', 'eyelash', 'cheek', 'chin', 'forehead', 'neck', 'shoulder', 'arm', 'elbow', 'wrist', 'finger', 'thumb', 'palm', 'chest', 'stomach', 'waist', 'hip', 'leg', 'knee', 'ankle', 'foot', 'toe', 'heel'
                }
            },
            
            # ë™ì‚¬ íŒ¨í„´ (800ê°œ ì´ìƒ)
            'verb_patterns': {
                'endings': ['ed', 'ing', 's', 'es', 'en', 'ize', 'ise', 'fy', 'ate'],
                'common_verbs': {
                    # ê¸°ë³¸ ë™ì‚¬ë“¤
                    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'get', 'go', 'make', 'see', 'know', 'take', 'think', 'come', 'give', 'use', 'find', 'want', 'tell', 'ask', 'work', 'seem', 'feel', 'try', 'leave', 'call', 'need', 'become', 'show', 'move', 'live', 'believe', 'bring', 'happen', 'write', 'provide', 'sit', 'stand', 'lose', 'pay', 'meet', 'include', 'continue', 'set', 'learn', 'change', 'lead', 'understand', 'watch', 'follow', 'stop', 'create', 'speak', 'read', 'allow', 'add', 'spend', 'grow', 'open', 'walk', 'win', 'offer', 'remember', 'love', 'consider', 'appear', 'buy', 'wait', 'serve', 'die', 'send', 'expect', 'build', 'stay', 'fall', 'cut', 'reach', 'kill', 'remain', 'suggest', 'raise', 'pass', 'sell', 'require', 'report', 'decide', 'pull', 'return', 'explain', 'hope', 'develop', 'carry', 'break', 'receive', 'agree', 'support', 'hit', 'produce', 'eat', 'cover', 'catch', 'draw', 'choose', 'cause', 'point', 'push', 'run', 'imagine', 'connect', 'share', 'discover', 'encounter', 'empower', 'discern',
                    
                    # í–‰ë™ ë™ì‚¬ë“¤
                    'walk', 'run', 'jump', 'swim', 'play', 'dance', 'sing', 'laugh', 'cry', 'sleep', 'wake', 'eat', 'drink', 'cook', 'clean', 'wash', 'drive', 'ride', 'travel', 'visit', 'explore', 'climb', 'exercise', 'rest', 'study', 'teach', 'practice', 'train', 'compete', 'fight', 'argue', 'discuss', 'chat', 'whisper', 'shout', 'listen', 'hear', 'watch', 'observe', 'notice', 'search', 'hide', 'seek', 'touch', 'feel', 'hug', 'kiss', 'wave', 'point', 'grab', 'catch', 'throw', 'kick', 'punch', 'lift', 'carry', 'hold', 'drop', 'place', 'organize', 'arrange', 'plan', 'prepare', 'design', 'build', 'repair', 'fix', 'break', 'destroy', 'paint', 'draw', 'type', 'print', 'send', 'receive', 'buy', 'sell', 'spend', 'save', 'invest', 'collect', 'gather', 'share', 'distribute', 'help', 'assist', 'protect', 'defend', 'attack', 'escape', 'rescue', 'heal', 'cure', 'treat', 'care', 'maintain', 'preserve', 'store', 'record', 'document', 'publish', 'broadcast', 'communicate', 'inform', 'announce', 'declare', 'prove', 'confirm', 'verify', 'approve', 'accept', 'agree', 'refuse', 'reject', 'deny', 'permit', 'allow', 'forbid', 'prevent', 'encourage', 'motivate', 'inspire', 'influence', 'persuade', 'force', 'require', 'demand', 'request', 'order', 'command', 'guide', 'lead', 'follow', 'accompany', 'supervise', 'monitor', 'control', 'manage', 'operate', 'function', 'perform', 'execute', 'implement', 'apply', 'adapt', 'adjust', 'modify', 'transform', 'convert', 'improve', 'develop', 'expand', 'increase', 'decrease', 'reduce', 'raise', 'lower', 'approach', 'arrive', 'reach', 'enter', 'exit', 'return', 'stay', 'remain', 'continue', 'proceed', 'advance', 'progress', 'succeed', 'fail', 'struggle', 'endure', 'survive', 'exist', 'born', 'marry', 'divorce', 'graduate', 'retire', 'celebrate', 'enjoy', 'suffer', 'worry', 'fear', 'hope', 'wish', 'dream', 'imagine', 'remember', 'forget', 'recognize', 'identify', 'compare', 'contrast', 'relate', 'connect', 'combine', 'separate', 'divide', 'join', 'unite', 'integrate', 'include', 'exclude', 'involve', 'participate', 'engage', 'interact', 'cooperate', 'collaborate', 'coordinate', 'schedule', 'focus', 'concentrate', 'emphasize', 'highlight', 'classify', 'categorize', 'sort', 'rank', 'rate', 'evaluate', 'assess', 'judge', 'criticize', 'praise', 'appreciate', 'value', 'respect', 'honor', 'admire', 'envy', 'hate', 'dislike', 'prefer', 'choose', 'select', 'decide', 'determine', 'resolve', 'solve', 'invent', 'investigate', 'examine', 'inspect', 'check', 'test', 'attempt', 'experiment', 'measure', 'weigh', 'count', 'calculate', 'estimate', 'predict', 'forecast', 'guess', 'assume', 'suppose', 'doubt', 'question', 'wonder', 'comprehend', 'realize', 'interpret', 'describe', 'define', 'illustrate', 'demonstrate', 'represent', 'symbolize', 'mean', 'imply', 'suggest', 'indicate', 'refer', 'mention', 'cite', 'quote', 'repeat', 'copy', 'imitate', 'duplicate', 'reproduce', 'recreate', 'restore', 'renew', 'refresh', 'revive', 'recover', 'bounce', 'spring', 'leap', 'hop', 'skip', 'march', 'stroll', 'wander', 'drift', 'float', 'slide', 'slip', 'stumble', 'trip', 'rise', 'climb', 'descend', 'roll', 'spin', 'rotate', 'circle', 'surround', 'contain', 'squeeze', 'press', 'push', 'pull', 'drag', 'shift', 'transfer', 'transport', 'deliver', 'ship', 'mail', 'obtain', 'acquire', 'gain', 'earn', 'lose', 'miss', 'lack', 'desire', 'cost', 'charge', 'afford', 'owe', 'lend', 'borrow', 'steal', 'rob', 'cheat', 'lie', 'deceive', 'confuse', 'surprise', 'shock', 'frighten', 'scare', 'calm', 'comfort', 'relax', 'stress', 'concern', 'bother', 'disturb', 'interrupt', 'annoy', 'irritate', 'anger', 'upset', 'hurt', 'harm', 'damage', 'injure', 'wound', 'recover', 'worsen', 'deteriorate', 'decay', 'spoil', 'ruin', 'waste', 'conserve', 'donate', 'contribute', 'volunteer', 'supply', 'obtain', 'possess', 'own', 'belong', 'rent', 'lease', 'hire', 'employ', 'labor', 'toil', 'effort', 'rehearse', 'drill', 'stretch', 'strengthen', 'weaken', 'tire', 'exhaust', 'drain', 'energize', 'refresh', 'revitalize', 'stimulate', 'excite', 'thrill', 'amaze', 'astonish', 'impress', 'disappoint', 'satisfy', 'please', 'delight', 'entertain', 'amuse', 'bore', 'interest', 'fascinate', 'attract', 'repel', 'disgust', 'revolt', 'offend', 'insult', 'compliment', 'flatter', 'blame', 'accuse', 'charge', 'prosecute', 'defend', 'justify', 'excuse', 'forgive', 'pardon', 'apologize', 'regret', 'mourn', 'grieve', 'congratulate', 'thank', 'acknowledge', 'reward', 'punish', 'discipline', 'scold', 'warn', 'threaten', 'promise', 'guarantee', 'assure', 'convince', 'persuade', 'affect', 'impact', 'concern', 'matter', 'count', 'balance', 'tip', 'lean', 'bend', 'twist', 'turn', 'flip', 'reverse', 'invert'
                }
            },
            
            # í˜•ìš©ì‚¬ íŒ¨í„´ (600ê°œ ì´ìƒ)
            'adjective_patterns': {
                'endings': ['ful', 'less', 'ive', 'able', 'ible', 'ous', 'ious', 'eous', 'al', 'ic', 'ical', 'ant', 'ent', 'ing', 'ed', 'ly', 'y', 'ish', 'like', 'ward', 'wise'],
                'common_adjectives': {
                    'good', 'bad', 'great', 'small', 'big', 'large', 'little', 'new', 'old', 'young', 'long', 'short', 'high', 'low', 'right', 'wrong', 'different', 'same', 'important', 'few', 'many', 'much', 'more', 'most', 'less', 'least', 'first', 'last', 'next', 'early', 'late', 'easy', 'hard', 'difficult', 'simple', 'complex', 'basic', 'advanced', 'public', 'private', 'social', 'personal', 'local', 'national', 'international', 'global', 'general', 'specific', 'particular', 'special', 'common', 'rare', 'unique', 'similar', 'equal', 'fair', 'true', 'false', 'real', 'fake', 'actual', 'virtual', 'possible', 'impossible', 'certain', 'uncertain', 'clear', 'unclear', 'obvious', 'visible', 'bright', 'dark', 'light', 'heavy', 'thin', 'thick', 'wide', 'narrow', 'broad', 'tall', 'deep', 'fast', 'slow', 'quick', 'sudden', 'immediate', 'urgent', 'casual', 'formal', 'official', 'legal', 'safe', 'dangerous', 'stable', 'firm', 'soft', 'hard', 'tough', 'gentle', 'harsh', 'mild', 'severe', 'extreme', 'moderate', 'average', 'normal', 'strange', 'weird', 'regular', 'consistent', 'reliable', 'honest', 'loyal', 'responsible', 'mature', 'wise', 'smart', 'intelligent', 'clever', 'brilliant', 'talented', 'skilled', 'experienced', 'professional', 'creative', 'original', 'innovative', 'traditional', 'modern', 'ancient', 'contemporary', 'historical', 'progressive', 'conservative', 'peaceful', 'violent', 'aggressive', 'passive', 'active', 'busy', 'lazy', 'energetic', 'tired', 'fresh', 'healthy', 'sick', 'strong', 'weak', 'powerful', 'mighty', 'robust', 'fragile', 'sturdy', 'delicate', 'rough', 'smooth', 'dense', 'empty', 'full', 'available', 'near', 'far', 'close', 'distant', 'connected', 'related', 'relevant', 'significant', 'major', 'minor', 'primary', 'main', 'central', 'superior', 'inferior', 'higher', 'lower', 'upper', 'bottom', 'front', 'back', 'left', 'inside', 'outside', 'internal', 'external', 'northern', 'southern', 'eastern', 'western', 'urban', 'rural', 'domestic', 'foreign', 'native', 'familiar', 'known', 'famous', 'popular', 'favorite', 'beloved', 'loved', 'respected', 'valuable', 'precious', 'expensive', 'cheap', 'free', 'successful', 'effective', 'efficient', 'productive', 'useful', 'helpful', 'beneficial', 'positive', 'negative', 'optimistic', 'pessimistic', 'hopeful', 'confident', 'nervous', 'calm', 'excited', 'relaxed', 'happy', 'sad', 'cheerful', 'pleased', 'satisfied', 'comfortable', 'convenient', 'pleasant', 'enjoyable', 'boring', 'interesting', 'exciting', 'thrilling', 'amusing', 'serious', 'funny', 'logical', 'rational', 'reasonable', 'sensible', 'practical', 'realistic', 'flexible', 'cooperative', 'friendly', 'kind', 'nice', 'sweet', 'bitter', 'sour', 'salty', 'spicy', 'hot', 'cold', 'warm', 'cool', 'wet', 'dry', 'moist', 'humid', 'solid', 'liquid', 'temporary', 'permanent', 'eternal', 'infinite', 'limited', 'unlimited', 'open', 'closed', 'secret', 'hidden', 'exposed', 'elegant', 'stylish', 'fashionable', 'trendy', 'classic', 'vintage', 'dim', 'shiny', 'dull', 'steady', 'constant', 'variable', 'changing', 'moving', 'mobile', 'fixed', 'rigid', 'loose', 'tight', 'tense', 'stressed', 'rushed', 'rapid', 'gradual', 'delayed', 'planned', 'spontaneous', 'deliberate', 'accidental', 'intentional', 'meaningful', 'essential', 'necessary', 'required', 'optional', 'voluntary', 'independent', 'wealthy', 'poor', 'rich', 'abundant', 'brief', 'extended', 'increased', 'decreased', 'balanced', 'uniform', 'diverse', 'mixed', 'pure', 'separate', 'individual', 'collective', 'single', 'multiple', 'whole', 'partial', 'complete', 'total', 'past', 'present', 'future', 'current', 'recent', 'latest', 'former', 'previous', 'upcoming', 'alien', 'human', 'animal', 'natural', 'artificial', 'genuine', 'authentic', 'ordinary', 'extraordinary', 'exceptional', 'typical', 'standard', 'conventional', 'alternative', 'revolutionary', 'liberal', 'straight', 'curved', 'round', 'square', 'flat', 'steep', 'sharp', 'blunt', 'even', 'level', 'organized', 'neat', 'clean', 'dirty', 'innocent', 'guilty', 'moral', 'ethical', 'virtuous', 'holy', 'sacred', 'blessed', 'lucky', 'fortunate', 'victorious', 'winning', 'leading', 'prompt', 'timely', 'frequent', 'continuous', 'finished', 'accomplished', 'joyful', 'merry', 'festive', 'solemn', 'humorous', 'grave', 'massive', 'tiny', 'huge', 'enormous', 'miniature', 'apparent', 'distinct', 'focused', 'concentrated', 'intense', 'forceful', 'quiet', 'silent', 'loud', 'vocal', 'spoken', 'written', 'digital', 'electronic', 'manual', 'automatic', 'voluntary', 'conscious', 'aware', 'alert', 'dynamic', 'elastic', 'meaningful', 'important', 'complete', 'smooth', 'active', 'amazing', 'wonderful', 'excellent', 'fantastic', 'awesome', 'perfect', 'outstanding', 'superb', 'magnificent', 'beautiful', 'fine', 'better', 'best', 'terrible', 'awful', 'horrible', 'disgusting', 'poor', 'weak', 'worse', 'worst', 'ineffective', 'useless', 'worthless', 'fake', 'significant', 'instant', 'constantly'
                }
            },
            
            # ë¶€ì‚¬ íŒ¨í„´ (400ê°œ ì´ìƒ)
            'adverb_patterns': {
                'endings': ['ly', 'ward', 'wise', 'wards'],
                'common_adverbs': {
                    'not', 'up', 'out', 'so', 'only', 'just', 'now', 'how', 'then', 'more', 'also', 'here', 'well', 'where', 'why', 'back', 'down', 'very', 'still', 'way', 'even', 'never', 'today', 'however', 'too', 'each', 'much', 'before', 'right', 'again', 'off', 'far', 'always', 'sometimes', 'usually', 'often', 'really', 'around', 'once', 'enough', 'quite', 'almost', 'especially', 'certainly', 'particularly', 'exactly', 'probably', 'recently', 'quickly', 'slowly', 'suddenly', 'carefully', 'clearly', 'simply', 'basically', 'generally', 'specifically', 'actually', 'finally', 'definitely', 'absolutely', 'completely', 'totally', 'extremely', 'highly', 'mostly', 'nearly', 'hardly', 'barely', 'seriously', 'immediately', 'directly', 'easily', 'possibly', 'obviously', 'unfortunately', 'surprisingly', 'interestingly', 'importantly', 'effectively', 'successfully', 'perfectly', 'regularly', 'frequently', 'constantly', 'instantly', 'rarely', 'seldom', 'normally', 'typically', 'commonly', 'occasionally', 'periodically', 'consistently', 'continuously', 'forever', 'temporarily', 'permanently', 'briefly', 'momentarily', 'rapidly', 'swiftly', 'speedily', 'gradually', 'steadily', 'smoothly', 'roughly', 'gently', 'harshly', 'softly', 'loudly', 'quietly', 'silently', 'apparently', 'evidently', 'positively', 'negatively', 'maybe', 'perhaps', 'likely', 'unlikely', 'surely', 'truly', 'genuinely', 'honestly', 'frankly', 'literally', 'virtually', 'practically', 'essentially', 'fundamentally', 'primarily', 'mainly', 'chiefly', 'largely', 'greatly', 'rather', 'fairly', 'pretty', 'somewhat', 'slightly', 'scarcely', 'entirely', 'wholly', 'partially', 'partly', 'half', 'quarter', 'twice', 'thrice', 'repeatedly', 'persistently', 'progressively', 'increasingly', 'decreasingly', 'better', 'worse', 'best', 'worst', 'faster', 'slower', 'quicker', 'sooner', 'later', 'earlier', 'previously', 'formerly', 'lately', 'currently', 'presently', 'yesterday', 'tomorrow', 'tonight', 'there', 'everywhere', 'anywhere', 'somewhere', 'nowhere', 'wherever', 'nearby', 'away', 'about', 'above', 'below', 'beneath', 'under', 'over', 'across', 'through', 'throughout', 'beyond', 'behind', 'ahead', 'forward', 'backward', 'backwards', 'upward', 'upwards', 'downward', 'downwards', 'inward', 'inwards', 'outward', 'outwards', 'sideways', 'straight', 'indirectly', 'north', 'south', 'east', 'west', 'within', 'without', 'alongside', 'together', 'apart', 'separately', 'alone', 'jointly', 'collectively', 'individually', 'personally', 'privately', 'publicly', 'openly', 'secretly', 'carelessly', 'safely', 'dangerously', 'securely', 'loosely', 'tightly', 'firmly', 'weakly', 'strongly', 'powerfully', 'mightily', 'forcefully', 'violently', 'peacefully', 'calmly', 'nervously', 'anxiously', 'confidently', 'proudly', 'humbly', 'modestly', 'boldly', 'bravely', 'courageously', 'fearlessly', 'fearfully', 'timidly', 'shyly', 'truthfully', 'sincerely', 'naturally', 'artificially', 'manually', 'automatically', 'mechanically', 'electronically', 'digitally', 'physically', 'mentally', 'emotionally', 'spiritually', 'intellectually', 'academically', 'professionally', 'personally', 'socially', 'politically', 'economically', 'financially', 'commercially', 'industrially', 'agriculturally', 'educationally', 'medically', 'legally', 'militarily', 'religiously', 'culturally', 'historically', 'traditionally', 'conventionally', 'unconventionally', 'originally', 'creatively', 'innovatively', 'artistically', 'scientifically', 'technically', 'theoretically', 'logically', 'rationally', 'reasonably', 'sensibly', 'wisely', 'foolishly', 'stupidly', 'intelligently', 'cleverly', 'brilliantly', 'skillfully', 'expertly', 'professionally', 'amateurishly', 'inexpertly', 'clumsily', 'awkwardly', 'gracefully', 'elegantly', 'beautifully', 'attractively', 'pleasantly', 'nicely', 'badly', 'poorly', 'terribly', 'awfully', 'horribly', 'wonderfully', 'amazingly', 'incredibly', 'unbelievably', 'shockingly', 'disappointingly', 'satisfyingly', 'pleasingly', 'annoyingly', 'irritatingly', 'frustratingly', 'confusingly', 'undoubtedly', 'affirmatively', 'yes', 'no', 'approximately', 'precisely', 'accurately', 'inaccurately', 'wrongly', 'incorrectly', 'mistakenly', 'accidentally', 'intentionally', 'deliberately', 'purposely', 'consciously', 'unconsciously', 'voluntarily', 'involuntarily', 'willingly', 'unwillingly', 'gladly', 'happily', 'sadly', 'fortunately', 'luckily', 'unluckily', 'hopefully', 'doubtfully', 'uncertainly', 'jokingly', 'playfully', 'casually', 'formally', 'informally', 'officially', 'unofficially', 'illegally', 'morally', 'immorally', 'ethically', 'unethically', 'righteously', 'wickedly', 'virtuously', 'sinfully', 'innocently', 'guiltily', 'purely', 'impurely', 'cleanly', 'dirtily', 'neatly', 'messily', 'tidily', 'untidily', 'orderly', 'disorderly', 'systematically', 'randomly', 'irregularly', 'reliably', 'unreliably', 'dependably', 'undependably', 'predictably', 'unpredictably', 'unsurprisingly', 'expectedly', 'unexpectedly', 'abnormally', 'atypically', 'unusually', 'uncommonly', 'sporadically', 'intermittently', 'discontinuously', 'inconstantly', 'continually', 'several', 'some', 'any', 'all', 'every', 'both', 'either', 'neither', 'none', 'nothing', 'something', 'anything', 'everything', 'everyone', 'anyone', 'someone', 'nobody', 'somebody', 'anybody', 'everybody'
                }
            }
        }

        # ë‹¨ì–´ ë¶„ë¦¬ ë° ë¶„ì„
        words = re.findall(r'\b[a-zA-Z]+\b', cleaned_text.lower())
        
        manual_results = []
        pos_counts = {'ëª…ì‚¬': 0, 'ë™ì‚¬': 0, 'í˜•ìš©ì‚¬': 0, 'ë¶€ì‚¬': 0, 'ê¸°íƒ€': 0}
        
        for word in words:
            if len(word) < 2:
                continue
                
            pos_tag = 'ê¸°íƒ€'
            rule_applied = ''
            
            # ìˆœì„œë¥¼ ìµœì í™”í•˜ì—¬ ì •í™•í•œ ë¶„ë¥˜
            
            # 1. ì¼ë°˜ì ì¸ ë‹¨ì–´ ëª©ë¡ ìš°ì„  í™•ì¸ (ê°€ì¥ ì •í™•)
            if word in manual_rules['noun_patterns']['common_nouns']:
                pos_tag = 'ëª…ì‚¬'
                rule_applied = "ì¼ë°˜ ëª…ì‚¬ ëª©ë¡"
            elif word in manual_rules['verb_patterns']['common_verbs']:
                pos_tag = 'ë™ì‚¬'
                rule_applied = "ì¼ë°˜ ë™ì‚¬ ëª©ë¡"
            elif word in manual_rules['adjective_patterns']['common_adjectives']:
                pos_tag = 'í˜•ìš©ì‚¬'
                rule_applied = "ì¼ë°˜ í˜•ìš©ì‚¬ ëª©ë¡"
            elif word in manual_rules['adverb_patterns']['common_adverbs']:
                pos_tag = 'ë¶€ì‚¬'
                rule_applied = "ì¼ë°˜ ë¶€ì‚¬ ëª©ë¡"
            
            # 2. ì–´ë¯¸ íŒ¨í„´ í™•ì¸ (íŠ¹ë³„í•œ ìˆœì„œë¡œ ì •í™•ë„ í–¥ìƒ)
            # ë¶€ì‚¬ëŠ” -lyë¡œ ëë‚˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ìš°ì„  í™•ì¸
            elif word.endswith('ly') and len(word) > 3:
                pos_tag = 'ë¶€ì‚¬'
                rule_applied = "ì–´ë¯¸ íŒ¨í„´: -ly"
            
            # í˜•ìš©ì‚¬ ì–´ë¯¸ë“¤
            elif any(word.endswith(ending) for ending in ['ful', 'less', 'ive', 'able', 'ible', 'ous', 'ious']):
                pos_tag = 'í˜•ìš©ì‚¬'
                matching_ending = next(ending for ending in ['ful', 'less', 'ive', 'able', 'ible', 'ous', 'ious'] if word.endswith(ending))
                rule_applied = f"ì–´ë¯¸ íŒ¨í„´: -{matching_ending}"
            
            # ëª…ì‚¬ ì–´ë¯¸ë“¤
            elif any(word.endswith(ending) for ending in ['tion', 'sion', 'ness', 'ment', 'ship', 'hood', 'ity']):
                pos_tag = 'ëª…ì‚¬'
                matching_ending = next(ending for ending in ['tion', 'sion', 'ness', 'ment', 'ship', 'hood', 'ity'] if word.endswith(ending))
                rule_applied = f"ì–´ë¯¸ íŒ¨í„´: -{matching_ending}"
            
            # ë™ì‚¬ ì–´ë¯¸ë“¤ (ê³¼ê±°í˜•, í˜„ì¬ë¶„ì‚¬ ë“±)
            elif word.endswith('ed') and len(word) > 3:
                pos_tag = 'ë™ì‚¬'
                rule_applied = "ì–´ë¯¸ íŒ¨í„´: -ed (ê³¼ê±°í˜•)"
            elif word.endswith('ing') and len(word) > 4:
                pos_tag = 'ë™ì‚¬'
                rule_applied = "ì–´ë¯¸ íŒ¨í„´: -ing (í˜„ì¬ë¶„ì‚¬)"
            
            # 3. ê¸°íƒ€ ì–´ë¯¸ íŒ¨í„´ë“¤
            elif any(word.endswith(ending) for ending in manual_rules['adjective_patterns']['endings']):
                pos_tag = 'í˜•ìš©ì‚¬'
                rule_applied = f"ì–´ë¯¸ íŒ¨í„´ (í˜•ìš©ì‚¬)"
            elif any(word.endswith(ending) for ending in manual_rules['noun_patterns']['endings']):
                pos_tag = 'ëª…ì‚¬'
                rule_applied = f"ì–´ë¯¸ íŒ¨í„´ (ëª…ì‚¬)"
            elif any(word.endswith(ending) for ending in manual_rules['verb_patterns']['endings']):
                pos_tag = 'ë™ì‚¬'
                rule_applied = f"ì–´ë¯¸ íŒ¨í„´ (ë™ì‚¬)"
            elif any(word.endswith(ending) for ending in manual_rules['adverb_patterns']['endings']):
                pos_tag = 'ë¶€ì‚¬'
                rule_applied = f"ì–´ë¯¸ íŒ¨í„´ (ë¶€ì‚¬)"
            
            pos_counts[pos_tag] += 1
            if len(manual_results) < 30:  # ì²˜ìŒ 30ê°œë§Œ ì €ì¥
                manual_results.append({
                    'word': word,
                    'pos': pos_tag,
                    'rule': rule_applied if rule_applied else 'ê¸°íƒ€ ë¶„ë¥˜'
                })
        
        # ë¹„ìœ¨ ê³„ì‚°
        total_words = sum(pos_counts.values())
        pos_ratios = {}
        if total_words > 0:
            for pos, count in pos_counts.items():
                pos_ratios[pos] = (count / total_words) * 100
        
        # ë‹¨ì–´ ëª©ë¡ í¬ê¸° ê³„ì‚°
        total_known_words = (
            len(manual_rules['noun_patterns']['common_nouns']) +
            len(manual_rules['verb_patterns']['common_verbs']) +
            len(manual_rules['adjective_patterns']['common_adjectives']) +
            len(manual_rules['adverb_patterns']['common_adverbs'])
        )
        
        return {
            'method': 'ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜ (ì™„ì „ ê°œì„ )',
            'word_analysis': manual_results,
            'pos_counts': pos_counts,
            'pos_ratios': pos_ratios,
            'total_words': total_words,
            'known_words_count': total_known_words,
            'explanation': f"ëŒ€í­ í™•ì¥ëœ ë‹¨ì–´ ëª©ë¡ ({total_known_words}ê°œ) + ìµœì í™”ëœ ì–´ë¯¸ íŒ¨í„´ìœ¼ë¡œ ì™„ì „ ê°œì„ . ì´ {total_words}ê°œ ë‹¨ì–´ ì¤‘ ê¸°íƒ€: {pos_counts['ê¸°íƒ€']}ê°œ ({pos_ratios.get('ê¸°íƒ€', 0):.1f}%)"
        }

    def educational_pos_analysis_step2_nltk_basic(self, text):
        """2ë‹¨ê³„: NLTK ê¸°ë³¸ í’ˆì‚¬ íƒœê¹… - êµìœ¡ìš©"""
        if not text or pd.isna(text):
            return {}
        
        # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
        cleaned_text = self.extract_essay_content(text)
        if not cleaned_text:
            return {}
        
        try:
            # NLTK í’ˆì‚¬ íƒœê¹…
            words = nltk.word_tokenize(cleaned_text)
            pos_tags = nltk.pos_tag(words)
            
            # í’ˆì‚¬ íƒœê·¸ ì„¤ëª…
            pos_tag_explanations = {
                'NN': 'ëª…ì‚¬(ë‹¨ìˆ˜)', 'NNS': 'ëª…ì‚¬(ë³µìˆ˜)', 'NNP': 'ê³ ìœ ëª…ì‚¬(ë‹¨ìˆ˜)', 'NNPS': 'ê³ ìœ ëª…ì‚¬(ë³µìˆ˜)',
                'VB': 'ë™ì‚¬(ì›í˜•)', 'VBD': 'ë™ì‚¬(ê³¼ê±°)', 'VBG': 'ë™ì‚¬(í˜„ì¬ë¶„ì‚¬)', 'VBN': 'ë™ì‚¬(ê³¼ê±°ë¶„ì‚¬)', 
                'VBP': 'ë™ì‚¬(í˜„ì¬)', 'VBZ': 'ë™ì‚¬(3ì¸ì¹­ë‹¨ìˆ˜)',
                'JJ': 'í˜•ìš©ì‚¬', 'JJR': 'í˜•ìš©ì‚¬(ë¹„êµê¸‰)', 'JJS': 'í˜•ìš©ì‚¬(ìµœìƒê¸‰)',
                'RB': 'ë¶€ì‚¬', 'RBR': 'ë¶€ì‚¬(ë¹„êµê¸‰)', 'RBS': 'ë¶€ì‚¬(ìµœìƒê¸‰)',
                'DT': 'ê´€ì‚¬', 'IN': 'ì „ì¹˜ì‚¬', 'CC': 'ì ‘ì†ì‚¬', 'PRP': 'ëŒ€ëª…ì‚¬'
            }
            
            # ì£¼ìš” í’ˆì‚¬ë³„ ë¶„ë¥˜
            pos_categories = {
                'ëª…ì‚¬': ['NN', 'NNS', 'NNP', 'NNPS'],
                'ë™ì‚¬': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],
                'í˜•ìš©ì‚¬': ['JJ', 'JJR', 'JJS'],
                'ë¶€ì‚¬': ['RB', 'RBR', 'RBS'],
                'ê¸°ëŠ¥ì–´': ['DT', 'IN', 'CC', 'PRP', 'TO', 'WDT', 'WP', 'WRB']
            }
            
            # ê²°ê³¼ ë¶„ì„
            detailed_analysis = []
            category_counts = {'ëª…ì‚¬': 0, 'ë™ì‚¬': 0, 'í˜•ìš©ì‚¬': 0, 'ë¶€ì‚¬': 0, 'ê¸°ëŠ¥ì–´': 0, 'ê¸°íƒ€': 0}
            
            for word, pos in pos_tags:
                if word.isalpha() and len(word) > 1:
                    category = 'ê¸°íƒ€'
                    for cat, tags in pos_categories.items():
                        if pos in tags:
                            category = cat
                            break
                    
                    category_counts[category] += 1
                    
                    detailed_analysis.append({
                        'word': word.lower(),
                        'pos_tag': pos,
                        'pos_explanation': pos_tag_explanations.get(pos, 'ê¸°íƒ€'),
                        'category': category
                    })
            
            # ë¹„ìœ¨ ê³„ì‚°
            total_words = sum(category_counts.values())
            category_ratios = {}
            if total_words > 0:
                for category, count in category_counts.items():
                    category_ratios[category] = (count / total_words) * 100
            
            return {
                'method': 'NLTK í’ˆì‚¬ íƒœê¹…',
                'detailed_analysis': detailed_analysis[:30],  # ì²˜ìŒ 30ê°œë§Œ
                'category_counts': category_counts,
                'category_ratios': category_ratios,
                'total_words': total_words,
                'pos_tag_explanations': pos_tag_explanations,
                'pos_tagged_words': pos_tags,  # main.pyì—ì„œ í•„ìš”í•œ í‚¤ ì¶”ê°€
                'explanation': "NLTKì˜ ê¸°ê³„í•™ìŠµ ê¸°ë°˜ í’ˆì‚¬ íƒœê±°ë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ë¶„ë¥˜"
            }
            
        except Exception as e:
            return {
                'method': 'NLTK í’ˆì‚¬ íƒœê¹…',
                'error': f'NLTK ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'explanation': 'NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì œë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }

    def educational_pos_analysis_step3_pattern_discovery(self, text):
        """3ë‹¨ê³„: íŒ¨í„´ ë°œê²¬ ë° ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„ - êµìœ¡ìš©"""
        if not text or pd.isna(text):
            return {}
        
        # ì—ì„¸ì´ ë‚´ìš©ë§Œ ì¶”ì¶œ
        cleaned_text = self.extract_essay_content(text)
        if not cleaned_text:
            return {}
        
        try:
            # ë¬¸ì¥ë³„ ë¶„ì„
            sentences = nltk.sent_tokenize(cleaned_text)
            
            sentence_analysis = []
            overall_patterns = {
                'avg_sentence_length': 0,
                'noun_density': [],
                'verb_density': [],
                'adj_density': [],
                'complex_sentences': 0,
                'simple_sentences': 0
            }
            
            for i, sentence in enumerate(sentences[:5]):  # ì²˜ìŒ 5ë¬¸ì¥ë§Œ
                words = nltk.word_tokenize(sentence)
                pos_tags = nltk.pos_tag(words)
                
                # í’ˆì‚¬ë³„ ì¹´ìš´íŠ¸
                nouns = sum(1 for _, pos in pos_tags if pos.startswith('NN'))
                verbs = sum(1 for _, pos in pos_tags if pos.startswith('VB'))
                adjectives = sum(1 for _, pos in pos_tags if pos.startswith('JJ'))
                total_content_words = nouns + verbs + adjectives
                
                if total_content_words > 0:
                    noun_ratio = (nouns / total_content_words) * 100
                    verb_ratio = (verbs / total_content_words) * 100
                    adj_ratio = (adjectives / total_content_words) * 100
                else:
                    noun_ratio = verb_ratio = adj_ratio = 0
                
                # ë¬¸ì¥ ë³µì¡ë„ íŒë‹¨
                is_complex = len(words) > 15 or any(word.lower() in ['because', 'although', 'since', 'while', 'if'] for word, _ in pos_tags)
                
                if is_complex:
                    overall_patterns['complex_sentences'] += 1
                else:
                    overall_patterns['simple_sentences'] += 1
                
                overall_patterns['noun_density'].append(noun_ratio)
                overall_patterns['verb_density'].append(verb_ratio)
                overall_patterns['adj_density'].append(adj_ratio)
                
                sentence_analysis.append({
                    'sentence_num': i + 1,
                    'sentence': sentence[:100] + "..." if len(sentence) > 100 else sentence,
                    'word_count': len(words),
                    'nouns': nouns,
                    'verbs': verbs,
                    'adjectives': adjectives,
                    'noun_ratio': noun_ratio,
                    'verb_ratio': verb_ratio,
                    'adj_ratio': adj_ratio,
                    'complexity': 'Complex' if is_complex else 'Simple',
                    'content_words': total_content_words
                })
            
            # ì „ì²´ íŒ¨í„´ ë¶„ì„
            if overall_patterns['noun_density']:
                avg_noun_density = sum(overall_patterns['noun_density']) / len(overall_patterns['noun_density'])
                avg_verb_density = sum(overall_patterns['verb_density']) / len(overall_patterns['verb_density'])
                avg_adj_density = sum(overall_patterns['adj_density']) / len(overall_patterns['adj_density'])
            else:
                avg_noun_density = avg_verb_density = avg_adj_density = 0
            
            # ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ë¶„ì„
            writing_style = {}
            
            if avg_noun_density > 50:
                writing_style['noun_heavy'] = "ëª…ì‚¬ ì¤‘ì‹¬ì˜ ì •ë³´ ì „ë‹¬í˜• ê¸€ì“°ê¸°"
            elif avg_verb_density > 30:
                writing_style['verb_heavy'] = "ë™ì‚¬ ì¤‘ì‹¬ì˜ ë™ì ì¸ ê¸€ì“°ê¸°"
            elif avg_adj_density > 20:
                writing_style['descriptive'] = "í˜•ìš©ì‚¬ê°€ í’ë¶€í•œ ë¬˜ì‚¬ì  ê¸€ì“°ê¸°"
            else:
                writing_style['balanced'] = "ê· í˜•ì¡íŒ ê¸€ì“°ê¸°"
            
            complexity_ratio = overall_patterns['complex_sentences'] / (overall_patterns['complex_sentences'] + overall_patterns['simple_sentences']) * 100 if (overall_patterns['complex_sentences'] + overall_patterns['simple_sentences']) > 0 else 0
            
            # ê³µí†µ íŒ¨í„´ ìƒì„± (main.pyì—ì„œ í•„ìš”)
            common_patterns = []
            if avg_noun_density > 50:
                common_patterns.append("NN-DT-NN (ëª…ì‚¬-ê´€ì‚¬-ëª…ì‚¬ íŒ¨í„´ ë¹ˆë°œ)")
            if avg_verb_density > 30:
                common_patterns.append("VB-DT-NN (ë™ì‚¬-ê´€ì‚¬-ëª…ì‚¬ íŒ¨í„´)")
            if complexity_ratio > 50:
                common_patterns.append("IN-PRP-VB (ì „ì¹˜ì‚¬-ëŒ€ëª…ì‚¬-ë™ì‚¬ ë³µí•© íŒ¨í„´)")
            if len(sentence_analysis) > 3:
                common_patterns.append("PRP-VB-JJ (ëŒ€ëª…ì‚¬-ë™ì‚¬-í˜•ìš©ì‚¬ íŒ¨í„´)")
            common_patterns.append("DT-JJ-NN (ê´€ì‚¬-í˜•ìš©ì‚¬-ëª…ì‚¬ íŒ¨í„´)")

            # ì–¸ì–´ì  íŠ¹ì„± ì •ë¦¬ (main.pyì—ì„œ í•„ìš”)
            linguistic_features = {
                'complexity_score': complexity_ratio / 100,  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                'lexical_diversity': min(1.0, avg_adj_density / 25.0),  # ì–´íœ˜ ë‹¤ì–‘ì„± ì¶”ì •
                'avg_sentence_length': sum(sa['word_count'] for sa in sentence_analysis) / len(sentence_analysis) if sentence_analysis else 0
            }

            return {
                'method': 'íŒ¨í„´ ë°œê²¬ ë° ì–¸ì–´ì  íŠ¹ì„±',
                'sentence_analysis': sentence_analysis,
                'overall_patterns': {
                    'avg_noun_density': avg_noun_density,
                    'avg_verb_density': avg_verb_density,
                    'avg_adj_density': avg_adj_density,
                    'complexity_ratio': complexity_ratio,
                    'total_sentences': len(sentence_analysis)
                },
                'writing_style': writing_style,
                'common_patterns': common_patterns,  # main.pyì—ì„œ í•„ìš”í•œ í‚¤ ì¶”ê°€
                'linguistic_features': linguistic_features,  # main.pyì—ì„œ í•„ìš”í•œ í‚¤ ì¶”ê°€
                'explanation': "ë¬¸ì¥ë³„ í’ˆì‚¬ ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ ê¸€ì“°ê¸° íŒ¨í„´ê³¼ ìŠ¤íƒ€ì¼ì„ ë°œê²¬"
            }
            
        except Exception as e:
            return {
                'method': 'íŒ¨í„´ ë°œê²¬ ë° ì–¸ì–´ì  íŠ¹ì„±',
                'error': f'íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'explanation': 'íŒ¨í„´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }

    def educational_pos_comparison(self, essay_data, essay_index=0):
        """ì„¸ ê°€ì§€ í’ˆì‚¬ ë¶„ì„ ë°©ë²• ë¹„êµ - êµìœ¡ìš©"""
        if essay_data.empty or essay_index >= len(essay_data):
            return {}
        
        # ì„ íƒëœ ì—ì„¸ì´
        selected_essay = essay_data.iloc[essay_index]
        essay_text = selected_essay.get('essay_text', '')
        
        if not essay_text:
            return {}
        
        # ì„¸ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë¶„ì„
        result1 = self.educational_pos_analysis_step1_manual_rules(essay_text)
        result2 = self.educational_pos_analysis_step2_nltk_basic(essay_text)
        result3 = self.educational_pos_analysis_step3_pattern_discovery(essay_text)
        
        return {
            'essay_info': {
                'essay_num': essay_index + 1,
                'topic': selected_essay.get('topic_name', 'Unknown'),
                'text_preview': self.extract_essay_content(essay_text)[:200] + "..."
            },
            'method1_manual': result1,
            'method2_nltk': result2,
            'method3_patterns': result3
        }
    
    def educational_pos_analysis_step4_benchmarking(self, text):
        """4ë‹¨ê³„: ë²¤ì¹˜ë§ˆí‚¹ ë° ê¸€ì“°ê¸° ìˆ˜ì¤€ ì§„ë‹¨"""
        
        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
        BENCHMARK_DATA = {
            "middle_school_excellent": {
                "name": "ğŸ“ ì¤‘í•™ìƒ ìš°ìˆ˜ì‘",
                "noun_ratio": 42.1,
                "verb_ratio": 22.8,
                "adj_ratio": 15.3,
                "complexity_ratio": 58.7,
                "vocabulary_diversity": 0.72
            },
            "academic_essay": {
                "name": "ğŸ“š í•™ìˆ  ì—ì„¸ì´",
                "noun_ratio": 45.2,
                "verb_ratio": 18.3,
                "adj_ratio": 12.1,
                "complexity_ratio": 78.5,
                "vocabulary_diversity": 0.85
            },
            "creative_writing": {
                "name": "ğŸ¨ ì°½ì˜ì  ê¸€ì“°ê¸°",
                "noun_ratio": 38.7,
                "verb_ratio": 25.4,
                "adj_ratio": 18.2,
                "complexity_ratio": 65.2,
                "vocabulary_diversity": 0.78
            }
        }
        
        try:
            # ì‚¬ìš©ì í…ìŠ¤íŠ¸ ë¶„ì„ (ê¸°ì¡´ ë©”ì„œë“œë“¤ í™œìš©)
            step1_result = self.educational_pos_analysis_step1_manual_rules(text)
            step2_result = self.educational_pos_analysis_step2_nltk_basic(text)
            step3_result = self.educational_pos_analysis_step3_pattern_discovery(text)
            
            # ì‚¬ìš©ì í†µê³„ ì¶”ì¶œ
            user_stats = self._extract_user_statistics(step1_result, step2_result, step3_result, text)
            
            # ê° ë²¤ì¹˜ë§ˆí¬ì™€ ë¹„êµ
            comparisons = {}
            for benchmark_key, benchmark_data in BENCHMARK_DATA.items():
                comparison = self._compare_with_benchmark(user_stats, benchmark_data)
                comparisons[benchmark_key] = comparison
            
            # ì¢…í•© í‰ê°€
            overall_assessment = self._calculate_writing_level(user_stats, BENCHMARK_DATA)
            
            # ê°œì„  ì œì•ˆ
            improvement_suggestions = self._generate_improvement_suggestions(user_stats, BENCHMARK_DATA)
            
            return {
                'user_stats': user_stats,
                'benchmark_comparisons': comparisons,
                'overall_assessment': overall_assessment,
                'improvement_suggestions': improvement_suggestions,
                'writing_strengths': self._identify_strengths(user_stats, BENCHMARK_DATA),
                'growth_areas': self._identify_growth_areas(user_stats, BENCHMARK_DATA)
            }
            
        except Exception as e:
            return {'error': f"ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

    def _extract_user_statistics(self, step1_result, step2_result, step3_result, text):
        """ì‚¬ìš©ì í…ìŠ¤íŠ¸ í†µê³„ ì¶”ì¶œ"""
        
        # Step1ì—ì„œ ê¸°ë³¸ í’ˆì‚¬ ë¹„ìœ¨
        pos_ratios = step1_result.get('pos_ratios', {})
        
        # Step3ì—ì„œ ë³µì¡ë„ì™€ íŒ¨í„´
        patterns = step3_result.get('overall_patterns', {})
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± ê³„ì‚°
        words = text.lower().split()
        unique_words = len(set(words))
        total_words = len(words)
        vocabulary_diversity = unique_words / total_words if total_words > 0 else 0
        
        # ë¬¸ì¥ ìˆ˜ ê³„ì‚°
        sentences = text.split('.')
        sentences = [s.strip() for s in sentences if s.strip()]
        avg_sentence_length = total_words / len(sentences) if sentences else 0
        
        return {
            'noun_ratio': pos_ratios.get('ëª…ì‚¬', 0),
            'verb_ratio': pos_ratios.get('ë™ì‚¬', 0),
            'adj_ratio': pos_ratios.get('í˜•ìš©ì‚¬', 0),
            'complexity_ratio': patterns.get('complexity_ratio', 0),
            'vocabulary_diversity': vocabulary_diversity,
            'total_words': total_words,
            'total_sentences': len(sentences),
            'avg_sentence_length': avg_sentence_length
        }

    def _compare_with_benchmark(self, user_stats, benchmark_data):
        """ë²¤ì¹˜ë§ˆí¬ì™€ ë¹„êµ ë¶„ì„"""
        
        comparison = {
            'benchmark_name': benchmark_data['name'],
            'scores': {},
            'total_score': 0,
            'strengths': [],
            'improvements': []
        }
        
        # ê° ì§€í‘œë³„ ë¹„êµ (100ì  ë§Œì )
        metrics = ['noun_ratio', 'verb_ratio', 'adj_ratio', 'complexity_ratio', 'vocabulary_diversity']
        
        total_score = 0
        for metric in metrics:
            user_value = user_stats.get(metric, 0)
            benchmark_value = benchmark_data.get(metric, 0)
            
            # ì°¨ì´ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ (ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            if benchmark_value > 0:
                difference = abs(user_value - benchmark_value) / benchmark_value
                score = max(0, 100 - (difference * 100))
            else:
                score = 50  # ê¸°ë³¸ ì ìˆ˜
            
            comparison['scores'][metric] = score
            total_score += score
        
        comparison['total_score'] = total_score / len(metrics)
        
        return comparison

    def _calculate_writing_level(self, user_stats, benchmark_data):
        """ì¢…í•© ê¸€ì“°ê¸° ìˆ˜ì¤€ ê³„ì‚°"""
        
        # ì¤‘í•™ìƒ ìš°ìˆ˜ì‘ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
        middle_school_benchmark = benchmark_data['middle_school_excellent']
        comparison = self._compare_with_benchmark(user_stats, middle_school_benchmark)
        
        score = comparison['total_score']
        
        if score >= 85:
            level = "ğŸ† ìš°ìˆ˜ (Excellent)"
            level_desc = "ì¤‘í•™ìƒ ìµœìƒìœ„ ìˆ˜ì¤€ì˜ ê¸€ì“°ê¸° ì‹¤ë ¥!"
            color = "success"
        elif score >= 70:
            level = "ğŸ¥ˆ ì–‘í˜¸ (Good)"
            level_desc = "ì¤‘í•™ìƒ ìƒìœ„ ìˆ˜ì¤€ì˜ ì•ˆì •ì ì¸ ê¸€ì“°ê¸°"
            color = "info"
        elif score >= 55:
            level = "ğŸ¥‰ ë³´í†µ (Average)"
            level_desc = "ì¤‘í•™ìƒ í‰ê·  ìˆ˜ì¤€, ê¾¸ì¤€í•œ ì—°ìŠµì´ í•„ìš”"
            color = "warning"
        else:
            level = "ğŸ“š ê°œì„  í•„ìš” (Needs Improvement)"
            level_desc = "ê¸°ì´ˆë¥¼ ë‹¤ì§€ë©° ë‹¨ê³„ì ìœ¼ë¡œ í–¥ìƒí•´ë³´ì„¸ìš”"
            color = "error"
        
        return {
            'level': level,
            'description': level_desc,
            'score': round(score, 1),
            'color': color
        }

    def _generate_improvement_suggestions(self, user_stats, benchmark_data):
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        middle_school = benchmark_data['middle_school_excellent']
        
        # ëª…ì‚¬ ì‚¬ìš© ë¶„ì„
        noun_diff = user_stats['noun_ratio'] - middle_school['noun_ratio']
        if noun_diff > 8:
            suggestions.append({
                'type': 'ëª…ì‚¬ ì‚¬ìš©',
                'icon': 'ğŸ“',
                'suggestion': 'ëª…ì‚¬ ì‚¬ìš©ì„ ì¤„ì´ê³  ë™ì‚¬ì™€ í˜•ìš©ì‚¬ë¡œ ë” ìƒë™ê° ìˆê²Œ í‘œí˜„í•´ë³´ì„¸ìš”',
                'priority': 'high'
            })
        elif noun_diff < -8:
            suggestions.append({
                'type': 'ëª…ì‚¬ ì‚¬ìš©',
                'icon': 'ğŸ¯',
                'suggestion': 'ë” êµ¬ì²´ì ì´ê³  ì •í™•í•œ ëª…ì‚¬ë¥¼ ì‚¬ìš©í•´ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì „ë‹¬í•´ë³´ì„¸ìš”',
                'priority': 'medium'
            })
        
        # ë™ì‚¬ í™œìš© ë¶„ì„
        verb_diff = user_stats['verb_ratio'] - middle_school['verb_ratio']
        if verb_diff < -5:
            suggestions.append({
                'type': 'ë™ì‚¬ í™œìš©',
                'icon': 'ğŸ¬',
                'suggestion': 'ë‹¤ì–‘í•œ ë™ì‘ ë™ì‚¬ë¥¼ í™œìš©í•´ ê¸€ì— ìƒë™ê°ì„ ë”í•´ë³´ì„¸ìš”',
                'priority': 'high'
            })
        
        # í˜•ìš©ì‚¬ í‘œí˜„ ë¶„ì„
        adj_diff = user_stats['adj_ratio'] - middle_school['adj_ratio']
        if adj_diff < -3:
            suggestions.append({
                'type': 'í˜•ìš©ì‚¬ í‘œí˜„',
                'icon': 'ğŸ¨',
                'suggestion': 'ì ì ˆí•œ í˜•ìš©ì‚¬ë¥¼ ì‚¬ìš©í•´ ë¬˜ì‚¬ë¥¼ ë” í’ë¶€í•˜ê³  ìƒìƒí•˜ê²Œ ë§Œë“¤ì–´ë³´ì„¸ìš”',
                'priority': 'medium'
            })
        elif adj_diff > 8:
            suggestions.append({
                'type': 'í˜•ìš©ì‚¬ í‘œí˜„',
                'icon': 'âœ‚ï¸',
                'suggestion': 'ê³¼ë„í•œ í˜•ìš©ì‚¬ ì‚¬ìš©ì„ ì¤„ì´ê³  í•µì‹¬ ë‚´ìš©ì— ì§‘ì¤‘í•´ë³´ì„¸ìš”',
                'priority': 'low'
            })
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„
        diversity_diff = user_stats['vocabulary_diversity'] - middle_school['vocabulary_diversity']
        if diversity_diff < -0.1:
            suggestions.append({
                'type': 'ì–´íœ˜ ë‹¤ì–‘ì„±',
                'icon': 'ğŸ“š',
                'suggestion': 'ê°™ì€ ë‹¨ì–´ ë°˜ë³µì„ í”¼í•˜ê³  ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”',
                'priority': 'high'
            })
        
        # ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„
        complexity_diff = user_stats['complexity_ratio'] - middle_school['complexity_ratio']
        if complexity_diff < -15:
            suggestions.append({
                'type': 'ë¬¸ì¥ êµ¬ì¡°',
                'icon': 'ğŸ§ ',
                'suggestion': 'ë‹¨ìˆœ ë¬¸ì¥ê³¼ ë³µí•© ë¬¸ì¥ì„ ì ì ˆíˆ ì¡°í•©í•´ ê¸€ì˜ íë¦„ì„ ê°œì„ í•´ë³´ì„¸ìš”',
                'priority': 'medium'
            })
        elif complexity_diff > 20:
            suggestions.append({
                'type': 'ë¬¸ì¥ êµ¬ì¡°',
                'icon': 'âœ¨',
                'suggestion': 'ë„ˆë¬´ ë³µì¡í•œ ë¬¸ì¥ë³´ë‹¤ëŠ” ëª…í™•í•˜ê³  ê°„ê²°í•œ í‘œí˜„ì„ ì—°ìŠµí•´ë³´ì„¸ìš”',
                'priority': 'medium'
            })
        
        # ê¸°ë³¸ ì œì•ˆì´ ì—†ìœ¼ë©´ ê²©ë ¤ ë©”ì‹œì§€
        if not suggestions:
            suggestions.append({
                'type': 'ì „ì²´ì  í‰ê°€',
                'icon': 'ğŸŒŸ',
                'suggestion': 'ê· í˜• ì¡íŒ ì¢‹ì€ ê¸€ì“°ê¸°ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤! ì°½ì˜ì  í‘œí˜„ì— ë„ì „í•´ë³´ì„¸ìš”',
                'priority': 'low'
            })
        
        return suggestions

    def _identify_strengths(self, user_stats, benchmark_data):
        """ê¸€ì“°ê¸° ê°•ì  ì‹ë³„"""
        
        strengths = []
        middle_school = benchmark_data['middle_school_excellent']
        
        # ê° ì˜ì—­ë³„ ê°•ì  ì²´í¬
        if abs(user_stats['noun_ratio'] - middle_school['noun_ratio']) <= 5:
            strengths.append("ëª…ì‚¬ ì‚¬ìš©ì´ ì ì ˆí•˜ê³  ê· í˜•ì¡í˜€ ìˆìŠµë‹ˆë‹¤")
        
        if abs(user_stats['verb_ratio'] - middle_school['verb_ratio']) <= 3:
            strengths.append("ë™ì‚¬ í™œìš©ì´ ìš°ìˆ˜í•˜ì—¬ ê¸€ì— ìƒë™ê°ì´ ìˆìŠµë‹ˆë‹¤")
        
        if abs(user_stats['adj_ratio'] - middle_school['adj_ratio']) <= 3:
            strengths.append("í˜•ìš©ì‚¬ í‘œí˜„ì´ ì ì ˆí•˜ì—¬ ë¬˜ì‚¬ê°€ í’ë¶€í•©ë‹ˆë‹¤")
        
        if user_stats['vocabulary_diversity'] >= middle_school['vocabulary_diversity']:
            strengths.append("ì–´íœ˜ ì‚¬ìš©ì´ ë‹¤ì–‘í•˜ê³  í’ë¶€í•©ë‹ˆë‹¤")
        
        if abs(user_stats['complexity_ratio'] - middle_school['complexity_ratio']) <= 10:
            strengths.append("ë¬¸ì¥ êµ¬ì¡°ê°€ ì ì ˆíˆ ë³µí•©ì ì…ë‹ˆë‹¤")
        
        if user_stats['avg_sentence_length'] >= 8 and user_stats['avg_sentence_length'] <= 15:
            strengths.append("ë¬¸ì¥ ê¸¸ì´ê°€ ì½ê¸° ì¢‹ê²Œ ì¡°ì ˆë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        return strengths if strengths else ["ê¾¸ì¤€í•œ ì—°ìŠµìœ¼ë¡œ ì‹¤ë ¥ì´ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤"]

    def _identify_growth_areas(self, user_stats, benchmark_data):
        """ì„±ì¥ì´ í•„ìš”í•œ ì˜ì—­ ì‹ë³„"""
        
        growth_areas = []
        middle_school = benchmark_data['middle_school_excellent']
        
        # ê°œì„ ì´ ê°€ì¥ í•„ìš”í•œ ì˜ì—­ ì‹ë³„
        improvements_needed = []
        
        noun_diff = abs(user_stats['noun_ratio'] - middle_school['noun_ratio'])
        if noun_diff > 8:
            improvements_needed.append(('ëª…ì‚¬ ì‚¬ìš© ê· í˜•', noun_diff))
        
        verb_diff = abs(user_stats['verb_ratio'] - middle_school['verb_ratio'])
        if verb_diff > 5:
            improvements_needed.append(('ë™ì‚¬ í™œìš© ë‹¤ì–‘ì„±', verb_diff))
        
        adj_diff = abs(user_stats['adj_ratio'] - middle_school['adj_ratio'])
        if adj_diff > 5:
            improvements_needed.append(('í˜•ìš©ì‚¬ í‘œí˜„ ì¡°ì ˆ', adj_diff))
        
        if user_stats['vocabulary_diversity'] < middle_school['vocabulary_diversity'] - 0.1:
            improvements_needed.append(('ì–´íœ˜ ë‹¤ì–‘ì„± í–¥ìƒ', 
                                    (middle_school['vocabulary_diversity'] - user_stats['vocabulary_diversity']) * 100))
        
        complexity_diff = abs(user_stats['complexity_ratio'] - middle_school['complexity_ratio'])
        if complexity_diff > 15:
            improvements_needed.append(('ë¬¸ì¥ êµ¬ì¡° ê°œì„ ', complexity_diff))
        
        # ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ì˜ì—­ë¶€í„° ì •ë ¬
        improvements_needed.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ 3ê°œ ì˜ì—­ë§Œ ì„ íƒ
        for area, _ in improvements_needed[:3]:
            growth_areas.append(area)
        
        return growth_areas if growth_areas else ["í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ë©° ì°½ì˜ì„± ê°œë°œì— ì§‘ì¤‘"]



    def comprehensive_writing_analysis(self, text):
        """í†µí•© ê¸€ì“°ê¸° ìˆ˜ì¤€ ì¢…í•© ì§„ë‹¨"""
        
        
        try:
            # 1ë‹¨ê³„: í†µê³„ì  ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„
            step1_result = self._statistical_benchmarking_analysis(text)
            
            # 2ë‹¨ê³„: ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„
            step2_result = self._vocabulary_level_analysis(text)
            
            # 3ë‹¨ê³„: ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
            step3_result = self.analyze_grammar_patterns(text)
            
            # 4ë‹¨ê³„: ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„
            step4_result = self._sentence_similarity_analysis(text)
            
            # 5ë‹¨ê³„: ì¢…í•© ì§„ë‹¨
            step5_result = self._comprehensive_assessment(text, step1_result, step2_result, step3_result, step4_result)
            
            return {
                'step1_stats': step1_result,
                'step2_vocabulary': step2_result,
                'step3_grammar': step3_result,
                'step4_similarity': step4_result,
                'step5_comprehensive': step5_result,
                'overall_score': step5_result['overall_score'],
                'final_level': step5_result['final_level'],
                'improvement_roadmap': step5_result['improvement_roadmap']
            }
            
        except Exception as e:
            return {'error': f"í†µí•© ê¸€ì“°ê¸° ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

    def _statistical_benchmarking_analysis(self, text):
        """1ë‹¨ê³„: í†µê³„ì  ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„"""
        
        
        # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
        EXPERT_BENCHMARKS = {
            "middle_school_excellent": {
                "name": "ğŸ“ ì¤‘í•™ìƒ ìš°ìˆ˜ì‘",
                "noun_ratio": 42.1,
                "verb_ratio": 22.8,
                "adj_ratio": 15.3,
                "complexity_ratio": 58.7,
                "vocabulary_diversity": 0.72,
                "avg_sentence_length": 12.5
            },
            "academic_essay": {
                "name": "ğŸ“š í•™ìˆ  ì—ì„¸ì´",
                "noun_ratio": 45.2,
                "verb_ratio": 18.3,
                "adj_ratio": 12.1,
                "complexity_ratio": 78.5,
                "vocabulary_diversity": 0.85,
                "avg_sentence_length": 16.2
            },
            "creative_writing": {
                "name": "ğŸ¨ ì°½ì˜ì  ê¸€ì“°ê¸°",
                "noun_ratio": 38.7,
                "verb_ratio": 25.4,
                "adj_ratio": 18.2,
                "complexity_ratio": 65.2,
                "vocabulary_diversity": 0.78,
                "avg_sentence_length": 14.1
            }
        }
        
        # ì‚¬ìš©ì í…ìŠ¤íŠ¸ í†µê³„ ê³„ì‚°
        user_stats = self._calculate_text_statistics(text)
        
        # ê° ë²¤ì¹˜ë§ˆí¬ì™€ ë¹„êµ
        benchmark_scores = {}
        for key, benchmark in EXPERT_BENCHMARKS.items():
            similarity_score = self._calculate_similarity_score(user_stats, benchmark)
            benchmark_scores[key] = {
                'benchmark_name': benchmark['name'],
                'similarity_score': similarity_score,
                'individual_scores': self._get_individual_scores(user_stats, benchmark)
            }
        
        # ìµœê³  ìœ ì‚¬ë„ ë²¤ì¹˜ë§ˆí¬ ì„ ì •
        best_match = max(benchmark_scores.items(), key=lambda x: x[1]['similarity_score'])
        
        return {
            'user_statistics': user_stats,
            'benchmark_comparisons': benchmark_scores,
            'best_match': best_match,
            'statistical_score': best_match[1]['similarity_score'],
            'insights': self._generate_statistical_insights(user_stats, EXPERT_BENCHMARKS)
        }

    def _vocabulary_level_analysis(self, text):
        """2ë‹¨ê³„: ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„ (Word Embedding ì‹œë®¬ë ˆì´ì…˜)"""
        
        words = text.lower().split()
        
        # ê³ ê¸‰ ì–´íœ˜ ì‚¬ì „ (ì‹¤ì œë¡œëŠ” Word2Vec/GloVe ì‚¬ìš©)
        ADVANCED_VOCABULARY = {
            # í•™ìˆ ì  ì–´íœ˜
            'academic': ['analyze', 'synthesize', 'evaluate', 'critique', 'demonstrate', 
                        'illustrate', 'establish', 'determine', 'investigate', 'examine',
                        'significant', 'substantial', 'comprehensive', 'fundamental', 'crucial'],
            
            # ê³ ê¸‰ í˜•ìš©ì‚¬
            'descriptive': ['magnificent', 'extraordinary', 'remarkable', 'exceptional', 
                        'profound', 'intricate', 'sophisticated', 'elaborate', 'vivid',
                        'compelling', 'fascinating', 'intriguing', 'captivating'],
            
            # ì—°ê²° ì–´êµ¬
            'transitions': ['furthermore', 'consequently', 'nevertheless', 'moreover', 
                        'therefore', 'however', 'subsequently', 'additionally', 'ultimately',
                        'specifically', 'particularly', 'essentially', 'significantly'],
            
            # ê³ ê¸‰ ë™ì‚¬
            'advanced_verbs': ['enhance', 'facilitate', 'demonstrate', 'implement', 'establish',
                            'contribute', 'emphasize', 'illustrate', 'represent', 'indicate',
                            'reflect', 'reveal', 'suggest', 'imply', 'encompass']
        }
        
        # ì–´íœ˜ ë¶„ì„
        total_words = len(words)
        unique_words = len(set(words))
        vocabulary_diversity = unique_words / total_words if total_words > 0 else 0
        
        # ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©ë¥  ê³„ì‚°
        advanced_count = 0
        category_usage = {}
        
        for category, vocab_list in ADVANCED_VOCABULARY.items():
            category_count = sum(1 for word in words if word in vocab_list)
            category_usage[category] = {
                'count': category_count,
                'ratio': (category_count / total_words * 100) if total_words > 0 else 0
            }
            advanced_count += category_count
        
        advanced_vocabulary_ratio = (advanced_count / total_words * 100) if total_words > 0 else 0
        
        # ì–´íœ˜ ìˆ˜ì¤€ ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
        diversity_score = min(vocabulary_diversity * 100, 50)  # ë‹¤ì–‘ì„± ìµœëŒ€ 50ì 
        advanced_score = min(advanced_vocabulary_ratio * 2, 50)  # ê³ ê¸‰ì–´íœ˜ ìµœëŒ€ 50ì 
        vocabulary_score = diversity_score + advanced_score
        
        # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
        if vocabulary_score >= 80:
            level = "ğŸ† ê³ ê¸‰ (Advanced)"
            level_desc = "í’ë¶€í•˜ê³  ì •êµí•œ ì–´íœ˜ ì‚¬ìš©"
        elif vocabulary_score >= 60:
            level = "ğŸ¥ˆ ì¤‘ê¸‰ (Intermediate)"
            level_desc = "ì ì ˆí•œ ì–´íœ˜ êµ¬ì‚¬ë ¥"
        elif vocabulary_score >= 40:
            level = "ğŸ¥‰ ê¸°ì´ˆ (Basic)"
            level_desc = "ê¸°ë³¸ì ì¸ ì–´íœ˜ ì‚¬ìš©"
        else:
            level = "ğŸ“š ì…ë¬¸ (Beginner)"
            level_desc = "ì–´íœ˜ í™•ì¥ì´ í•„ìš”"
        
        # ì–´íœ˜ ë³µì¡ë„ ë¶„ì„ ì¶”ê°€
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        unique_word_ratio = (unique_words / total_words * 100) if total_words > 0 else 0
        
        # ìˆ˜ì¤€ë³„ ì–´íœ˜ ë¶„ì„
        basic_words = sum(1 for word in words if len(word) <= 4)
        intermediate_words = sum(1 for word in words if 5 <= len(word) <= 7)
        advanced_words = sum(1 for word in words if len(word) > 7)
        academic_words = advanced_count
        
        return {
            'overall_level': level,
            'advanced_vocabulary_ratio': advanced_vocabulary_ratio,
            'academic_vocabulary_score': advanced_score,
            'level_analysis': {
                'basic_words': basic_words,
                'intermediate_words': intermediate_words,
                'advanced_words': advanced_words,
                'academic_words': academic_words
            },
            'complexity_analysis': {
                'avg_word_length': avg_word_length,
                'unique_word_ratio': unique_word_ratio,
                'sophistication_score': vocabulary_score,
                'level_description': level_desc
            },
            'vocabulary_recommendations': self._generate_vocabulary_recommendations(advanced_vocabulary_ratio, vocabulary_score),
            'category_usage': category_usage,
            'vocabulary_diversity': vocabulary_diversity
        }

    def _sentence_similarity_analysis(self, text):
        """3ë‹¨ê³„: ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ (ì „ë¬¸ê°€ ê¸€ê³¼ì˜ ë…¼ë¦¬ì„± ë¹„êµ)"""
        
        try:
            import nltk
            sentences = nltk.sent_tokenize(text)
        except:
            sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        if not sentences:
            return {
                'average_similarity': 0,
                'coherence_score': 0,
                'logical_flow_level': 'Unknown',
                'sentence_pair_analysis': [],
                'topic_consistency': {}
            }
        
        # ë…¼ë¦¬ì  ì—°ê²°ì–´ ë¶„ì„
        logical_connectors = ['however', 'therefore', 'furthermore', 'moreover', 'consequently',
                            'nevertheless', 'additionally', 'specifically', 'ultimately', 'initially',
                            'firstly', 'secondly', 'finally', 'in conclusion', 'for example', 'such as']
        
        transition_words = ['but', 'and', 'or', 'so', 'because', 'since', 'while', 'although', 
                          'unless', 'before', 'after', 'when', 'if', 'thus', 'hence']
        
        # ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¨ì–´ ê¸°ë°˜ Jaccard ìœ ì‚¬ë„)
        sentence_similarities = []
        sentence_pairs = []
        
        for i in range(len(sentences)-1):
            sentence1 = sentences[i].lower()
            sentence2 = sentences[i+1].lower()
            
            words1 = set(word.strip('.,!?;:') for word in sentence1.split() if word.isalpha())
            words2 = set(word.strip('.,!?;:') for word in sentence2.split() if word.isalpha())
            
            if words1 and words2:
                intersection = words1.intersection(words2)
                union = words1.union(words2)
                similarity = len(intersection) / len(union) if union else 0
                sentence_similarities.append(similarity)
                
                if len(sentence_pairs) < 5:  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                    sentence_pairs.append({
                        'similarity': similarity,
                        'sentence1_preview': sentences[i][:100],
                        'sentence2_preview': sentences[i+1][:100]
                    })
        
        # í‰ê·  ìœ ì‚¬ë„
        avg_similarity = sum(sentence_similarities) / len(sentence_similarities) if sentence_similarities else 0
        
        # ë…¼ë¦¬ì  íë¦„ ë¶„ì„
        connector_count = 0
        for sentence in sentences:
            sentence_lower = sentence.lower()
            connector_count += sum(1 for conn in logical_connectors + transition_words if conn in sentence_lower)
        
        connector_ratio = connector_count / len(sentences) if sentences else 0
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        coherence_score = min(100, (avg_similarity * 50) + (connector_ratio * 30) + 20)
        
        # ë…¼ë¦¬ì  íë¦„ ìˆ˜ì¤€ ê²°ì •
        if coherence_score >= 80:
            logical_flow = "ğŸŸ¢ ë§¤ìš° ìš°ìˆ˜"
        elif coherence_score >= 60:
            logical_flow = "ğŸŸ¡ ì–‘í˜¸"
        elif coherence_score >= 40:
            logical_flow = "ğŸŸ  ë³´í†µ"
        else:
            logical_flow = "ğŸ”´ ê°œì„  í•„ìš”"
        
        # ì£¼ì œ ì¼ê´€ì„± ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)
        all_words = []
        for sentence in sentences:
            words = [word.lower().strip('.,!?;:') for word in sentence.split() if word.isalpha() and len(word) > 3]
            all_words.extend(words)
        
        if all_words:
            from collections import Counter
            word_freq = Counter(all_words)
            top_words = word_freq.most_common(5)
            
            # ì£¼ìš” í‚¤ì›Œë“œì˜ ë¶„í¬
            total_occurrences = sum(freq for word, freq in top_words)
            if total_occurrences > 0:
                main_theme_strength = (top_words[0][1] / total_occurrences * 100) if top_words else 0
                topic_drift_score = max(0, 100 - main_theme_strength)
            else:
                main_theme_strength = 0
                topic_drift_score = 100
        else:
            main_theme_strength = 0
            topic_drift_score = 100
        
        return {
            'average_similarity': avg_similarity,
            'coherence_score': coherence_score,
            'logical_flow_level': logical_flow,
            'sentence_pair_analysis': sentence_pairs,
            'topic_consistency': {
                'topic_drift_score': topic_drift_score,
                'main_theme_strength': main_theme_strength
            }
        }
    
    def _generate_vocabulary_recommendations(self, advanced_ratio, vocab_score):
        """ì–´íœ˜ ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if advanced_ratio < 10:
            recommendations.append("ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©ì„ ëŠ˜ë ¤ë³´ì„¸ìš” (í˜„ì¬ {:.1f}%)".format(advanced_ratio))
            recommendations.append("í•™ìˆ ì  ë™ì‚¬(analyze, evaluate, demonstrate) í™œìš© ì—°ìŠµ")
        
        if vocab_score < 60:
            recommendations.append("ì–´íœ˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë™ì˜ì–´ í™œìš©ì„ ëŠ˜ë ¤ë³´ì„¸ìš”")
            recommendations.append("ì „ë¬¸ ë¶„ì•¼ë³„ ì–´íœ˜ í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤")
        
        if advanced_ratio > 20:
            recommendations.append("ìš°ìˆ˜í•œ ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©ë ¥ì„ ë³´ì…ë‹ˆë‹¤")
        
        return recommendations[:3] if recommendations else ["í˜„ì¬ ì–´íœ˜ ìˆ˜ì¤€ì´ ì ì ˆí•©ë‹ˆë‹¤"]

    def _comprehensive_assessment(self, text, step1_result, step2_result, step3_result, step4_result):
        """5ë‹¨ê³„: ì¢…í•© ì§„ë‹¨ ë° ê°œì„  ë¡œë“œë§µ"""
        
        # ê° ë‹¨ê³„ë³„ ì ìˆ˜ (100ì  ë§Œì ìœ¼ë¡œ ì •ê·œí™”)
        statistical_score = step1_result.get('statistical_score', 0)
        vocabulary_score = step2_result.get('complexity_analysis', {}).get('sophistication_score', 0)
        grammar_score = step3_result.get('grammar_score', 100)  # ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ë²• ì ìˆ˜
        similarity_score = step4_result.get('coherence_score', 0)
        
        # ê°€ì¤‘ì¹˜ ì ìš© ì¢…í•© ì ìˆ˜ (4ë‹¨ê³„ë¡œ í™•ì¥)
        overall_score = (
            statistical_score * 0.25 +  # í†µê³„ì  íŠ¹ì„± 25%
            vocabulary_score * 0.25 +   # ì–´íœ˜ ìˆ˜ì¤€ 25%
            grammar_score * 0.25 +      # ë¬¸ë²• ì •í™•ì„± 25%
            similarity_score * 0.25     # ë…¼ë¦¬ì  êµ¬ì„± 25%
        )
        
        # ìµœì¢… ë“±ê¸‰ íŒì •
        if overall_score >= 85:
            final_level = "ğŸ† ìš°ìˆ˜ (Excellent)"
            level_desc = "ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë›°ì–´ë‚œ ê¸€ì“°ê¸° ì‹¤ë ¥"
            level_color = "success"
        elif overall_score >= 70:
            final_level = "ğŸ¥ˆ ì–‘í˜¸ (Good)"
            level_desc = "ìƒê¸‰ì ìˆ˜ì¤€ì˜ ì•ˆì •ì ì¸ ê¸€ì“°ê¸°"
            level_color = "info"
        elif overall_score >= 55:
            final_level = "ğŸ¥‰ ë³´í†µ (Average)"
            level_desc = "ì¤‘ê¸‰ì ìˆ˜ì¤€, ê¾¸ì¤€í•œ ë°œì „ì´ í•„ìš”"
            level_color = "warning"
        else:
            final_level = "ğŸ“š í–¥ìƒ í•„ìš” (Developing)"
            level_desc = "ê¸°ì´ˆ ì‹¤ë ¥ í–¥ìƒì— ì§‘ì¤‘ í•„ìš”"
            level_color = "error"
        
        # ë§ì¶¤í˜• ê°œì„  ë¡œë“œë§µ ìƒì„±
        improvement_roadmap = self._generate_improvement_roadmap(
            statistical_score, vocabulary_score, similarity_score, overall_score
        )
        
        # ê°•ì ê³¼ ì•½ì  ë¶„ì„
        strengths = []
        weaknesses = []
        
        if statistical_score >= 70:
            strengths.append("í†µê³„ì  ê¸€ì“°ê¸° íŒ¨í„´ì´ ìš°ìˆ˜í•¨")
        else:
            weaknesses.append("í’ˆì‚¬ ì‚¬ìš©ê³¼ ë¬¸ì¥ êµ¬ì„±ì˜ ê· í˜• ê°œì„  í•„ìš”")
        
        if vocabulary_score >= 70:
            strengths.append("ì–´íœ˜ ì‚¬ìš©ì´ í’ë¶€í•˜ê³  ë‹¤ì–‘í•¨")
        else:
            weaknesses.append("ì–´íœ˜ ë‹¤ì–‘ì„±ê³¼ ê³ ê¸‰ í‘œí˜„ í™•ì¥ í•„ìš”")
        
        if similarity_score >= 70:
            strengths.append("ë…¼ë¦¬ì ì´ê³  ì¼ê´€ì„± ìˆëŠ” êµ¬ì„±")
        else:
            weaknesses.append("ë¬¸ì¥ ê°„ ì—°ê²°ì„±ê³¼ ë…¼ë¦¬ì  íë¦„ ê°œì„  í•„ìš”")
        
        return {
            'overall_score': round(overall_score, 1),
            'final_level': final_level,
            'level_description': level_desc,
            'level_color': level_color,
            'component_scores': {
                'statistical': round(statistical_score, 1),
                'vocabulary': round(vocabulary_score, 1),
                'similarity': round(similarity_score, 1)
            },
            'strengths': strengths,
            'weaknesses': weaknesses,
            'improvement_roadmap': improvement_roadmap
        }

    # ë³´ì¡° ë©”ì„œë“œë“¤
    def _calculate_text_statistics(self, text):
        """ì˜ì–´ í…ìŠ¤íŠ¸ í†µê³„ ê³„ì‚°"""
        
        if not text or not text.strip():
            return {
                'total_words': 0,
                'total_sentences': 0, 
                'unique_words': 0,
                'vocabulary_diversity': 0,
                'avg_sentence_length': 0,
                'noun_ratio': 0,
                'verb_ratio': 0,
                'adj_ratio': 0,
                'complexity_ratio': 0
            }
        
        try:
            import nltk
            
            # ë¬¸ì¥ê³¼ ë‹¨ì–´ í† í°í™”
            sentences = nltk.sent_tokenize(text)
            words = nltk.word_tokenize(text.lower())
            
            # ì•ŒíŒŒë²³ ë‹¨ì–´ë§Œ í•„í„°ë§ (ê¸¸ì´ 2 ì´ìƒ)
            words = [word for word in words if word.isalpha() and len(word) >= 2]
            total_words = len(words)
            
            
            if total_words == 0:
                return {
                    'total_words': 0,
                    'total_sentences': 0,
                    'unique_words': 0,
                    'vocabulary_diversity': 0,
                    'avg_sentence_length': 0,
                    'noun_ratio': 0,
                    'verb_ratio': 0,
                    'adj_ratio': 0,
                    'complexity_ratio': 0
                }
            
            # POS íƒœê¹…
            pos_tagged = nltk.pos_tag(words)
            
            # í’ˆì‚¬ë³„ ê°œìˆ˜ ê³„ì‚°
            noun_count = sum(1 for word, pos in pos_tagged if pos.startswith('NN'))
            verb_count = sum(1 for word, pos in pos_tagged if pos.startswith('VB'))
            adj_count = sum(1 for word, pos in pos_tagged if pos.startswith('JJ'))
            
            # ë³µì¡ë„ ê³„ì‚° (ë³µí•©ë¬¸/ë³µë¬¸ì˜ ë¹„ìœ¨ ì¶”ì •)
            complex_indicators = sum(1 for word in words if word in ['however', 'therefore', 'furthermore', 'although', 'because', 'since', 'while', 'whereas', 'unless', 'though'])
            complexity_ratio = min(100, (complex_indicators / len(sentences)) * 50) if sentences else 0
            
            unique_words_count = len(set(words))
            diversity = unique_words_count / total_words
            
            return {
                'total_words': total_words,
                'total_sentences': len(sentences),
                'unique_words': unique_words_count,
                'vocabulary_diversity': diversity,
                'avg_sentence_length': total_words / len(sentences) if sentences else 0,
                'noun_ratio': (noun_count / total_words * 100),
                'verb_ratio': (verb_count / total_words * 100),
                'adj_ratio': (adj_count / total_words * 100),
                'complexity_ratio': complexity_ratio
            }
            
        except Exception as e:
            # NLTKê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶„ì„
            import re
            
            # ë‹¨ì–´ ì¶”ì¶œ (êµ¬ë‘ì  ì œê±°)
            words = re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())
            sentences = [s.strip() for s in text.split('.') if s.strip()]
            total_words = len(words)
            
            if total_words == 0:
                return {
                    'total_words': 0,
                    'total_sentences': 0,
                    'unique_words': 0,
                    'vocabulary_diversity': 0,
                    'avg_sentence_length': 0,
                    'noun_ratio': 0,
                    'verb_ratio': 0,
                    'adj_ratio': 0,
                    'complexity_ratio': 0
                }
            
            # ê°„ë‹¨í•œ ì˜ì–´ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
            noun_count = sum(1 for word in words if word.endswith(('tion', 'sion', 'ment', 'ness', 'ity', 'ty', 'ence', 'ance')))
            verb_count = sum(1 for word in words if word.endswith(('ed', 'ing', 'ize', 'ise', 'ate')))
            adj_count = sum(1 for word in words if word.endswith(('ful', 'less', 'ous', 'ive', 'able', 'ible', 'al', 'ic')))
            
            unique_count = len(set(words))
            diversity = unique_count / total_words
            
            return {
                'total_words': total_words,
                'total_sentences': len(sentences),
                'unique_words': unique_count,
                'vocabulary_diversity': diversity,
                'avg_sentence_length': total_words / len(sentences) if sentences else 0,
                'noun_ratio': (noun_count / total_words * 100),
                'verb_ratio': (verb_count / total_words * 100),
                'adj_ratio': (adj_count / total_words * 100),
                'complexity_ratio': 50  # ê¸°ë³¸ê°’
            }

    def _calculate_similarity_score(self, user_stats, benchmark):
        """ë²¤ì¹˜ë§ˆí¬ì™€ì˜ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°"""
        
        metrics = ['noun_ratio', 'verb_ratio', 'adj_ratio', 'vocabulary_diversity', 'avg_sentence_length']
        
        total_score = 0
        valid_metrics = 0
        
        for metric in metrics:
            user_value = user_stats.get(metric, 0)
            benchmark_value = benchmark.get(metric, 0)
            
            if benchmark_value > 0:
                # ì°¨ì´ë¥¼ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°
                difference = abs(user_value - benchmark_value) / benchmark_value
                # ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ìµœëŒ€ 100ì )
                score = max(0, 100 - (difference * 100))
                total_score += score
                valid_metrics += 1
        
        return total_score / valid_metrics if valid_metrics > 0 else 0

    def _get_individual_scores(self, user_stats, benchmark):
        """ê°œë³„ ì§€í‘œë³„ ì ìˆ˜"""
        
        metrics = {
            'noun_ratio': 'noun_score',
            'verb_ratio': 'verb_score', 
            'adj_ratio': 'adj_score',
            'vocabulary_diversity': 'diversity_score',
            'avg_sentence_length': 'length_score',
            'complexity_ratio': 'complexity_score'
        }
        
        scores = {}
        
        for metric, score_key in metrics.items():
            user_value = user_stats.get(metric, 0)
            benchmark_value = benchmark.get(metric, 0)
            
            if benchmark_value > 0:
                difference = abs(user_value - benchmark_value) / benchmark_value
                score = max(0, 100 - (difference * 100))
                scores[score_key] = score
            else:
                scores[score_key] = 50
        
        return scores

    def _generate_statistical_insights(self, user_stats, benchmarks):
        """í†µê³„ì  ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        insights = []
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        avg_length = user_stats.get('avg_sentence_length', 0)
        if avg_length > 15:
            insights.append("ë¬¸ì¥ì´ ê¸¸ì–´ ë³µì¡í•œ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤")
        elif avg_length < 8:
            insights.append("ë¬¸ì¥ì´ ì§§ì•„ ê°„ê²°í•œ í‘œí˜„ì„ ì„ í˜¸í•©ë‹ˆë‹¤")
        else:
            insights.append("ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ë¡œ ì½ê¸° ì¢‹ì€ ê¸€ì…ë‹ˆë‹¤")
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„
        diversity = user_stats.get('vocabulary_diversity', 0)
        if diversity > 0.7:
            insights.append("ì–´íœ˜ ì‚¬ìš©ì´ ë§¤ìš° ë‹¤ì–‘í•©ë‹ˆë‹¤")
        elif diversity < 0.5:
            insights.append("ì–´íœ˜ ë‹¤ì–‘ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        return insights

    def _generate_vocabulary_insights(self, category_usage, advanced_ratio):
        """ì–´íœ˜ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        insights = []
        
        if advanced_ratio > 5:
            insights.append("ê³ ê¸‰ ì–´íœ˜ë¥¼ ì ì ˆíˆ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤")
        elif advanced_ratio < 2:
            insights.append("ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©ì„ ëŠ˜ë ¤ë³´ì„¸ìš”")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¸ì‚¬ì´íŠ¸
        for category, usage in category_usage.items():
            if usage['ratio'] > 2:
                if category == 'academic':
                    insights.append("í•™ìˆ ì  í‘œí˜„ì´ í’ë¶€í•©ë‹ˆë‹¤")
                elif category == 'transitions':
                    insights.append("ë…¼ë¦¬ì  ì—°ê²°ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤")
        
        return insights

    def _generate_similarity_insights(self, sentence_analysis, avg_coherence):
        """ë¬¸ì¥ ìœ ì‚¬ë„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        insights = []
        
        if avg_coherence > 70:
            insights.append("ë¬¸ì¥ êµ¬ì„±ì´ ë…¼ë¦¬ì ì´ê³  ì¼ê´€ì„±ì´ ìˆìŠµë‹ˆë‹¤")
        elif avg_coherence < 50:
            insights.append("ë¬¸ì¥ ê°„ ì—°ê²°ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ë…¼ë¦¬ì  ì—°ê²°ì–´ ì‚¬ìš© ë¶„ì„
        total_connectors = sum(s.get('logical_connectors', 0) for s in sentence_analysis)
        if total_connectors > len(sentence_analysis):
            insights.append("ë…¼ë¦¬ì  ì—°ê²°ì–´ë¥¼ ì˜ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤")
        else:
            insights.append("ë…¼ë¦¬ì  ì—°ê²°ì–´ ì‚¬ìš©ì„ ëŠ˜ë ¤ë³´ì„¸ìš”")
        
        return insights

    def _generate_improvement_roadmap(self, stat_score, vocab_score, sim_score, overall_score):
        """ë§ì¶¤í˜• ê°œì„  ë¡œë“œë§µ ìƒì„±"""
        
        roadmap = {
            'immediate_actions': [],
            'short_term_goals': [],
            'long_term_goals': []
        }
        
        # ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì‚¬í•­
        if stat_score < 60:
            roadmap['immediate_actions'].append({
                'action': 'ë¬¸ì¥ ê¸¸ì´ ì¡°ì ˆí•˜ê¸°',
                'description': 'ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë‚˜ëˆ„ê³ , ì§§ì€ ë¬¸ì¥ì€ ì—°ê²°í•´ë³´ì„¸ìš”',
                'priority': 'high'
            })
        
        if vocab_score < 60:
            roadmap['immediate_actions'].append({
                'action': 'ê³ ê¸‰ ì–´íœ˜ í•™ìŠµ',
                'description': 'ë§¤ì¼ ìƒˆë¡œìš´ ì–´íœ˜ 3ê°œì”© í•™ìŠµí•˜ê³  ê¸€ì— ì ìš©í•´ë³´ì„¸ìš”',
                'priority': 'high'
            })
        
        if sim_score < 60:
            roadmap['immediate_actions'].append({
                'action': 'ë…¼ë¦¬ì  ì—°ê²°ì–´ í™œìš©',
                'description': '"ê·¸ëŸ¬ë¯€ë¡œ", "ë”°ë¼ì„œ", "í•˜ì§€ë§Œ" ë“±ì„ í™œìš©í•´ ë¬¸ì¥ì„ ì—°ê²°í•´ë³´ì„¸ìš”',
                'priority': 'high'
            })
        
        # ë‹¨ê¸° ëª©í‘œ (1-3ê°œì›”)
        if overall_score < 70:
            roadmap['short_term_goals'].append('ë§¤ì£¼ ì—ì„¸ì´ 1í¸ì”© ì‘ì„±í•˜ë©° ì²´ê³„ì  ì—°ìŠµ')
            roadmap['short_term_goals'].append('ë‹¤ì–‘í•œ ì¥ë¥´ì˜ ìš°ìˆ˜ì‘ ì½ê¸° ë° ë¶„ì„')
            roadmap['short_term_goals'].append('ê¸€ì“°ê¸° í”¼ë“œë°± ë°›ê¸° ë° ê°œì„ ì  ì ìš©')
        
        # ì¥ê¸° ëª©í‘œ (3-6ê°œì›”)
        roadmap['long_term_goals'].append('ê°œì¸ë§Œì˜ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ í™•ë¦½')
        roadmap['long_term_goals'].append('ì°½ì˜ì ì´ê³  ë…ì°½ì ì¸ í‘œí˜„ë ¥ ê°œë°œ')
        roadmap['long_term_goals'].append('ë³µí•©ì  ì£¼ì œì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ê¸€ì“°ê¸°')
        
        return roadmap

    def analyze_grammar_patterns(self, text):
        """ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        import re
        import nltk
        from nltk.tokenize import sent_tokenize, word_tokenize
        from nltk.tag import pos_tag
        
        try:
            sentences = sent_tokenize(text)
            grammar_analysis = {
                'total_sentences': len(sentences),
                'potential_errors': [],
                'error_patterns': {},
                'error_count_by_type': {},
                'sentences_with_issues': [],
                'grammar_score': 0
            }
            
            error_count = 0
            total_issues = 0
            
            for i, sentence in enumerate(sentences):
                sentence_issues = []
                words = word_tokenize(sentence)
                pos_tags = pos_tag(words)
                
                # 1. ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜ ê²€ì‚¬ (ê°„ë‹¨í•œ íŒ¨í„´)
                subject_verb_issues = self._check_subject_verb_agreement(sentence, pos_tags)
                if subject_verb_issues:
                    sentence_issues.extend(subject_verb_issues)
                    error_count += len(subject_verb_issues)
                
                # 2. ì‹œì œ ì¼ê´€ì„± ê²€ì‚¬
                tense_issues = self._check_tense_consistency(sentence, pos_tags)
                if tense_issues:
                    sentence_issues.extend(tense_issues)
                    error_count += len(tense_issues)
                
                # 3. ê´€ì‚¬ ì‚¬ìš© ê²€ì‚¬ (ê¸°ì´ˆ íŒ¨í„´)
                article_issues = self._check_article_usage(sentence, pos_tags)
                if article_issues:
                    sentence_issues.extend(article_issues)
                    error_count += len(article_issues)
                
                # 4. ì „ì¹˜ì‚¬ ì‚¬ìš© ê²€ì‚¬
                preposition_issues = self._check_preposition_patterns(sentence, pos_tags)
                if preposition_issues:
                    sentence_issues.extend(preposition_issues)
                    error_count += len(preposition_issues)
                
                # 5. ë¬¸ì¥ êµ¬ì¡° ê²€ì‚¬
                structure_issues = self._check_sentence_structure(sentence, pos_tags)
                if structure_issues:
                    sentence_issues.extend(structure_issues)
                    error_count += len(structure_issues)
                
                if sentence_issues:
                    grammar_analysis['sentences_with_issues'].append({
                        'sentence_number': i + 1,
                        'sentence': sentence,
                        'issues': sentence_issues
                    })
                    total_issues += len(sentence_issues)
            
            # ì˜¤ë¥˜ íŒ¨í„´ë³„ ë¶„ë¥˜
            for sentence_data in grammar_analysis['sentences_with_issues']:
                for issue in sentence_data['issues']:
                    error_type = issue['type']
                    if error_type not in grammar_analysis['error_patterns']:
                        grammar_analysis['error_patterns'][error_type] = []
                    grammar_analysis['error_patterns'][error_type].append({
                        'sentence': sentence_data['sentence'],
                        'description': issue['description'],
                        'suggestion': issue.get('suggestion', '')
                    })
                    
                    # ì˜¤ë¥˜ ìœ í˜•ë³„ ì¹´ìš´íŠ¸
                    if error_type not in grammar_analysis['error_count_by_type']:
                        grammar_analysis['error_count_by_type'][error_type] = 0
                    grammar_analysis['error_count_by_type'][error_type] += 1
            
            # ë¬¸ë²• ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
            if len(sentences) > 0:
                error_rate = total_issues / len(sentences)
                grammar_analysis['grammar_score'] = max(0, 100 - (error_rate * 20))
            else:
                grammar_analysis['grammar_score'] = 100
            
            # ì£¼ìš” ê°œì„  ì˜ì—­ ì‹ë³„
            grammar_analysis['improvement_areas'] = self._identify_improvement_areas(grammar_analysis['error_count_by_type'])
            
            return grammar_analysis
            
        except Exception as e:
            return {
                'total_sentences': 0,
                'potential_errors': [],
                'error_patterns': {},
                'error_count_by_type': {},
                'sentences_with_issues': [],
                'grammar_score': 100,
                'improvement_areas': [],
                'error': f"ë¬¸ë²• ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    def _check_subject_verb_agreement(self, sentence, pos_tags):
        """ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜ ê²€ì‚¬"""
        issues = []
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ê²€ì‚¬ (I/He/She + ë™ì‚¬ í˜•íƒœ)
        sentence_lower = sentence.lower()
        
        # ìì£¼ í‹€ë¦¬ëŠ” íŒ¨í„´ë“¤
        error_patterns = [
            (r'\bi am\s+\w+ing\b', 'í˜„ì¬ì§„í–‰í˜•ì´ ë§ë‚˜ìš”?'),
            (r'\bhe are\b|\bshe are\b', 'He/SheëŠ” isë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤'),
            (r'\bthey is\b', 'TheyëŠ” areë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤'),
            (r'\bi are\b', 'IëŠ” amì„ ì¨ì•¼ í•©ë‹ˆë‹¤')
        ]
        
        for pattern, description in error_patterns:
            if re.search(pattern, sentence_lower):
                issues.append({
                    'type': 'subject_verb_agreement',
                    'description': description,
                    'suggestion': 'ì£¼ì–´ì™€ ë™ì‚¬ì˜ ìˆ˜ë¥¼ ë§ì¶°ë³´ì„¸ìš”'
                })
        
        return issues
    
    def _check_tense_consistency(self, sentence, pos_tags):
        """ì‹œì œ ì¼ê´€ì„± ê²€ì‚¬"""
        issues = []
        
        # ê³¼ê±°í˜•ê³¼ í˜„ì¬í˜•ì´ í˜¼ì¬ëœ ê²½ìš° ì²´í¬
        past_verbs = [word for word, pos in pos_tags if pos in ['VBD']]  # ê³¼ê±°í˜•
        present_verbs = [word for word, pos in pos_tags if pos in ['VBZ', 'VBP']]  # í˜„ì¬í˜•
        
        if len(past_verbs) > 0 and len(present_verbs) > 0:
            issues.append({
                'type': 'tense_consistency',
                'description': 'í•œ ë¬¸ì¥ì—ì„œ ê³¼ê±°í˜•ê³¼ í˜„ì¬í˜•ì´ í˜¼ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤',
                'suggestion': 'ë¬¸ì¥ ì „ì²´ì˜ ì‹œì œë¥¼ ì¼ì¹˜ì‹œì¼œë³´ì„¸ìš”'
            })
        
        return issues
    
    def _check_article_usage(self, sentence, pos_tags):
        """ê´€ì‚¬ ì‚¬ìš© ê²€ì‚¬"""
        issues = []
        
        # ê´€ì‚¬ê°€ ë¹ ì§„ íŒ¨í„´ (ê°„ë‹¨í•œ ì²´í¬)
        sentence_lower = sentence.lower()
        
        # ìì£¼ ì‹¤ìˆ˜í•˜ëŠ” íŒ¨í„´
        if re.search(r'\b(go to school|at home|in bed)\b', sentence_lower):
            # ì´ëŸ° ê²½ìš°ëŠ” ê´€ì‚¬ê°€ ì—†ëŠ” ê²Œ ë§ìŒ
            pass
        elif re.search(r'\b[a-z]+ (cat|dog|book|house|car)\b', sentence_lower):
            if not re.search(r'\b(a|an|the) (cat|dog|book|house|car)\b', sentence_lower):
                issues.append({
                    'type': 'article_usage',
                    'description': 'ì…€ ìˆ˜ ìˆëŠ” ëª…ì‚¬ ì•ì—ëŠ” ê´€ì‚¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤',
                    'suggestion': 'a/an/the ì¤‘ ì ì ˆí•œ ê´€ì‚¬ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”'
                })
        
        return issues
    
    def _check_preposition_patterns(self, sentence, pos_tags):
        """ì „ì¹˜ì‚¬ ì‚¬ìš© ê²€ì‚¬"""
        issues = []
        
        sentence_lower = sentence.lower()
        
        # ìì£¼ í‹€ë¦¬ëŠ” ì „ì¹˜ì‚¬ íŒ¨í„´
        error_patterns = [
            (r'\bin the morning\b.*\bin the afternoon\b', 'ì‹œê°„ ì „ì¹˜ì‚¬ ì‚¬ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”'),
            (r'\bgo to home\b', 'go homeì´ ë§ìŠµë‹ˆë‹¤ (to ë¶ˆí•„ìš”)'),
            (r'\blisten music\b', 'listen to musicì´ ë§ìŠµë‹ˆë‹¤'),
        ]
        
        for pattern, description in error_patterns:
            if re.search(pattern, sentence_lower):
                issues.append({
                    'type': 'preposition_usage',
                    'description': description,
                    'suggestion': 'ì „ì¹˜ì‚¬ ì‚¬ìš© ê·œì¹™ì„ í™•ì¸í•´ë³´ì„¸ìš”'
                })
        
        return issues
    
    def _check_sentence_structure(self, sentence, pos_tags):
        """ë¬¸ì¥ êµ¬ì¡° ê²€ì‚¬"""
        issues = []
        
        # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§§ì€ ê²½ìš°
        word_count = len([word for word, pos in pos_tags if pos not in ['.',  ',', ':', ';']])
        
        if word_count > 25:
            issues.append({
                'type': 'sentence_length',
                'description': 'ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì–´ ì½ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤',
                'suggestion': 'ë¬¸ì¥ì„ ë‚˜ëˆ„ì–´ë³´ì„¸ìš”'
            })
        elif word_count < 3:
            issues.append({
                'type': 'sentence_length', 
                'description': 'ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤',
                'suggestion': 'ì¢€ ë” ìì„¸í•œ ì„¤ëª…ì„ ì¶”ê°€í•´ë³´ì„¸ìš”'
            })
        
        # ì£¼ì–´ë‚˜ ë™ì‚¬ê°€ ì—†ëŠ” ê²½ìš° (ê°„ë‹¨ ì²´í¬)
        has_subject = any(pos in ['PRP', 'NN', 'NNS', 'NNP', 'NNPS'] for word, pos in pos_tags)
        has_verb = any(pos.startswith('VB') for word, pos in pos_tags)
        
        if not has_subject:
            issues.append({
                'type': 'sentence_structure',
                'description': 'ì£¼ì–´ê°€ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤',
                'suggestion': 'ë¬¸ì¥ì˜ ì£¼ì–´ë¥¼ ëª…í™•íˆ í•´ë³´ì„¸ìš”'
            })
        
        if not has_verb:
            issues.append({
                'type': 'sentence_structure',
                'description': 'ë™ì‚¬ê°€ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤',
                'suggestion': 'ë¬¸ì¥ì— ë™ì‚¬ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”'
            })
        
        return issues
    
    def _identify_improvement_areas(self, error_count_by_type):
        """ì£¼ìš” ê°œì„  ì˜ì—­ ì‹ë³„"""
        if not error_count_by_type:
            return ['ë¬¸ë²• ì‚¬ìš©ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤']
        
        # ê°€ì¥ ë§ì´ ë°œìƒí•œ ì˜¤ë¥˜ ìœ í˜• ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_errors = sorted(error_count_by_type.items(), key=lambda x: x[1], reverse=True)
        
        improvement_map = {
            'subject_verb_agreement': 'ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤',
            'tense_consistency': 'ì‹œì œ ì¼ê´€ì„± ìœ ì§€ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤',
            'article_usage': 'ê´€ì‚¬(a, an, the) ì‚¬ìš©ë²• í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤',
            'preposition_usage': 'ì „ì¹˜ì‚¬ ì‚¬ìš©ë²• ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤',
            'sentence_structure': 'ë¬¸ì¥ êµ¬ì¡° ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤',
            'sentence_length': 'ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ ì¡°ì ˆì´ í•„ìš”í•©ë‹ˆë‹¤'
        }
        
        areas = []
        for error_type, count in sorted_errors[:3]:  # ìƒìœ„ 3ê°œë§Œ
            if error_type in improvement_map:
                areas.append(improvement_map[error_type])
        
        return areas if areas else ['ì „ë°˜ì ì¸ ë¬¸ë²• ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤']

    def educational_sentiment_analysis_step4_emotions(self, text):
        """4ë‹¨ê³„: ë‹¤ì¤‘ ê°ì„± ë¶„ì„ - 8ê°€ì§€ ê¸°ë³¸ ê°ì • íƒì§€"""
        if not text or pd.isna(text):
            return {}
        
        try:
            # ê°ì •ë³„ í‚¤ì›Œë“œ ì‚¬ì „ (ì¤‘í•™ìƒ ìˆ˜ì¤€ì˜ ì˜ì–´ ë‹¨ì–´)
            emotion_keywords = {
                'ê¸°ì¨ (Joy)': {
                    'keywords': ['happy', 'joy', 'excited', 'wonderful', 'amazing', 'great', 'awesome', 
                                'fantastic', 'excellent', 'love', 'enjoy', 'smile', 'laugh', 'fun', 'cheerful'],
                    'emoji': 'ğŸ˜Š',
                    'color': '#FFD700'
                },
                'ìŠ¬í”” (Sadness)': {
                    'keywords': ['sad', 'cry', 'tears', 'disappointed', 'sorry', 'hurt', 'pain', 
                                'lonely', 'depressed', 'upset', 'broken', 'miss', 'lost', 'tragic'],
                    'emoji': 'ğŸ˜¢',
                    'color': '#4169E1'
                },
                'ë¶„ë…¸ (Anger)': {
                    'keywords': ['angry', 'mad', 'hate', 'furious', 'annoyed', 'frustrated', 'rage', 
                                'irritated', 'upset', 'disgusted', 'terrible', 'awful', 'stupid', 'worst'],
                    'emoji': 'ğŸ˜ ',
                    'color': '#DC143C'
                },
                'ë‘ë ¤ì›€ (Fear)': {
                    'keywords': ['scared', 'afraid', 'fear', 'worried', 'nervous', 'anxious', 'panic', 
                                'terrified', 'frightened', 'concerned', 'stress', 'dangerous', 'risky'],
                    'emoji': 'ğŸ˜¨',
                    'color': '#9932CC'
                },
                'ë†€ëŒ (Surprise)': {
                    'keywords': ['surprised', 'shocked', 'amazed', 'incredible', 'unexpected', 'sudden', 
                                'wow', 'unbelievable', 'astonishing', 'remarkable', 'stunning'],
                    'emoji': 'ğŸ˜²',
                    'color': '#FF69B4'
                },
                'í˜ì˜¤ (Disgust)': {
                    'keywords': ['disgusting', 'gross', 'yuck', 'nasty', 'horrible', 'disgusted', 
                                'revolting', 'sickening', 'repulsive', 'unpleasant'],
                    'emoji': 'ğŸ¤¢',
                    'color': '#8B4513'
                },
                'ì‹ ë¢° (Trust)': {
                    'keywords': ['trust', 'believe', 'reliable', 'honest', 'faithful', 'loyal', 
                                'dependable', 'confident', 'secure', 'certain', 'sure', 'respect'],
                    'emoji': 'ğŸ¤',
                    'color': '#228B22'
                },
                'ê¸°ëŒ€ (Anticipation)': {
                    'keywords': ['hope', 'expect', 'anticipate', 'excited', 'eager', 'looking forward', 
                                'ready', 'prepared', 'future', 'plan', 'dream', 'wish', 'goal'],
                    'emoji': 'ğŸŒŸ',
                    'color': '#FF8C00'
                }
            }
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_text = self.extract_essay_content(text)
            if not cleaned_text:
                return {}
            
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  ì†Œë¬¸ì ë³€í™˜
            import re
            words = re.findall(r'\b[a-zA-Z]+\b', cleaned_text.lower())
            
            # ê° ê°ì •ë³„ ì ìˆ˜ ê³„ì‚°
            emotion_scores = {}
            emotion_details = {}
            total_emotional_words = 0
            
            for emotion, data in emotion_keywords.items():
                keywords = data['keywords']
                found_words = [word for word in words if word in keywords]
                score = len(found_words)
                
                emotion_scores[emotion] = score
                emotion_details[emotion] = {
                    'score': score,
                    'found_words': found_words[:10],  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì €ì¥
                    'emoji': data['emoji'],
                    'color': data['color']
                }
                total_emotional_words += score
            
            # ì£¼ë„ì  ê°ì • ì°¾ê¸° (ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê°ì •)
            if total_emotional_words > 0:
                dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])
                dominant_name = dominant_emotion[0]
                dominant_score = dominant_emotion[1]
                
                # ê°ì • ê°•ë„ ê³„ì‚° (ì „ì²´ ë‹¨ì–´ ëŒ€ë¹„ ê°ì • ë‹¨ì–´ ë¹„ìœ¨)
                emotion_intensity = (total_emotional_words / len(words)) * 100 if words else 0
                
                # ê°ì • ë‹¤ì–‘ì„± ê³„ì‚° (0ì´ ì•„ë‹Œ ê°ì • ì¢…ë¥˜ ìˆ˜)
                emotion_variety = len([score for score in emotion_scores.values() if score > 0])
                
            else:
                dominant_name = "ì¤‘ë¦½ (Neutral)"
                dominant_score = 0
                emotion_intensity = 0
                emotion_variety = 0
                emotion_details["ì¤‘ë¦½ (Neutral)"] = {
                    'score': 0,
                    'found_words': [],
                    'emoji': 'ğŸ˜',
                    'color': '#808080'
                }
            
            # ë¬¸ì¥ë³„ ê°ì • ë¶„ì„ (ì²˜ìŒ 3ë¬¸ì¥)
            sentences = [s.strip() for s in cleaned_text.split('.') if s.strip()]
            sentence_emotions = []
            
            for i, sentence in enumerate(sentences[:3]):
                sentence_words = re.findall(r'\b[a-zA-Z]+\b', sentence.lower())
                sentence_emotion_scores = {}
                
                for emotion, data in emotion_keywords.items():
                    keywords = data['keywords']
                    found = [word for word in sentence_words if word in keywords]
                    sentence_emotion_scores[emotion] = len(found)
                
                if any(score > 0 for score in sentence_emotion_scores.values()):
                    sentence_dominant = max(sentence_emotion_scores.items(), key=lambda x: x[1])
                    sentence_emotions.append({
                        'sentence': sentence[:80] + "..." if len(sentence) > 80 else sentence,
                        'emotion': sentence_dominant[0],
                        'score': sentence_dominant[1],
                        'emoji': emotion_keywords[sentence_dominant[0]]['emoji']
                    })
                else:
                    sentence_emotions.append({
                        'sentence': sentence[:80] + "..." if len(sentence) > 80 else sentence,
                        'emotion': 'ì¤‘ë¦½ (Neutral)',
                        'score': 0,
                        'emoji': 'ğŸ˜'
                    })
            
            return {
                'method': 'ë‹¤ì¤‘ ê°ì„± ë¶„ì„ (8ê°€ì§€ ê¸°ë³¸ ê°ì •)',
                'dominant_emotion': dominant_name,
                'dominant_score': dominant_score,
                'dominant_emoji': emotion_details.get(dominant_name, {}).get('emoji', 'ğŸ˜'),
                'emotion_intensity': round(emotion_intensity, 2),
                'emotion_variety': emotion_variety,
                'total_emotional_words': total_emotional_words,
                'total_words': len(words),
                'emotion_scores': emotion_scores,
                'emotion_details': emotion_details,
                'sentence_emotions': sentence_emotions,
                'interpretation': self._interpret_multi_emotions(dominant_name, emotion_intensity, emotion_variety)
            }
            
        except Exception as e:
            return {
                'error': f'ë‹¤ì¤‘ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'dominant_emotion': 'ë¶„ì„ ë¶ˆê°€',
                'dominant_emoji': 'â“'
            }
    
    def _interpret_multi_emotions(self, dominant_emotion, intensity, variety):
        """ë‹¤ì¤‘ ê°ì„± ë¶„ì„ ê²°ê³¼ í•´ì„"""
        interpretations = []
        
        # ì£¼ë„ì  ê°ì •ì— ë”°ë¥¸ í•´ì„
        emotion_meanings = {
            'ê¸°ì¨ (Joy)': "ê¸€ì— ê¸ì •ì ì´ê³  ë°ì€ ì—ë„ˆì§€ê°€ ë‹´ê²¨ ìˆì–´ ì½ëŠ” ì´ì—ê²Œ ì¢‹ì€ ì¸ìƒì„ ì¤ë‹ˆë‹¤.",
            'ìŠ¬í”” (Sadness)': "ê°ì •ì  ê¹Šì´ê°€ ìˆëŠ” ê¸€ë¡œ, ë…ìì˜ ê³µê°ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            'ë¶„ë…¸ (Anger)': "ê°•í•œ ì˜ê²¬ì´ë‚˜ ë¶ˆë§Œì´ í‘œí˜„ë˜ì–´ ìˆì–´ ì£¼ì¥ì´ ëª…í™•í•©ë‹ˆë‹¤.",
            'ë‘ë ¤ì›€ (Fear)': "ìš°ë ¤ë‚˜ ê±±ì •ì´ ë“œëŸ¬ë‚˜ ìˆì–´ ì‹ ì¤‘í•œ ì‚¬ê³  ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
            'ë†€ëŒ (Surprise)': "ìƒˆë¡œìš´ ë°œê²¬ì´ë‚˜ ê¹¨ë‹¬ìŒì´ ë‹´ê¸´ í¥ë¯¸ë¡œìš´ ë‚´ìš©ì…ë‹ˆë‹¤.",
            'í˜ì˜¤ (Disgust)': "ê°•í•œ ë°˜ê°ì´ë‚˜ ë¹„íŒ ì˜ì‹ì´ ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.",
            'ì‹ ë¢° (Trust)': "ë¯¿ìŒê³¼ í™•ì‹ ì´ ë‹´ê¸´ ì‹ ë¢°í•  ë§Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
            'ê¸°ëŒ€ (Anticipation)': "ë¯¸ë˜ì— ëŒ€í•œ í¬ë§ê³¼ ê³„íšì´ ì˜ ë“œëŸ¬ë‚˜ ìˆìŠµë‹ˆë‹¤.",
            'ì¤‘ë¦½ (Neutral)': "ê°ê´€ì ì´ê³  ì¤‘ë¦½ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
        
        if dominant_emotion in emotion_meanings:
            interpretations.append(emotion_meanings[dominant_emotion])
        
        # ê°ì • ê°•ë„ì— ë”°ë¥¸ í•´ì„
        if intensity > 15:
            interpretations.append("ë§¤ìš° ê°•í•œ ê°ì •ì´ í‘œí˜„ë˜ì–´ ìˆì–´ ë…ìì—ê²Œ ê°•í•œ ì¸ìƒì„ ì¤„ ê²ƒì…ë‹ˆë‹¤.")
        elif intensity > 8:
            interpretations.append("ì ì ˆí•œ ìˆ˜ì¤€ì˜ ê°ì • í‘œí˜„ìœ¼ë¡œ ê· í˜•ì¡íŒ ê¸€ì…ë‹ˆë‹¤.")
        elif intensity > 3:
            interpretations.append("ê°ì • í‘œí˜„ì´ ì ˆì œë˜ì–´ ìˆì–´ ì°¨ë¶„í•œ ëŠë‚Œì„ ì¤ë‹ˆë‹¤.")
        else:
            interpretations.append("ê°ì • í‘œí˜„ì„ ì¡°ê¸ˆ ë” í’ë¶€í•˜ê²Œ í•˜ë©´ ê¸€ì´ ë”ìš± ìƒë™ê° ìˆì–´ì§‘ë‹ˆë‹¤.")
        
        # ê°ì • ë‹¤ì–‘ì„±ì— ë”°ë¥¸ í•´ì„
        if variety >= 4:
            interpretations.append("ë‹¤ì–‘í•œ ê°ì •ì´ ì–´ìš°ëŸ¬ì ¸ í’ë¶€í•œ ê°ì •ì  í‘œí˜„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        elif variety >= 2:
            interpretations.append("ì—¬ëŸ¬ ê°ì •ì´ ì¡°í™”ë¡­ê²Œ í‘œí˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            interpretations.append("ê°ì • í‘œí˜„ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ë©´ ë” í¥ë¯¸ë¡œìš´ ê¸€ì´ ë  ê²ƒì…ë‹ˆë‹¤.")
        
        return interpretations