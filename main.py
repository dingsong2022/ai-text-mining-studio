import streamlit as st
import pandas as pd
from modules.data_loader import DataLoader
from modules.preprocessor import TextPreprocessor
import plotly.graph_objects as go

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI Text Mining Studio",
    page_icon="ğŸ”",
    layout="wide"
)

# ë¡œê·¸ì¸ í•¨ìˆ˜
def check_login(username, password):
    """ì‚¬ìš©ì ì¸ì¦"""
    try:
        data_loader = st.session_state.get('data_loader')
        if not data_loader:
            data_loader = DataLoader()
            st.session_state.data_loader = data_loader
        
        # Google Sheets ì—°ê²° í™•ì¸
        if not data_loader.sheet:
            st.error("Google Sheets ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Streamlit Secrets ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return False
        
        # ì‚¬ìš©ìì •ë³´ ì‹œíŠ¸ì—ì„œ í™•ì¸
        users_sheet = data_loader.sheet.worksheet("ì‚¬ìš©ìì •ë³´")
        all_values = users_sheet.get_all_values()
        
        for row in all_values[1:]:  # í—¤ë” ì œì™¸
            if len(row) >= 2:
                stored_username = row[0].strip()
                stored_password = row[1].strip()
                
                if stored_username == username and stored_password == password:
                    return True
        return False
        
    except Exception as e:
        st.error(f"ë¡œê·¸ì¸ í™•ì¸ ì˜¤ë¥˜: {e}")
        return False

def login_page():
    """ë¡œê·¸ì¸ í˜ì´ì§€"""
    st.markdown('<h1 class="main-header">ğŸ” AI Text Mining Studio</h1>', unsafe_allow_html=True)
    st.markdown("**ê°œì¸ ë§ì¶¤í˜• ì˜ì–´ ì—ì„¸ì´ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë¶„ì„ í”Œë«í¼**")
    
    st.markdown("---")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.subheader("ğŸ”‘ ë¡œê·¸ì¸")
        
        with st.form("login_form"):
            username = st.text_input("ì•„ì´ë””")
            password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
            login_button = st.form_submit_button("ë¡œê·¸ì¸", use_container_width=True)
            
            if login_button:
                if username and password:
                    if check_login(username, password):
                        st.session_state.logged_in = True
                        st.session_state.username = username
                        st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
                        st.rerun()
                    else:
                        st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        st.markdown("---")
        st.info("ğŸ’¡ **English Essay Writing Studio**ì—ì„œ ì‚¬ìš©í•˜ë˜ ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•˜ì„¸ìš”!")

def main_app():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    # ë°ì´í„° ë¡œë” ë° ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    if 'data_loader' not in st.session_state:
        st.session_state.data_loader = DataLoader()
    if 'preprocessor' not in st.session_state:
        st.session_state.preprocessor = TextPreprocessor()
    
    data_loader = st.session_state.data_loader
    preprocessor = st.session_state.preprocessor
    username = st.session_state.username
    
    # í—¤ë”
    col1, col2 = st.columns([4, 1])
    with col1:
        st.markdown('<h1 class="main-header">ğŸ” AI Text Mining Studio</h1>', unsafe_allow_html=True)
        st.markdown(f"**{username}ë‹˜ì˜ ì˜ì–´ ì—ì„¸ì´ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë¶„ì„**")
    
    with col2:
        if st.button("ë¡œê·¸ì•„ì›ƒ", type="secondary"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    st.markdown("---")
    
    # ì‚¬ìš©ì ë°ì´í„° ë¡œë”©
    with st.spinner(f"{username}ë‹˜ì˜ ì—ì„¸ì´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        essay_data = data_loader.get_student_essays(username)
    
    if essay_data.empty:
        st.warning(f"ğŸ“ {username}ë‹˜ì˜ ì—ì„¸ì´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("**English Essay Writing Studio**ì—ì„œ ë¨¼ì € ì—ì„¸ì´ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”!")
        return
    
    # ê¸°ë³¸ í†µê³„ í‘œì‹œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ë‚´ ì—ì„¸ì´ ìˆ˜", len(essay_data))
    with col2:
        if 'total_score' in essay_data.columns:
            avg_score = essay_data['total_score'].mean()
            st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.1f}ì ")
        else:
            st.metric("í‰ê·  ì ìˆ˜", "N/A")
    with col3:
        total_words = 0
        for text in essay_data['essay_text']:
            if text and not pd.isna(text):
                total_words += len(str(text).split())
        st.metric("ì´ ì‘ì„± ë‹¨ì–´", f"{total_words:,}")
    with col4:
        unique_topics = essay_data['topic_name'].nunique() if 'topic_name' in essay_data.columns else 0
        st.metric("ë‹¤ë£¬ ì£¼ì œ ìˆ˜", unique_topics)
    
    # íƒ­ìœ¼ë¡œ ê¸°ëŠ¥ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs([
        "ğŸ“š ë‚´ ì—ì„¸ì´ ëª¨ìŒ", 
        "ğŸ”¬ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì‹¤ìŠµ", 
        "ğŸ“ ì¢…í•© ë¶„ì„"
    ])
    
    with tab1:
        show_essay_collection(essay_data, username, data_loader)
    
    with tab2:
        show_text_mining_practice(essay_data, preprocessor, username, data_loader)
    
    with tab3:
        show_comprehensive_analysis(essay_data, preprocessor, username)

def show_essay_collection(essay_data, username, data_loader):
    """ì—ì„¸ì´ ëª¨ìŒ í‘œì‹œ"""
    st.subheader(f"ğŸ“ {username}ë‹˜ì´ ì‘ì„±í•œ ëª¨ë“  ì—ì„¸ì´")
    
    if essay_data.empty:
        st.warning("ì‘ì„±í•œ ì—ì„¸ì´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ í†µê³„
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì´ ì—ì„¸ì´ ìˆ˜", len(essay_data))
    with col2:
        total_words = 0
        for text in essay_data['essay_text']:
            if text and not pd.isna(text):
                total_words += len(str(text).split())
        st.metric("ì´ ì‘ì„± ë‹¨ì–´", f"{total_words:,}")
    with col3:
        if 'total_score' in essay_data.columns:
            avg_score = essay_data['total_score'].mean()
            st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.1f}ì ")
    
    # í•©ì¹œ í…ìŠ¤íŠ¸ ë³´ê¸°
    st.subheader("ğŸ“„ ëª¨ë“  ì—ì„¸ì´ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ í…ìŠ¤íŠ¸")
    
    if st.button("ë‚´ ëª¨ë“  ì—ì„¸ì´ í…ìŠ¤íŠ¸ ë³´ê¸°"):
        with st.spinner("ì—ì„¸ì´ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹˜ëŠ” ì¤‘..."):
            combined_text = data_loader.get_combined_essay_text(username)
            
            if combined_text:
                st.success(f"âœ… ì´ {len(combined_text.split())}ê°œ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤!")
                
                # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
                preview_length = 500
                preview_text = combined_text[:preview_length]
                if len(combined_text) > preview_length:
                    preview_text += "..."
                
                st.text_area(
                    "í•©ì³ì§„ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:", 
                    preview_text, 
                    height=200,
                    help=f"ì „ì²´ í…ìŠ¤íŠ¸ëŠ” {len(combined_text)}ìì…ë‹ˆë‹¤."
                )
                
                # ì„¸ì…˜ì— ì €ì¥
                st.session_state.combined_text = combined_text
                st.info("âœ… í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! 'í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì‹¤ìŠµ' íƒ­ì—ì„œ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            else:
                st.warning("ì—ì„¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê°œë³„ ì—ì„¸ì´ ëª©ë¡
    st.subheader("ğŸ“‹ ê°œë³„ ì—ì„¸ì´ ëª©ë¡")
    
    for i, (_, row) in enumerate(essay_data.iterrows(), 1):
        with st.expander(f"ì—ì„¸ì´ {i}: {row.get('topic_name', 'Unknown')[:50]}..."):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.write("**ì£¼ì œ:**", row.get('topic_description', 'N/A'))
                st.write("**ì‘ì„±ì¼:**", row.get('created_at', 'N/A'))
            
            with col2:
                if 'total_score' in row and not pd.isna(row['total_score']):
                    st.metric("ì ìˆ˜", f"{row['total_score']:.0f}ì ")
            
            # ì—ì„¸ì´ ë‚´ìš©
            essay_text = row.get('essay_text', '')
            if essay_text and not pd.isna(essay_text):
                st.text_area(
                    "ì—ì„¸ì´ ë‚´ìš©:", 
                    str(essay_text), 
                    height=150, 
                    disabled=True,
                    key=f"essay_{i}"
                )
            else:
                st.warning("ì—ì„¸ì´ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

def show_text_mining_practice(essay_data, preprocessor, username, data_loader):
    """ë‹¨ê³„ë³„ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì‹¤ìŠµ"""
    st.subheader(f"ğŸ”¬ {username}ë‹˜ì˜ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì‹¤ìŠµ")
    
    # í•©ì³ì§„ í…ìŠ¤íŠ¸ í™•ì¸
    if 'combined_text' not in st.session_state:
        st.warning("ğŸ“ ë¨¼ì € 'ë‚´ ì—ì„¸ì´ ëª¨ìŒ' íƒ­ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”!")
        return
    
    combined_text = st.session_state.combined_text
    
    if not combined_text:
        st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.success(f"âœ… ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ! (ì´ {len(combined_text.split())}ê°œ ë‹¨ì–´)")
    
    # ë‹¨ê³„ë³„ ì‹¤ìŠµ
    st.markdown("---")
    
    # ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ì‹¤ìŠµ
    st.subheader("ğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ì‹¤ìŠµ")
    st.write("í…ìŠ¤íŠ¸ë¥¼ ë‹¨ê³„ë³„ë¡œ ì •ì œí•´ë³´ë©° ê° ë‹¨ê³„ì˜ íš¨ê³¼ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.")
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì„ íƒ
    sample_length = min(500, len(combined_text))
    sample_text = combined_text[:sample_length]
    
    st.write("**ë¶„ì„í•  ìƒ˜í”Œ í…ìŠ¤íŠ¸:**")
    st.text_area("", sample_text, height=100, disabled=True, key="sample_text")
    
    if st.button("ğŸ” ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ë¶„ì„ ì‹œì‘"):
        with st.spinner("ë‹¨ê³„ë³„ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ëŠ” ì¤‘..."):
            # ì§ì ‘ NLTKë¡œ ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê²°ê³¼ ìƒì„±
            from nltk.stem import PorterStemmer, WordNetLemmatizer
            
            # ê° ë‹¨ê³„ë³„ ì²˜ë¦¬
            steps = {}
            
            # ì›ë³¸
            steps['ì›ë³¸'] = sample_text
            steps['ì›ë³¸_ë‹¨ì–´ìˆ˜'] = len(sample_text.split()) if sample_text else 0
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ
            step1 = preprocessor.step1_basic_cleaning(sample_text)
            steps['1ë‹¨ê³„_ê¸°ë³¸ì •ì œ'] = step1
            steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step1.split()) if step1 else 0
            
            # 2ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°
            step2 = preprocessor.step2_remove_stopwords(step1)
            steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'] = step2
            steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step2.split()) if step2 else 0
            
            # 3ë‹¨ê³„: ì–´ê°„ ì¶”ì¶œ (ì§ì ‘ êµ¬í˜„)
            if step2:
                stemmer = PorterStemmer()
                words = step2.split()
                stemmed_words = [stemmer.stem(word) for word in words]
                step3 = ' '.join(stemmed_words)
            else:
                step3 = step2
            steps['3ë‹¨ê³„_ì–´ê°„ì¶”ì¶œ'] = step3
            steps['3ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step3.split()) if step3 else 0
            
            # 4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ (ì§ì ‘ êµ¬í˜„)
            if step2:
                lemmatizer = WordNetLemmatizer()
                words = step2.split()
                lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
                step4 = ' '.join(lemmatized_words)
            else:
                step4 = step2
            steps['4ë‹¨ê³„_í‘œì œì–´ì¶”ì¶œ'] = step4
            steps['4ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] = len(step4.split()) if step4 else 0
            
            # 5ë‹¨ê³„: ê°œì²´ëª… ì¸ì‹ (NER) - ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜í–‰
            import re
            try:
                import spacy
                # spacy ëª¨ë¸ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ NER
                ner_results = []
                
                # ì¸ëª… íŒ¨í„´ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ë‹¨ì–´)
                person_pattern = r'\b[A-Z][a-z]+ [A-Z][a-z]+\b'
                persons = re.findall(person_pattern, sample_text)
                
                # ì¥ì†Œëª… íŒ¨í„´ (íŠ¹ì • í‚¤ì›Œë“œì™€ í•¨ê»˜ ë‚˜ì˜¤ëŠ” ëŒ€ë¬¸ì ë‹¨ì–´)
                place_keywords = r'\b(?:in|at|from|to)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b'
                places = re.findall(place_keywords, sample_text)
                
                # ê¸°ê´€ëª… íŒ¨í„´ (School, University, Company ë“±ì´ í¬í•¨ëœ íŒ¨í„´)
                org_pattern = r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:School|University|College|Company|Corporation|Inc|Ltd))\b'
                organizations = re.findall(org_pattern, sample_text)
                
                # ì‹œê°„ íŒ¨í„´ (ë…„ë„, ì›”, ìš”ì¼ ë“±)
                time_pattern = r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December|\d{4}|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|today|yesterday|tomorrow)\b'
                times = re.findall(time_pattern, sample_text, re.IGNORECASE)
                
                ner_results = {
                    'PERSON': list(set(persons)),
                    'PLACE': list(set(places)),
                    'ORG': list(set(organizations)),
                    'TIME': list(set(times))
                }
                
            except ImportError:
                # spacyê°€ ì—†ì„ ê²½ìš° ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ NERë§Œ ì‚¬ìš©
                import re
                ner_results = {}
                
                # ì¸ëª… íŒ¨í„´ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ë‹¨ì–´)
                person_pattern = r'\b[A-Z][a-z]+ [A-Z][a-z]+\b'
                persons = re.findall(person_pattern, sample_text)
                
                # ì¥ì†Œëª… íŒ¨í„´
                place_keywords = r'\b(?:in|at|from|to)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b'
                places = re.findall(place_keywords, sample_text)
                
                # ê¸°ê´€ëª… íŒ¨í„´
                org_pattern = r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:School|University|College|Company|Corporation|Inc|Ltd))\b'
                organizations = re.findall(org_pattern, sample_text)
                
                # ì‹œê°„ íŒ¨í„´
                time_pattern = r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December|\d{4}|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|today|yesterday|tomorrow)\b'
                times = re.findall(time_pattern, sample_text, re.IGNORECASE)
                
                ner_results = {
                    'PERSON': list(set(persons)),
                    'PLACE': list(set(places)), 
                    'ORG': list(set(organizations)),
                    'TIME': list(set(times))
                }
            
            steps['5ë‹¨ê³„_NERê²°ê³¼'] = ner_results
            steps['5ë‹¨ê³„_ê°œì²´ìˆ˜'] = sum(len(entities) for entities in ner_results.values())
            
            preprocessing_steps = steps
            
            # ê²°ê³¼ í‘œì‹œ
            st.subheader("ğŸ“‹ ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê²°ê³¼")
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ
            with st.expander("ğŸ”§ 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ (HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì •ë¦¬)", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Before (ì›ë³¸):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['ì›ë³¸_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['ì›ë³¸'][:200] + "...", height=120, disabled=True, key="before_1")
                
                with col2:
                    st.write("**After (ê¸°ë³¸ ì •ì œ):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['1ë‹¨ê³„_ê¸°ë³¸ì •ì œ'][:200] + "...", height=120, disabled=True, key="after_1")
                
                word_diff_1 = preprocessing_steps['ì›ë³¸_ë‹¨ì–´ìˆ˜'] - preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜']
                if word_diff_1 > 0:
                    st.success(f"âœ… {word_diff_1}ê°œ ë‹¨ì–´ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤! (HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì ì œê±°)")
                else:
                    st.info("íŠ¹ë³„íˆ ì œê±°ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°
            with st.expander("ğŸš« 2ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±° (ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë“¤ ì œê±°)", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Before (ê¸°ë³¸ ì •ì œ):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['1ë‹¨ê³„_ê¸°ë³¸ì •ì œ'][:200] + "...", height=120, disabled=True, key="before_2")
                
                with col2:
                    st.write("**After (ë¶ˆìš©ì–´ ì œê±°):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'][:200] + "...", height=120, disabled=True, key="after_2")
                
                word_diff_2 = preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] - preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜']
                if word_diff_2 > 0:
                    st.success(f"âœ… {word_diff_2}ê°œ ë¶ˆìš©ì–´ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤! (the, a, and, is ë“±)")
                else:
                    st.info("ì œê±°ëœ ë¶ˆìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                st.info("ğŸ’¡ **ë¶ˆìš©ì–´**: 'the', 'a', 'an', 'and', 'or', 'is', 'are' ë“± ë¶„ì„ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤")
            
            # 3ë‹¨ê³„: ì–´ê°„ ì¶”ì¶œ
            with st.expander("âœ‚ï¸ 3ë‹¨ê³„: ì–´ê°„ ì¶”ì¶œ (ë‹¨ì–´ì˜ ê¸°ë³¸ í˜•íƒœë¡œ ë³€í™˜)", expanded=False):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Before (ë¶ˆìš©ì–´ ì œê±°):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'][:200] + "...", height=120, disabled=True, key="before_3")
                
                with col2:
                    st.write("**After (ì–´ê°„ ì¶”ì¶œ):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['3ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['3ë‹¨ê³„_ì–´ê°„ì¶”ì¶œ'][:200] + "...", height=120, disabled=True, key="after_3")
                
                # ì–´ê°„ ì¶”ì¶œ ë³€í™” ì˜ˆì‹œ ë³´ì—¬ì£¼ê¸°
                if preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'] and preprocessing_steps['3ë‹¨ê³„_ì–´ê°„ì¶”ì¶œ']:
                    from nltk.stem import PorterStemmer
                    stemmer = PorterStemmer()
                    
                    before_words = preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'].split()[:20]  # ì²˜ìŒ 20ê°œ ë‹¨ì–´ë§Œ
                    stemmed_examples = []
                    
                    for word in before_words:
                        stemmed = stemmer.stem(word)
                        if word != stemmed:  # ë³€í™”ëœ ë‹¨ì–´ë§Œ í‘œì‹œ
                            stemmed_examples.append(f"'{word}' â†’ '{stemmed}'")
                    
                    if stemmed_examples:
                        st.success(f"âœ… {len(stemmed_examples)}ê°œ ë‹¨ì–´ê°€ ì–´ê°„ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.write("**í˜•íƒœê°€ ë³€í™”ëœ ë‹¨ì–´ ì˜ˆì‹œ:**")
                        # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ í‘œì‹œ
                        for example in stemmed_examples[:10]:
                            st.write(f"â€¢ {example}")
                        if len(stemmed_examples) > 10:
                            st.write(f"... ì™¸ {len(stemmed_examples) - 10}ê°œ ë”")
                    else:
                        st.info("ì´ í…ìŠ¤íŠ¸ì—ì„œëŠ” ì–´ê°„ ì¶”ì¶œë¡œ ë³€í™”ëœ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                st.info("ğŸ’¡ **ì–´ê°„ ì¶”ì¶œ**: 'running' â†’ 'run', 'studies' â†’ 'studi', 'better' â†’ 'better'")
                st.warning("âš ï¸ ì–´ê°„ ì¶”ì¶œì€ ë‹¨ìˆœí•œ ê·œì¹™ì„ ì‚¬ìš©í•´ì„œ ë•Œë¡œëŠ” ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # 4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ
            with st.expander("ğŸ¯ 4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ (ì‚¬ì „ í˜•íƒœë¡œ ì •í™•í•˜ê²Œ ë³€í™˜)", expanded=False):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Before (ë¶ˆìš©ì–´ ì œê±°):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'][:200] + "...", height=120, disabled=True, key="before_4")
                
                with col2:
                    st.write("**After (í‘œì œì–´ ì¶”ì¶œ):**")
                    st.write(f"ë‹¨ì–´ ìˆ˜: **{preprocessing_steps['4ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ**")
                    st.text_area("", preprocessing_steps['4ë‹¨ê³„_í‘œì œì–´ì¶”ì¶œ'][:200] + "...", height=120, disabled=True, key="after_4")
                
                # í‘œì œì–´ ì¶”ì¶œ ë³€í™” ì˜ˆì‹œ ë³´ì—¬ì£¼ê¸°
                if preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'] and preprocessing_steps['4ë‹¨ê³„_í‘œì œì–´ì¶”ì¶œ']:
                    from nltk.stem import WordNetLemmatizer
                    lemmatizer = WordNetLemmatizer()
                    
                    before_words = preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'].split()[:20]  # ì²˜ìŒ 20ê°œ ë‹¨ì–´ë§Œ
                    lemmatized_examples = []
                    
                    for word in before_words:
                        lemmatized = lemmatizer.lemmatize(word)
                        if word != lemmatized:  # ë³€í™”ëœ ë‹¨ì–´ë§Œ í‘œì‹œ
                            lemmatized_examples.append(f"'{word}' â†’ '{lemmatized}'")
                    
                    if lemmatized_examples:
                        st.success(f"âœ… {len(lemmatized_examples)}ê°œ ë‹¨ì–´ê°€ í‘œì œì–´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.write("**í˜•íƒœê°€ ë³€í™”ëœ ë‹¨ì–´ ì˜ˆì‹œ:**")
                        # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ í‘œì‹œ
                        for example in lemmatized_examples[:10]:
                            st.write(f"â€¢ {example}")
                        if len(lemmatized_examples) > 10:
                            st.write(f"... ì™¸ {len(lemmatized_examples) - 10}ê°œ ë”")
                    else:
                        st.info("ì´ í…ìŠ¤íŠ¸ì—ì„œëŠ” í‘œì œì–´ ì¶”ì¶œë¡œ ë³€í™”ëœ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ì–´ê°„ ì¶”ì¶œ vs í‘œì œì–´ ì¶”ì¶œ ë¹„êµ
                if preprocessing_steps['3ë‹¨ê³„_ì–´ê°„ì¶”ì¶œ'] != preprocessing_steps['4ë‹¨ê³„_í‘œì œì–´ì¶”ì¶œ']:
                    st.write("**ğŸ” ì–´ê°„ ì¶”ì¶œ vs í‘œì œì–´ ì¶”ì¶œ ë¹„êµ:**")
                    from nltk.stem import PorterStemmer, WordNetLemmatizer
                    stemmer = PorterStemmer()
                    lemmatizer = WordNetLemmatizer()
                    
                    before_words = preprocessing_steps['2ë‹¨ê³„_ë¶ˆìš©ì–´ì œê±°'].split()[:15]
                    comparison_examples = []
                    
                    for word in before_words:
                        stemmed = stemmer.stem(word)
                        lemmatized = lemmatizer.lemmatize(word)
                        if stemmed != lemmatized:  # ë‘ ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš°ë§Œ
                            comparison_examples.append(f"'{word}' â†’ ì–´ê°„: '{stemmed}' vs í‘œì œì–´: '{lemmatized}'")
                    
                    if comparison_examples:
                        for example in comparison_examples[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                            st.write(f"â€¢ {example}")
                
                st.info("ğŸ’¡ **í‘œì œì–´ ì¶”ì¶œ**: 'running' â†’ 'run', 'studies' â†’ 'study', 'better' â†’ 'better' (ì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ ë” ì •í™•í•¨)")
                st.success("âœ… í‘œì œì–´ ì¶”ì¶œì´ ì–´ê°„ ì¶”ì¶œë³´ë‹¤ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤!")
            
            # 5ë‹¨ê³„: ê°œì²´ëª… ì¸ì‹ (NER)
            with st.expander("ğŸ·ï¸ 5ë‹¨ê³„: ê°œì²´ëª… ì¸ì‹ (NER) - ì¸ë¬¼, ì¥ì†Œ, ê¸°ê´€, ì‹œê°„ ì¶”ì¶œ", expanded=True):
                st.markdown("**ğŸ¯ ê°œì²´ëª… ì¸ì‹ì´ë€?**")
                st.write("í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì˜ë¯¸ë¥¼ ê°€ì§„ ê³ ìœ ëª…ì‚¬ë“¤(ì¸ë¬¼ëª…, ì§€ëª…, ê¸°ê´€ëª…, ì‹œê°„ ë“±)ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.")
                
                ner_results = preprocessing_steps['5ë‹¨ê³„_NERê²°ê³¼']
                total_entities = preprocessing_steps['5ë‹¨ê³„_ê°œì²´ìˆ˜']
                
                if total_entities > 0:
                    st.success(f"âœ… ì´ **{total_entities}ê°œ**ì˜ ê°œì²´ëª…ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ê°œì²´ëª… ìœ í˜•ë³„ í‘œì‹œ
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("ğŸ‘¤ ì¸ë¬¼ëª…", len(ner_results['PERSON']))
                        if ner_results['PERSON']:
                            for person in ner_results['PERSON'][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                                st.write(f"â€¢ {person}")
                            if len(ner_results['PERSON']) > 5:
                                st.write(f"... ì™¸ {len(ner_results['PERSON']) - 5}ê°œ ë”")
                    
                    with col2:
                        st.metric("ğŸ—ºï¸ ì¥ì†Œëª…", len(ner_results['PLACE']))
                        if ner_results['PLACE']:
                            for place in ner_results['PLACE'][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                                st.write(f"â€¢ {place}")
                            if len(ner_results['PLACE']) > 5:
                                st.write(f"... ì™¸ {len(ner_results['PLACE']) - 5}ê°œ ë”")
                    
                    with col3:
                        st.metric("ğŸ¢ ê¸°ê´€ëª…", len(ner_results['ORG']))
                        if ner_results['ORG']:
                            for org in ner_results['ORG'][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                                st.write(f"â€¢ {org}")
                            if len(ner_results['ORG']) > 5:
                                st.write(f"... ì™¸ {len(ner_results['ORG']) - 5}ê°œ ë”")
                    
                    with col4:
                        st.metric("â° ì‹œê°„ í‘œí˜„", len(ner_results['TIME']))
                        if ner_results['TIME']:
                            for time in ner_results['TIME'][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                                st.write(f"â€¢ {time}")
                            if len(ner_results['TIME']) > 5:
                                st.write(f"... ì™¸ {len(ner_results['TIME']) - 5}ê°œ ë”")
                    
                    # ë¶„ì„ ì˜ë¯¸ í•´ì„
                    st.markdown("**ğŸ“Š ë¶„ì„ í•´ì„:**")
                    interpretation = []
                    
                    if len(ner_results['PERSON']) > 0:
                        interpretation.append(f"â€¢ **ì¸ë¬¼ ì–¸ê¸‰**: {len(ner_results['PERSON'])}ëª…ì˜ ì¸ë¬¼ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì—¬ ë‚´ìš©ì˜ ì‹ ë¢°ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.")
                    
                    if len(ner_results['PLACE']) > 0:
                        interpretation.append(f"â€¢ **ì¥ì†Œ ì •ë³´**: {len(ner_results['PLACE'])}ê³³ì˜ êµ¬ì²´ì  ì¥ì†Œë¥¼ ì œì‹œí•˜ì—¬ ìƒí™©ì„ ëª…í™•íˆ í–ˆìŠµë‹ˆë‹¤.")
                    
                    if len(ner_results['ORG']) > 0:
                        interpretation.append(f"â€¢ **ê¸°ê´€ ì •ë³´**: {len(ner_results['ORG'])}ê°œ ê¸°ê´€ì„ ì–¸ê¸‰í•˜ì—¬ ì „ë¬¸ì„±ê³¼ ê°ê´€ì„±ì„ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.")
                    
                    if len(ner_results['TIME']) > 0:
                        interpretation.append(f"â€¢ **ì‹œê°„ ì •ë³´**: {len(ner_results['TIME'])}ê°œì˜ ì‹œê°„ í‘œí˜„ìœ¼ë¡œ ì‹œê°„ì  ë§¥ë½ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
                    
                    if interpretation:
                        for interp in interpretation:
                            st.write(interp)
                    
                    # êµ¬ì²´ì„± ì ìˆ˜ ê³„ì‚°
                    specificity_score = min(100, total_entities * 10)  # ê°œì²´ëª… ê°œìˆ˜ Ã— 10ì , ìµœëŒ€ 100ì 
                    
                    if specificity_score >= 70:
                        st.success(f"ğŸŒŸ **êµ¬ì²´ì„± ì ìˆ˜: {specificity_score}ì ** - ë§¤ìš° êµ¬ì²´ì ì´ê³  í’ë¶€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤!")
                    elif specificity_score >= 40:
                        st.info(f"ğŸ“ˆ **êµ¬ì²´ì„± ì ìˆ˜: {specificity_score}ì ** - ì ì ˆí•œ ìˆ˜ì¤€ì˜ êµ¬ì²´ì  ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        st.warning(f"ğŸ’¡ **êµ¬ì²´ì„± ì ìˆ˜: {specificity_score}ì ** - ë” êµ¬ì²´ì ì¸ ì¸ë¬¼, ì¥ì†Œ, ê¸°ê´€ëª…ì„ ì–¸ê¸‰í•˜ë©´ ê¸€ì˜ ì„¤ë“ë ¥ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.")
                
                else:
                    st.info("ğŸ” ì´ í…ìŠ¤íŠ¸ì—ì„œëŠ” ëª…í™•í•œ ê°œì²´ëª…ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    st.write("ğŸ’¡ **ê°œì„  ì œì•ˆ**: êµ¬ì²´ì ì¸ ì¸ë¬¼ëª…, ì¥ì†Œëª…, ê¸°ê´€ëª…ì„ ì–¸ê¸‰í•˜ë©´ ê¸€ì˜ ì‹ ë¢°ì„±ê³¼ ì„¤ë“ë ¥ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.")
                
                st.info("""
                ğŸ’¡ **ê°œì²´ëª… ì¸ì‹ ê¸°ìˆ :**
                - **íŒ¨í„´ ê¸°ë°˜**: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ íŠ¹ì • íŒ¨í„´(ëŒ€ë¬¸ì ì—°ì†, í‚¤ì›Œë“œ ì¡°í•© ë“±)ì„ íƒì§€
                - **ê¸°ê³„í•™ìŠµ**: ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ ë¬¸ë§¥ì„ ê³ ë ¤í•´ ê°œì²´ëª… ë¶„ë¥˜
                - **ì‹¤ì œ í™œìš©**: Google ê²€ìƒ‰, ë‰´ìŠ¤ ë¶„ë¥˜, ë¬¸ì„œ ìš”ì•½, ì •ë³´ ì¶”ì¶œ ë“±ì—ì„œ í•µì‹¬ ê¸°ìˆ 
                """)
            
            # ìš”ì•½ í†µê³„
            st.markdown("---")
            st.subheader("ğŸ“Š ì „ì²˜ë¦¬ ìš”ì•½ í†µê³„")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric(
                    "ì›ë³¸", 
                    f"{preprocessing_steps['ì›ë³¸_ë‹¨ì–´ìˆ˜']}ê°œ",
                    help="ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ìˆ˜"
                )
            
            with col2:
                diff_1 = preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] - preprocessing_steps['ì›ë³¸_ë‹¨ì–´ìˆ˜']
                st.metric(
                    "ê¸°ë³¸ ì •ì œ", 
                    f"{preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ",
                    f"{diff_1:+d}ê°œ",
                    help="HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„"
                )
            
            with col3:
                diff_2 = preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] - preprocessing_steps['1ë‹¨ê³„_ë‹¨ì–´ìˆ˜']
                st.metric(
                    "ë¶ˆìš©ì–´ ì œê±°", 
                    f"{preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ",
                    f"{diff_2:+d}ê°œ",
                    help="ë¶ˆìš©ì–´ ì œê±° í›„"
                )
            
            with col4:
                diff_4 = preprocessing_steps['4ë‹¨ê³„_ë‹¨ì–´ìˆ˜'] - preprocessing_steps['2ë‹¨ê³„_ë‹¨ì–´ìˆ˜']
                st.metric(
                    "í‘œì œì–´ ì¶”ì¶œ", 
                    f"{preprocessing_steps['4ë‹¨ê³„_ë‹¨ì–´ìˆ˜']}ê°œ",
                    f"{diff_4:+d}ê°œ" if diff_4 != 0 else "ë³€í™”ì—†ìŒ",
                    help="ìµœì¢… ì „ì²˜ë¦¬ ì™„ë£Œ (ë¶ˆìš©ì–´ ì œê±° í›„ ê¸°ì¤€)"
                )
            
            with col5:
                st.metric(
                    "ğŸ·ï¸ ê°œì²´ëª…", 
                    f"{preprocessing_steps['5ë‹¨ê³„_ê°œì²´ìˆ˜']}ê°œ",
                    help="ì¸ë¬¼, ì¥ì†Œ, ê¸°ê´€, ì‹œê°„ ë“± ê³ ìœ ëª…ì‚¬"
                )
            
            # ì„¸ì…˜ì— ìµœì¢… ê²°ê³¼ ì €ì¥
            st.session_state.final_preprocessed_text = preprocessing_steps['4ë‹¨ê³„_í‘œì œì–´ì¶”ì¶œ']
            
            # ì „ì²´ ìš”ì•½
            total_reduction = preprocessing_steps['ì›ë³¸_ë‹¨ì–´ìˆ˜'] - preprocessing_steps['4ë‹¨ê³„_ë‹¨ì–´ìˆ˜']
            st.success(f"ğŸ‰ ëª¨ë“  ì „ì²˜ë¦¬ ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ {total_reduction}ê°œ ë‹¨ì–´ê°€ ì •ì œ/ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    if 'final_preprocessed_text' in st.session_state:
        st.markdown("---")
        st.info("âœ… ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ì„ ì§„í–‰í•´ë³´ì„¸ìš”.")
        
        # ë¹ˆë„ ë¶„ì„
        if st.button("ğŸ“Š ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘"):
            final_text = st.session_state.final_preprocessed_text
            
            if final_text:
                from collections import Counter
                words = final_text.split()
                word_freq = Counter(words)
                top_words = word_freq.most_common(20)
                
                if top_words:
                    st.subheader("ğŸ“ˆ ë‚´ê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©í•œ ë‹¨ì–´ TOP 20")
                    
                    words_list, freq_list = zip(*top_words)
                    
                    fig = go.Figure(data=[
                        go.Bar(
                            x=list(freq_list), 
                            y=list(words_list), 
                            orientation='h',
                            marker_color='lightcoral',
                            text=[f'{freq}íšŒ' for freq in freq_list],
                            textposition='outside'
                        )
                    ])
                    
                    fig.update_layout(
                        title="ì „ì²˜ë¦¬ í›„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼",
                        xaxis_title="ì‚¬ìš© ë¹ˆë„",
                        yaxis_title="ë‹¨ì–´",
                        height=500,
                        yaxis={'categoryorder': 'total ascending'}
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ë¹ˆë„ ë¶„ì„ ì¸ì‚¬ì´íŠ¸
                    st.subheader("ğŸ” ë¶„ì„ ì¸ì‚¬ì´íŠ¸")
                    most_used_word = top_words[0]
                    st.write(f"â€¢ ê°€ì¥ ë§ì´ ì‚¬ìš©í•œ ë‹¨ì–´: **'{most_used_word[0]}'** ({most_used_word[1]}íšŒ)")
                    st.write(f"â€¢ ì´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: **{len(word_freq)}ê°œ**")
                    st.write(f"â€¢ ì „ì²˜ë¦¬ í›„ ì´ ë‹¨ì–´ ìˆ˜: **{len(words)}ê°œ**")
                    
                    st.session_state.word_freq = word_freq
                    st.success("âœ… ë¹ˆë„ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì›Œë“œí´ë¼ìš°ë“œ
    if 'word_freq' in st.session_state:
        st.markdown("---")
        st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±")
        st.write("ì „ì²˜ë¦¬ëœ ë‹¨ì–´ë“¤ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.")
        
        if st.button("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"):
            try:
                from wordcloud import WordCloud
                import matplotlib.pyplot as plt
                
                word_freq = st.session_state.word_freq
                
                if word_freq:
                    # ì˜ì–´ ë‹¨ì–´ë§Œ í•„í„°ë§ + ì‚¬ìš©ìëª… ì œì™¸
                    english_only_freq = {}
                    exclude_words = {username.lower(), 'y11111', 'test', 'user'}  # ì‚¬ìš©ìëª… ê´€ë ¨ ë‹¨ì–´ë“¤ ì œì™¸
                    
                    for word, freq in word_freq.items():
                        if (word.isascii() and 
                            word.isalpha() and 
                            len(word) > 2 and 
                            word.lower() not in exclude_words):  # ì‚¬ìš©ìëª… ì œì™¸
                            english_only_freq[word] = freq
                    
                    if english_only_freq and len(english_only_freq) >= 5:
                        # ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì • ê°œì„ 
                        wordcloud = WordCloud(
                            width=400,  # ë” ì‘ê²Œ
                            height=200,  # ë” ì‘ê²Œ
                            background_color='white',
                            max_words=20,
                            colormap='tab10',  # ë” ì„ ëª…í•œ ìƒ‰ìƒ
                            relative_scaling=1.0,  # í¬ê¸° ì°¨ì´ ê·¹ëŒ€í™”
                            prefer_horizontal=0.7,
                            min_font_size=12,   # ìµœì†Œ í¬ê¸° ì¦ê°€
                            max_font_size=80,   # ìµœëŒ€ í¬ê¸° ì¦ê°€
                            font_step=4,        # í¬ê¸° ë‹¨ê³„ ì¦ê°€
                            collocations=False, # ë‹¨ì–´ ì¡°í•© ë°©ì§€
                            margin=10
                        ).generate_from_frequencies(english_only_freq)
                        
                        # ì»¨í…Œì´ë„ˆ í¬ê¸° ì œí•œ
                        col1, col2, col3 = st.columns([1, 2, 1])
                        with col2:
                            fig, ax = plt.subplots(figsize=(6, 3))  # ë” ì‘ê²Œ
                            ax.imshow(wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            ax.set_title(f"{username}'s Most Used Words", 
                                       fontsize=12, fontweight='bold', pad=10)
                            
                            # ì—¬ë°± ì œê±°
                            plt.tight_layout(pad=0.5)
                            st.pyplot(fig, use_container_width=False)
                            plt.close()
                        
                        st.success("ğŸ‰ ì›Œë“œí´ë¼ìš°ë“œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.info("ğŸ’¡ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì¼ìˆ˜ë¡ í¬ê²Œ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        st.warning("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì— ì¶©ë¶„í•œ ì˜ì–´ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ì „ì²´ ì‹¤ìŠµ ì™„ë£Œ
                st.markdown("---")
                st.success("ğŸ‰ ëª¨ë“  í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì‹¤ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except ImportError:
                st.error("ì›Œë“œí´ë¼ìš°ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install wordcloud")
            except Exception as e:
                st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")

def show_comprehensive_analysis(essay_data, preprocessor, username):
    """ì¢…í•© ë¶„ì„"""
    st.header("ğŸ“ ì¢…í•© ë¶„ì„")
    st.markdown("""
    **í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì˜ í•µì‹¬ ì›ë¦¬ë“¤ì„ ë‹¨ê³„ë³„ë¡œ ì²´í—˜í•´ë³´ì„¸ìš”!**  
    ê° ë¶„ì„ì€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ AI ê¸°ìˆ ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    if essay_data.empty:
        st.warning("ë¶„ì„í•  ì—ì„¸ì´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì „ì²´ ì—ì„¸ì´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    all_essays_text = ""
    for _, row in essay_data.iterrows():
        essay_text = row.get('essay_text', '')
        if essay_text and not pd.isna(essay_text):
            cleaned_text = preprocessor.extract_essay_content(essay_text)
            if cleaned_text:
                all_essays_text += cleaned_text + " "
    
    if not all_essays_text.strip():
        st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.info(f"ğŸ“š **ë¶„ì„ ëŒ€ìƒ**: {len(essay_data)}ê°œ ì—ì„¸ì´ì˜ í†µí•© í…ìŠ¤íŠ¸ (ì´ {len(all_essays_text.split())}ê°œ ë‹¨ì–´)")
    
    # ì²´í—˜ ì„ íƒ íƒ­
    analysis_tabs = st.tabs([
        "ğŸ˜Š ê°ì„± ë¶„ì„ ì›ë¦¬ ì²´í—˜", 
        "ğŸ“ í’ˆì‚¬ ë¶„ì„ ì›ë¦¬ ì²´í—˜", 
        "ğŸ† ê¸€ì“°ê¸° ìˆ˜ì¤€ ì¢…í•© ì§„ë‹¨"
    ])
    
    # 1. ê°ì„± ë¶„ì„ íƒ­
    with analysis_tabs[0]:
        st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„ 4ë‹¨ê³„ ì›ë¦¬ ì²´í—˜")
        st.markdown("ê°ì„± ë¶„ì„ì˜ 4ê°€ì§€ ì ‘ê·¼ë²•ì„ ì§ì ‘ ì²´í—˜í•˜ë©° í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì˜ ì›ë¦¬ë¥¼ ì´í•´í•´ë³´ì„¸ìš”.")
        
        if st.button("ğŸ˜Š 4ë‹¨ê³„ ê°ì„± ë¶„ì„ ì›ë¦¬ ì²´í—˜ ì‹œì‘", key="educational_sentiment"):
            with st.spinner("ê°ì„± ë¶„ì„ ì›ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                # í†µí•© í…ìŠ¤íŠ¸ë¡œ êµìœ¡ì  ê°ì„± ë¶„ì„ ì‹¤í–‰
                
                # 1ë‹¨ê³„: ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜
                sentiment_method1 = preprocessor.educational_sentiment_analysis_step1_lexicon(all_essays_text)
                
                # 2ë‹¨ê³„: TF-IDF + ë¨¸ì‹ ëŸ¬ë‹
                sentiment_method2 = preprocessor.educational_sentiment_analysis_step2_tfidf(all_essays_text, [all_essays_text])
                
                # 3ë‹¨ê³„: VADER
                sentiment_method3 = preprocessor.educational_sentiment_analysis_step3_vader(all_essays_text)
                
                # 4ë‹¨ê³„: ë‹¤ì¤‘ ê°ì„± ë¶„ì„
                sentiment_method4 = preprocessor.educational_sentiment_analysis_step4_emotions(all_essays_text)
                
                sentiment_comparison_result = {
                    'essay_info': {
                        'topic': f"{username}ë‹˜ì˜ ëª¨ë“  ì—ì„¸ì´ í†µí•© í…ìŠ¤íŠ¸",
                        'text_preview': all_essays_text[:200] + "...",
                        'total_words': len(all_essays_text.split()),
                        'total_essays': len(essay_data)
                    },
                    'method1_lexicon': sentiment_method1,
                    'method2_tfidf': sentiment_method2,
                    'method3_vader': sentiment_method3,
                    'method4_emotions': sentiment_method4
                }
                
                if sentiment_comparison_result:
                    # ì—ì„¸ì´ ì •ë³´
                    essay_info = sentiment_comparison_result['essay_info']
                    st.subheader(f"ğŸ“ ê°ì„± ë¶„ì„ ëŒ€ìƒ: {essay_info['topic']}")
                    st.write(f"â€¢ ì´ ì—ì„¸ì´ ìˆ˜: {essay_info['total_essays']}ê°œ")
                    st.write(f"â€¢ ì´ ë‹¨ì–´ ìˆ˜: {essay_info['total_words']}ê°œ")
                    
                    st.markdown("**ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:**")
                    st.text_area("", essay_info['text_preview'], height=100, disabled=True, key="sentiment_preview_text")
                    
                    st.markdown("---")
                    
                    # 1ë‹¨ê³„: ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„
                    method1 = sentiment_comparison_result['method1_lexicon']
                    
                    st.markdown("## ğŸ“š 1ë‹¨ê³„: ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **ê°ì„± ì–´íœ˜ ì‚¬ì „**ì—ì„œ ê° ë‹¨ì–´ë³„ ê°ì„± ì ìˆ˜ë¥¼ ì¡°íšŒ
                    - **ë‹¨ìˆœ í•©ì‚° ë°©ì‹**ìœ¼ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ê°ì„± ê³„ì‚°
                    - **ë¹ ë¥´ê³  ì§ê´€ì **ì´ì§€ë§Œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ëŠ” í•œê³„
                    """)
                    
                    if 'error' not in method1:
                        col1, col2 = st.columns([1, 1])
                        
                        with col1:
                            # ê°ì„± ì ìˆ˜ì™€ ê²°ê³¼ (ìˆ˜ì •ëœ í‚¤ ì‚¬ìš©)
                            sentiment_score = method1.get('total_score', 0)
                            sentiment_label = method1.get('sentiment', 'Unknown')
                            emoji = method1.get('emoji', '')
                            
                            if 'ê¸ì •' in sentiment_label:
                                st.success(f"**ê°ì„± ê²°ê³¼**: {sentiment_label} {emoji}")
                            elif 'ë¶€ì •' in sentiment_label:
                                st.error(f"**ê°ì„± ê²°ê³¼**: {sentiment_label} {emoji}")
                            else:
                                st.info(f"**ê°ì„± ê²°ê³¼**: {sentiment_label} {emoji}")
                            
                            st.metric("ê°ì„± ì ìˆ˜", f"{sentiment_score}")
                            
                            # ì‹¤ì œ ì°¾ì€ ë‹¨ì–´ë“¤ í‘œì‹œ
                            positive_words = method1.get('positive_words_found', [])
                            negative_words = method1.get('negative_words_found', [])
                            st.write(f"â€¢ ê¸ì • ë‹¨ì–´: {len(positive_words)}ê°œ")
                            st.write(f"â€¢ ë¶€ì • ë‹¨ì–´: {len(negative_words)}ê°œ")
                        
                        with col2:
                            # ê°ì„± ë¶„í¬ ì‹œê°í™” (ìˆ˜ì •ëœ ë°ì´í„° ì‚¬ìš©)
                            positive_words = method1.get('positive_words_found', [])
                            negative_words = method1.get('negative_words_found', [])
                            positive_count = len(positive_words)
                            negative_count = len(negative_words)
                            
                            if positive_count + negative_count > 0:
                                fig = go.Figure(data=[
                                    go.Bar(
                                        x=['ê¸ì •', 'ë¶€ì •'],
                                        y=[positive_count, negative_count],
                                        marker_color=['#28a745', '#dc3545'],
                                        text=[positive_count, negative_count],
                                        textposition='auto'
                                    )
                                ])
                                
                                fig.update_layout(
                                    title="ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„í¬",
                                    xaxis_title="ê°ì„± ë²”ì£¼",
                                    yaxis_title="ë‹¨ì–´ ê°œìˆ˜",
                                    height=300
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                            else:
                                st.info("ê°ì„± ë‹¨ì–´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        
                        # ê°ì„± ë‹¨ì–´ ì˜ˆì‹œ (ìˆ˜ì •ëœ ë°ì´í„° ì‚¬ìš©)
                        positive_words = method1.get('positive_words_found', [])
                        negative_words = method1.get('negative_words_found', [])
                        
                        if positive_words or negative_words:
                            st.write("**ë°œê²¬ëœ ê°ì„± ë‹¨ì–´ ì˜ˆì‹œ:**")
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                if positive_words:
                                    st.success("**ê¸ì • ë‹¨ì–´ë“¤**")
                                    for word, score in positive_words[:5]:
                                        st.write(f"â€¢ '{word}': +{score}ì ")
                                    if len(positive_words) > 5:
                                        st.write(f"... ì™¸ {len(positive_words) - 5}ê°œ ë”")
                            
                            with col2:
                                if negative_words:
                                    st.error("**ë¶€ì • ë‹¨ì–´ë“¤**")
                                    for word, score in negative_words[:5]:
                                        st.write(f"â€¢ '{word}': {score}ì ")
                                    if len(negative_words) > 5:
                                        st.write(f"... ì™¸ {len(negative_words) - 5}ê°œ ë”")
                    else:
                        st.warning(method1.get('error', ''))
                    
                    st.markdown("""
                    **âœ… ì¥ì :** ë¹ ë¥¸ ì²˜ë¦¬, ì´í•´í•˜ê¸° ì‰¬ì›€, êµ¬í˜„ ê°„ë‹¨  
                    **âŒ í•œê³„:** ë¬¸ë§¥ ë¬´ì‹œ, ë³µí•© ê°ì • ì²˜ë¦¬ ì–´ë ¤ì›€, ì‚¬ì „ ì˜ì¡´ì„±
                    """)
                    
                    st.markdown("---")
                    
                    # 2ë‹¨ê³„: TF-IDF + ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„
                    method2 = sentiment_comparison_result['method2_tfidf']
                    
                    st.markdown("## ğŸ¤– 2ë‹¨ê³„: TF-IDF + ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **TF-IDF**ë¡œ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    - **ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸**ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ê°ì„± ë¶„ë¥˜
                    - **í†µê³„ì  ì ‘ê·¼**ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ì„¸ë°€í•œ ë¶„ì„ ê°€ëŠ¥
                    """)
                    
                    if 'error' not in method2:
                        col1, col2 = st.columns([1, 1])
                        
                        with col1:
                            # TF-IDF ê¸°ë°˜ ê°ì„± ê²°ê³¼ (ìˆ˜ì •ëœ í‚¤ ì‚¬ìš©)
                            tfidf_sentiment = method2.get('sentiment', 'Unknown')
                            final_score = method2.get('final_score', 0)
                            emoji = method2.get('emoji', '')
                            
                            if 'ê¸ì •' in tfidf_sentiment:
                                st.success(f"**TF-IDF ê°ì„± ê²°ê³¼**: {tfidf_sentiment} {emoji}")
                            elif 'ë¶€ì •' in tfidf_sentiment:
                                st.error(f"**TF-IDF ê°ì„± ê²°ê³¼**: {tfidf_sentiment} {emoji}")
                            else:
                                st.info(f"**TF-IDF ê°ì„± ê²°ê³¼**: {tfidf_sentiment} {emoji}")
                            
                            # ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                            confidence = min(abs(final_score) * 30, 100)  # ì ìˆ˜ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                            st.metric("ì‹ ë¢°ë„", f"{confidence:.1f}%")
                            st.metric("TF-IDF ì ìˆ˜", f"{final_score:.3f}")
                        
                        with col2:
                            # ì£¼ìš” TF-IDF ë‹¨ì–´ë“¤
                            top_words = method2.get('top_tfidf_words', [])
                            if top_words:
                                st.write("**ì£¼ìš” TF-IDF ë‹¨ì–´ë“¤:**")
                                tfidf_data = []
                                for word, score in top_words[:8]:
                                    # numpy íƒ€ì… ì²˜ë¦¬
                                    score_val = float(score) if hasattr(score, 'item') else score
                                    tfidf_data.append({'ë‹¨ì–´': word, 'TF-IDF ì ìˆ˜': f"{score_val:.4f}"})
                                
                                df_tfidf = pd.DataFrame(tfidf_data)
                                st.dataframe(df_tfidf, use_container_width=True)
                            
                            # ê°ì„± ì ìˆ˜ ë°±ë¶„ìœ¨
                            positive_score = method2.get('positive_score', 0)
                            negative_score = method2.get('negative_score', 0)
                            st.write(f"**ê°ì„± ì ìˆ˜ ë¶„í•´:**")
                            st.write(f"â€¢ ê¸ì • ê°€ì¤‘ì¹˜: {positive_score:.3f}")
                            st.write(f"â€¢ ë¶€ì • ê°€ì¤‘ì¹˜: {negative_score:.3f}")
                        
                        # TF-IDF ì ìˆ˜ ì‹œê°í™”
                        if top_words:
                            words = [word for word, score in top_words[:10]]
                            # numpy íƒ€ì… ì²˜ë¦¬
                            scores = [float(score) if hasattr(score, 'item') else score for word, score in top_words[:10]]
                            
                            fig = go.Figure(data=[
                                go.Bar(
                                    x=words,
                                    y=scores,
                                    marker_color='#17a2b8',
                                    text=[f"{score:.3f}" for score in scores],
                                    textposition='auto'
                                )
                            ])
                            
                            fig.update_layout(
                                title="ì£¼ìš” ë‹¨ì–´ë³„ TF-IDF ì ìˆ˜",
                                xaxis_title="ë‹¨ì–´",
                                yaxis_title="TF-IDF ì ìˆ˜",
                                height=400
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                        
                    else:
                        st.warning(method2.get('error', ''))
                    
                    st.markdown("""
                    **âœ… ì¥ì :** ë†’ì€ ì •í™•ë„, ë‹¨ì–´ ì¤‘ìš”ë„ ê³ ë ¤, í†µê³„ì  ì‹ ë¢°ì„±  
                    **âŒ í•œê³„:** ê³„ì‚° ë³µì¡ë„, ë°ì´í„° ì˜ì¡´ì„±, ë¬¸ë§¥ ì œí•œì  ì´í•´
                    """)
                    
                    st.markdown("---")
                    
                    # 3ë‹¨ê³„: VADER ê°ì„± ë¶„ì„
                    method3 = sentiment_comparison_result['method3_vader']
                    
                    st.markdown("## ğŸ¯ 3ë‹¨ê³„: VADER ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **ë¬¸ë§¥ê³¼ ê°•ë„**ë¥¼ ê³ ë ¤í•œ ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
                    - **ê°ì • ê°•í™”ì–´, ë¶€ì •ì–´, êµ¬ë‘ì ** ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
                    - **ì‹¤ì‹œê°„ ì†Œì…œë¯¸ë””ì–´** í…ìŠ¤íŠ¸ì— ìµœì í™”ëœ ìµœì‹  ê¸°ë²•
                    """)
                    
                    if 'error' not in method3:
                        # VADER ê°ì„± ì ìˆ˜ (ìˆ˜ì •ëœ í‚¤ ì‚¬ìš©)
                        compound_score = method3.get('compound', 0)
                        detailed_scores = method3.get('detailed_scores', {})
                        
                        col1, col2 = st.columns([1, 1])
                        
                        with col1:
                            st.metric("VADER ë³µí•© ì ìˆ˜", f"{compound_score:.3f}")
                            
                            # ê°ì„± ê²°ê³¼
                            vader_sentiment = method3.get('sentiment', 'Unknown')
                            emoji = method3.get('emoji', '')
                            if 'ê¸ì •' in vader_sentiment:
                                st.success(f"**VADER ê°ì„± ê²°ê³¼**: {vader_sentiment} {emoji}")
                            elif 'ë¶€ì •' in vader_sentiment:
                                st.error(f"**VADER ê°ì„± ê²°ê³¼**: {vader_sentiment} {emoji}")
                            else:
                                st.info(f"**VADER ê°ì„± ê²°ê³¼**: {vader_sentiment} {emoji}")
                        
                        with col2:
                            st.write("**ì„¸ë¶€ ê°ì„± ì ìˆ˜:**")
                            st.write(f"â€¢ ê¸ì • (Positive): {detailed_scores.get('positive', 0):.3f}")
                            st.write(f"â€¢ ì¤‘ë¦½ (Neutral): {detailed_scores.get('neutral', 0):.3f}")
                            st.write(f"â€¢ ë¶€ì • (Negative): {detailed_scores.get('negative', 0):.3f}")
                        
                        # VADER ì„¸ë¶€ ì ìˆ˜ ì‹œê°í™”
                        categories = ['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •']
                        values = [detailed_scores.get('positive', 0), detailed_scores.get('neutral', 0), detailed_scores.get('negative', 0)]
                        colors = ['#28a745', '#6c757d', '#dc3545']
                        
                        fig = go.Figure(data=[
                            go.Bar(
                                x=categories,
                                y=values,
                                marker_color=colors,
                                text=[f"{val:.3f}" for val in values],
                                textposition='auto'
                            )
                        ])
                        
                        fig.update_layout(
                            title="VADER ì„¸ë¶€ ê°ì„± ì ìˆ˜ ë¶„í¬",
                            xaxis_title="ê°ì„± ë²”ì£¼",
                            yaxis_title="ì ìˆ˜ (0-1)",
                            height=400
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                    else:
                        st.warning(method3.get('error', ''))
                    
                    st.markdown("""
                    **âœ… ì¥ì :** ë¬¸ë§¥ ê³ ë ¤, ê°ì • ê°•ë„ ì¸¡ì •, ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥  
                    **âŒ í•œê³„:** ì–¸ì–´ë³„ íŠœë‹ í•„ìš”, ë„ë©”ì¸ íŠ¹í™” ì–´ë ¤ì›€
                    """)
                    
                    st.markdown("---")
                    
                    # 4ë‹¨ê³„: ë‹¤ì¤‘ ê°ì„± ë¶„ì„ (8ê°€ì§€ ê¸°ë³¸ ê°ì •)
                    method4 = sentiment_comparison_result['method4_emotions']
                    
                    st.markdown("## ğŸŒˆ 4ë‹¨ê³„: ë‹¤ì¤‘ ê°ì„± ë¶„ì„ - 8ê°€ì§€ ê¸°ë³¸ ê°ì • íƒì§€")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **8ê°€ì§€ ê¸°ë³¸ ê°ì •**ì„ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ë¶„ì„ (ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€, ë†€ëŒ, í˜ì˜¤, ì‹ ë¢°, ê¸°ëŒ€)
                    - **ê°ì •ë³„ í‚¤ì›Œë“œ ì‚¬ì „**ì„ í™œìš©í•œ ë‹¤ì°¨ì› ê°ì • ë¶„ì„
                    - **ê°ì •ì˜ ê°•ë„ì™€ ë‹¤ì–‘ì„±**ê¹Œì§€ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •
                    """)
                    
                    if 'error' not in method4:
                        # ì£¼ë„ì  ê°ì •ê³¼ ì „ì²´ í†µê³„
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            dominant_emotion = method4.get('dominant_emotion', 'ì¤‘ë¦½')
                            dominant_emoji = method4.get('dominant_emoji', 'ğŸ˜')
                            st.success(f"**ì£¼ë„ì  ê°ì •**: {dominant_emotion} {dominant_emoji}")
                            
                        with col2:
                            emotion_intensity = method4.get('emotion_intensity', 0)
                            st.metric("ê°ì • ê°•ë„", f"{emotion_intensity}%")
                            
                        with col3:
                            emotion_variety = method4.get('emotion_variety', 0)
                            st.metric("ê°ì • ë‹¤ì–‘ì„±", f"{emotion_variety}ê°€ì§€")
                        
                        # 8ê°€ì§€ ê°ì •ë³„ ìƒì„¸ ê²°ê³¼
                        st.markdown("### ğŸ­ ê°ì •ë³„ ìƒì„¸ ë¶„ì„")
                        emotion_details = method4.get('emotion_details', {})
                        
                        if emotion_details:
                            # 2í–‰ 4ì—´ë¡œ ê°ì • í‘œì‹œ
                            row1_cols = st.columns(4)
                            row2_cols = st.columns(4)
                            
                            emotions_list = list(emotion_details.items())
                            
                            # ì²« ë²ˆì§¸ í–‰ (ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€)
                            for i, col in enumerate(row1_cols):
                                if i < len(emotions_list):
                                    emotion_name, details = emotions_list[i]
                                    with col:
                                        score = details.get('score', 0)
                                        emoji = details.get('emoji', 'ğŸ˜')
                                        found_words = details.get('found_words', [])
                                        
                                        st.metric(f"{emoji} {emotion_name.split('(')[0].strip()}", f"{score}ê°œ")
                                        if found_words:
                                            st.write("**ë°œê²¬ëœ ë‹¨ì–´:**")
                                            for word in found_words[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                                                st.write(f"â€¢ {word}")
                                        else:
                                            st.write("ë°œê²¬ëœ ë‹¨ì–´ ì—†ìŒ")
                            
                            # ë‘ ë²ˆì§¸ í–‰ (ë†€ëŒ, í˜ì˜¤, ì‹ ë¢°, ê¸°ëŒ€)
                            for i, col in enumerate(row2_cols):
                                emotion_index = i + 4
                                if emotion_index < len(emotions_list):
                                    emotion_name, details = emotions_list[emotion_index]
                                    with col:
                                        score = details.get('score', 0)
                                        emoji = details.get('emoji', 'ğŸ˜')
                                        found_words = details.get('found_words', [])
                                        
                                        st.metric(f"{emoji} {emotion_name.split('(')[0].strip()}", f"{score}ê°œ")
                                        if found_words:
                                            st.write("**ë°œê²¬ëœ ë‹¨ì–´:**")
                                            for word in found_words[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                                                st.write(f"â€¢ {word}")
                                        else:
                                            st.write("ë°œê²¬ëœ ë‹¨ì–´ ì—†ìŒ")
                        
                        # ë¬¸ì¥ë³„ ê°ì • ë¶„ì„ ê²°ê³¼
                        sentence_emotions = method4.get('sentence_emotions', [])
                        if sentence_emotions:
                            st.markdown("### ğŸ“ ë¬¸ì¥ë³„ ê°ì • ë¶„ì„")
                            for i, sent_emotion in enumerate(sentence_emotions, 1):
                                sentence = sent_emotion.get('sentence', '')
                                emotion = sent_emotion.get('emotion', 'ì¤‘ë¦½')
                                emoji = sent_emotion.get('emoji', 'ğŸ˜')
                                score = sent_emotion.get('score', 0)
                                
                                st.write(f"**ë¬¸ì¥ {i}:** {sentence}")
                                st.write(f"â†’ {emoji} **{emotion}** (ê°ì • ë‹¨ì–´ {score}ê°œ)")
                                st.write("")
                        
                        # í•´ì„ ë° ì¡°ì–¸
                        interpretations = method4.get('interpretation', [])
                        if interpretations:
                            st.markdown("### ğŸ’¡ ê°ì • ë¶„ì„ í•´ì„")
                            for interp in interpretations:
                                st.write(f"â€¢ {interp}")
                    
                    else:
                        st.error(f"ë‹¤ì¤‘ ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {method4.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    
                    st.info("""
                    **âœ… ì¥ì :** ì„¸ë°€í•œ ê°ì • ë¶„ë¥˜, ê°ì • ê°•ë„ ì¸¡ì •, ë‹¤ì–‘ì„± ë¶„ì„ ê°€ëŠ¥  
                    **âŒ í•œê³„:** í‚¤ì›Œë“œ ê¸°ë°˜ í•œê³„, ë³µí•© ê°ì • ì²˜ë¦¬ ì–´ë ¤ì›€, ë¬¸ë§¥ ì˜ì¡´ì„±
                    """)
                    
                    # ê²°ê³¼ ë¹„êµ ë° í•™ìŠµ ì •ë¦¬
                    st.markdown("---")
                    st.subheader("ğŸ“Š 4ê°€ì§€ ê°ì„± ë¶„ì„ ë°©ë²• ê²°ê³¼ ë¹„êµ")
                    
                    # ë¹„êµ í…Œì´ë¸”
                    comparison_data = []
                    
                    if method1 and 'error' not in method1:
                        comparison_data.append({
                            'ë°©ë²•': '1. ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜',
                            'ê°ì„± ê²°ê³¼': method1.get('sentiment', 'Unknown'),
                            'ì ìˆ˜/ì‹ ë¢°ë„': f"{method1.get('total_score', 0)}",
                            'ì²˜ë¦¬ ì†ë„': 'ë¹ ë¦„',
                            'ì •í™•ë„': 'ë³´í†µ'
                        })
                    
                    if method2 and 'error' not in method2:
                        final_score = method2.get('final_score', 0)
                        confidence = min(abs(final_score) * 30, 100)
                        comparison_data.append({
                            'ë°©ë²•': '2. TF-IDF + ML',
                            'ê°ì„± ê²°ê³¼': method2.get('sentiment', 'Unknown'),
                            'ì ìˆ˜/ì‹ ë¢°ë„': f"{confidence:.1f}%",
                            'ì²˜ë¦¬ ì†ë„': 'ë³´í†µ',
                            'ì •í™•ë„': 'ë†’ìŒ'
                        })
                    
                    if method3 and 'error' not in method3:
                        comparison_data.append({
                            'ë°©ë²•': '3. VADER ê³ ê¸‰',
                            'ê°ì„± ê²°ê³¼': method3.get('sentiment', 'Unknown'),
                            'ì ìˆ˜/ì‹ ë¢°ë„': f"{method3.get('compound', 0):.3f}",
                            'ì²˜ë¦¬ ì†ë„': 'ë¹ ë¦„',
                            'ì •í™•ë„': 'ë§¤ìš° ë†’ìŒ'
                        })
                    
                    if method4 and 'error' not in method4:
                        comparison_data.append({
                            'ë°©ë²•': '4. ë‹¤ì¤‘ ê°ì„±',
                            'ê°ì„± ê²°ê³¼': method4.get('dominant_emotion', 'Unknown'),
                            'ì ìˆ˜/ì‹ ë¢°ë„': f"{method4.get('emotion_intensity', 0)}%",
                            'ì²˜ë¦¬ ì†ë„': 'ë³´í†µ',
                            'ì •í™•ë„': 'ì„¸ë°€í•¨'
                        })
                    
                    if comparison_data:
                        df_sentiment_comparison = pd.DataFrame(comparison_data)
                        st.table(df_sentiment_comparison)
                    
                    # í•™ìŠµ ì •ë¦¬
                    st.subheader("ğŸ“ ê°ì„± ë¶„ì„ í•™ìŠµ ì •ë¦¬")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.info("""
                        **ğŸ” ë°°ìš´ ë‚´ìš©:**
                        - ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜ì˜ ë‹¨ìˆœ í•©ì‚° ë°©ì‹
                        - TF-IDFë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¶„ì„
                        - VADERì˜ ë¬¸ë§¥ ê³ ë ¤ ê³ ê¸‰ ê·œì¹™
                        - ë‹¤ì¤‘ ê°ì„±ìœ¼ë¡œ ì„¸ë°€í•œ ê°ì • ë¶„ë¥˜ ì²´í—˜
                        - ê° ë°©ë²•ë¡ ì˜ ì¥ë‹¨ì ê³¼ ì ìš© ë¶„ì•¼
                        """)
                    
                    with col2:
                        st.success("""
                        **ğŸ’¡ ì‹¤ë¬´ ì ìš©:**
                        - ë¹ ë¥¸ ë¶„ì„: ì–´íœ˜ ì‚¬ì „ ê¸°ë°˜
                        - ì •í™•í•œ ë¶„ë¥˜: TF-IDF + ë¨¸ì‹ ëŸ¬ë‹
                        - ì†Œì…œë¯¸ë””ì–´: VADER
                        - ì„¸ë°€í•œ ë¶„ì„: ë‹¤ì¤‘ ê°ì„±
                        - ì¢…í•© ë¶„ì„: ì—¬ëŸ¬ ë°©ë²• ì¡°í•© í™œìš©
                        """)
                    
                    # ì„¸ì…˜ì— ê²°ê³¼ ì €ì¥
                    st.session_state.educational_sentiment_results = sentiment_comparison_result
                    
                    st.success("âœ… í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ê°ì„± ë¶„ì„ ì›ë¦¬ ì²´í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                else:
                    st.error("ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 2. í’ˆì‚¬ ë¶„ì„ íƒ­
    with analysis_tabs[1]:
        st.subheader("ğŸ“ í’ˆì‚¬ ë¶„ì„ 3ë‹¨ê³„ ì›ë¦¬ ì²´í—˜")
        st.markdown("í˜•íƒœì†Œ ë¶„ì„ì˜ 3ê°€ì§€ ì ‘ê·¼ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì²´í—˜í•˜ë©° ì–¸ì–´ì²˜ë¦¬ ì›ë¦¬ë¥¼ ì´í•´í•´ë³´ì„¸ìš”.")
        
        # í•­ìƒ í‘œì‹œë˜ë„ë¡ ë³€ê²½
        with st.spinner("í’ˆì‚¬ ë¶„ì„ ì›ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ëŠ” ì¤‘..."):
            try:
                # í†µí•© í…ìŠ¤íŠ¸ë¡œ êµìœ¡ì  í’ˆì‚¬ ë¶„ì„ ì‹¤í–‰
                
                # 1ë‹¨ê³„: ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜
                pos_method1 = preprocessor.educational_pos_analysis_step1_manual_rules(all_essays_text)
                st.success("âœ… 1ë‹¨ê³„ ì„±ê³µ!")
            except Exception as e:
                st.error(f"âŒ 1ë‹¨ê³„ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                pos_method1 = {'error': str(e)}
                
        # 2ë‹¨ê³„: NLTK ê¸°ë³¸ í’ˆì‚¬ íƒœê¹…
        try:
            pos_method2 = preprocessor.educational_pos_analysis_step2_nltk_basic(all_essays_text)
            st.success("âœ… 2ë‹¨ê³„ ì„±ê³µ!")
        except Exception as e:
            st.error(f"âŒ 2ë‹¨ê³„ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
            pos_method2 = {'error': str(e)}
        
        # 3ë‹¨ê³„: íŒ¨í„´ ë°œê²¬
        try:
            pos_method3 = preprocessor.educational_pos_analysis_step3_pattern_discovery(all_essays_text)
            st.success("âœ… 3ë‹¨ê³„ ì„±ê³µ!")
        except Exception as e:
            st.error(f"âŒ 3ë‹¨ê³„ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
            pos_method3 = {'error': str(e)}
                
        pos_comparison_result = {
            'essay_info': {
                'topic': f"{username}ë‹˜ì˜ ëª¨ë“  ì—ì„¸ì´ í†µí•© í…ìŠ¤íŠ¸",
                'text_preview': all_essays_text[:200] + "...",
                'total_words': len(all_essays_text.split()),
                'total_essays': len(essay_data)
            },
            'method1_manual': pos_method1,
            'method2_nltk': pos_method2,
            'method3_patterns': pos_method3
        }
                
        if pos_comparison_result:
            # ì—ì„¸ì´ ì •ë³´
            essay_info = pos_comparison_result['essay_info']
            st.subheader(f"ğŸ“ í’ˆì‚¬ ë¶„ì„ ëŒ€ìƒ: {essay_info['topic']}")
            st.write(f"â€¢ ì´ ì—ì„¸ì´ ìˆ˜: {essay_info['total_essays']}ê°œ")
            st.write(f"â€¢ ì´ ë‹¨ì–´ ìˆ˜: {essay_info['total_words']}ê°œ")
            
            st.markdown("**ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:**")
            st.text_area("", essay_info['text_preview'], height=100, disabled=True, key="pos_preview_text")
            
            st.markdown("---")
            
            # ìƒì„¸ UI í‘œì‹œ
            essay_info = {
                'topic': f"{username}ë‹˜ì˜ ëª¨ë“  ì—ì„¸ì´ í†µí•© í…ìŠ¤íŠ¸",
                'text_preview': all_essays_text[:200] + "...",
                'total_words': len(all_essays_text.split()),
                'total_essays': len(essay_data)
            }
            
            # 1ë‹¨ê³„: ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜
            st.markdown("## ğŸ“– 1ë‹¨ê³„: ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜ í’ˆì‚¬ ë¶„ì„")
            st.markdown("""
            **ğŸ” ì›ë¦¬ ì„¤ëª…:**
            - **ë‹¨ì–´ íŒ¨í„´**ê³¼ **ì–´ë¯¸**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í’ˆì‚¬ ë¶„ë¥˜
            - **ê·œì¹™ ê¸°ë°˜** ì ‘ê·¼ë²•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥
            - **ë¹ ë¥¸ ì²˜ë¦¬**ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬ ì–´ë ¤ì›€
            """)
            
            
            if pos_method1 and 'error' not in pos_method1:
                pos_counts = pos_method1.get('pos_counts', {})
                if pos_counts:
                    # í’ˆì‚¬ë³„ ì°¨íŠ¸
                    fig = go.Figure(data=[
                        go.Bar(
                            x=list(pos_counts.keys()),
                            y=list(pos_counts.values()),
                            marker_color='lightblue',
                            text=list(pos_counts.values()),
                            textposition='auto'
                        )
                    ])
                    fig.update_layout(
                        title="1ë‹¨ê³„: ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜ í’ˆì‚¬ ë¶„í¬",
                        xaxis_title="í’ˆì‚¬",
                        yaxis_title="ë‹¨ì–´ ê°œìˆ˜",
                        height=300
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # í’ˆì‚¬ë³„ ë¹„ìœ¨ íŒŒì´ ì°¨íŠ¸ ì¶”ê°€
                    pos_ratios = pos_method1.get('pos_ratios', {})
                    if pos_ratios:
                        st.markdown("**ğŸ“Š í’ˆì‚¬ë³„ ë¹„ìœ¨ ë¶„í¬:**")
                        
                        # 0ì´ ì•„ë‹Œ ê°’ë§Œ íŒŒì´ ì°¨íŠ¸ì— í‘œì‹œ
                        filtered_ratios = {k: v for k, v in pos_ratios.items() if v > 0}
                        
                        if filtered_ratios:
                            pie_fig = go.Figure(data=[go.Pie(
                                labels=list(filtered_ratios.keys()),
                                values=list(filtered_ratios.values()),
                                hole=0.3,  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ê¸°
                                textinfo='label+percent',
                                textfont_size=12,
                                marker=dict(
                                    colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'],
                                    line=dict(color='#FFFFFF', width=2)
                                )
                            )])
                            
                            pie_fig.update_layout(
                                title="1ë‹¨ê³„: í’ˆì‚¬ë³„ ë¹„ìœ¨ ë¶„í¬",
                                height=400,
                                showlegend=True,
                                legend=dict(
                                    orientation="v",
                                    yanchor="middle",
                                    y=0.5,
                                    xanchor="left",
                                    x=1.01
                                )
                            )
                            st.plotly_chart(pie_fig, use_container_width=True)
                            
                            # ìƒì„¸ í†µê³„ í‘œì‹œ
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ì´ ë¶„ì„ ë‹¨ì–´", f"{pos_method1.get('total_words', 0)}ê°œ")
                            with col2:
                                most_common = max(filtered_ratios, key=filtered_ratios.get)
                                st.metric("ê°€ì¥ ë§ì€ í’ˆì‚¬", f"{most_common} ({filtered_ratios[most_common]:.1f}%)")
                            with col3:
                                coverage = 100 - filtered_ratios.get('ê¸°íƒ€', 0)
                                st.metric("í’ˆì‚¬ ì¸ì‹ë¥ ", f"{coverage:.1f}%")
                
                st.success("âœ… 1ë‹¨ê³„ ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜ í’ˆì‚¬ ë¶„ì„ ì™„ë£Œ!")
            else:
                st.warning("1ë‹¨ê³„ ë¶„ì„ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
            st.markdown("---")
            
            # 2ë‹¨ê³„: NLTK ê¸°ë°˜
            st.markdown("## ğŸ¤– 2ë‹¨ê³„: NLTK ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ í’ˆì‚¬ ë¶„ì„")
            st.markdown("""
            **ğŸ” ì›ë¦¬ ì„¤ëª…:**
            - **í†µê³„ ëª¨ë¸**ê³¼ **ê¸°ê³„í•™ìŠµ**ì„ í™œìš©í•œ í’ˆì‚¬ íƒœê¹…
            - **ë¬¸ë§¥ ì •ë³´**ë¥¼ ê³ ë ¤í•˜ì—¬ ë” ì •í™•í•œ ë¶„ë¥˜
            - **ëŒ€ìš©ëŸ‰ ë°ì´í„°**ë¡œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©
            """)
            
            
            if pos_method2 and 'error' not in pos_method2:
                st.success("âœ… 2ë‹¨ê³„ NLTK ê¸°ê³„í•™ìŠµ ê¸°ë°˜ í’ˆì‚¬ ë¶„ì„ ì™„ë£Œ!")
                pos_tagged = pos_method2.get('pos_tagged_words', [])
                if pos_tagged:
                    # Before/After ë¹„êµ UI ì¶”ê°€
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**ğŸ”´ ë³€í™˜ ì „ (ì›ë³¸ í…ìŠ¤íŠ¸):**")
                        sample_words = [word for word, pos in pos_tagged[:10]]
                        for i, word in enumerate(sample_words):
                            st.write(f"{i+1}. {word}")
                    
                    with col2:
                        st.markdown("**ğŸ”µ ë³€í™˜ í›„ (í’ˆì‚¬ íƒœê¹…):**")
                        for i, (word, pos) in enumerate(pos_tagged[:10]):
                            # í’ˆì‚¬ë³„ ìƒ‰ìƒ ì½”ë”©
                            if pos.startswith('NN'):  # ëª…ì‚¬
                                color = "ğŸŸ¦"
                            elif pos.startswith('VB'):  # ë™ì‚¬
                                color = "ğŸŸ©"
                            elif pos.startswith('JJ'):  # í˜•ìš©ì‚¬
                                color = "ğŸŸ¨"
                            elif pos.startswith('RB'):  # ë¶€ì‚¬
                                color = "ğŸŸª"
                            elif pos in ['DT', 'IN', 'TO']:  # ê´€ì‚¬, ì „ì¹˜ì‚¬
                                color = "ğŸŸ«"
                            else:
                                color = "âšª"
                            st.write(f"{i+1}. {color} {word} â†’ **{pos}**")
                    
                    # í’ˆì‚¬ ë³€í™” í†µê³„
                    pos_counts = {}
                    for word, pos in pos_tagged:
                        pos_counts[pos] = pos_counts.get(pos, 0) + 1
                    
                    st.markdown("**ğŸ“Š í’ˆì‚¬ë³„ ë¶„í¬ (ìƒìœ„ 8ê°œ):**")
                    top_pos = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)[:8]
                    
                    # í’ˆì‚¬ ë¶„í¬ ì°¨íŠ¸
                    fig = go.Figure(data=[
                        go.Bar(
                            x=[pos for pos, count in top_pos],
                            y=[count for pos, count in top_pos],
                            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', 
                                        '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F'][:len(top_pos)],
                            text=[f"{count}ê°œ" for pos, count in top_pos],
                            textposition='auto'
                        )
                    ])
                    fig.update_layout(
                        title="2ë‹¨ê³„: NLTK ê¸°ë°˜ í’ˆì‚¬ ë¶„í¬ (ìƒìœ„ 8ê°œ)",
                        xaxis_title="í’ˆì‚¬ íƒœê·¸",
                        yaxis_title="ê°œìˆ˜",
                        height=350
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ì²´í—˜í˜• ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ
                    st.markdown("**ğŸ¯ ì²´í—˜í•´ë³´ê¸°: ì§ì ‘ ë¬¸ì¥ ì…ë ¥í•˜ì—¬ í’ˆì‚¬ ë¶„ì„**")
                    user_sentence = st.text_input("ë¶„ì„í•  ë¬¸ì¥ì„ ì˜ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”:", 
                                                 value="I love studying English because it helps me grow.",
                                                 placeholder="ì˜ˆ: I enjoy reading books in the library.",
                                                 key="user_pos_input")
                    
                    # ë²„íŠ¼ê³¼ ê²°ê³¼ë¥¼ í•¨ê»˜ ì²˜ë¦¬
                    if st.button("ğŸ” ë‚´ ë¬¸ì¥ í’ˆì‚¬ ë¶„ì„í•˜ê¸°", key="pos_demo") and user_sentence.strip():
                        with st.spinner("ë¬¸ì¥ì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                            try:
                                import nltk
                                tokens = nltk.word_tokenize(user_sentence)
                                pos_demo = nltk.pos_tag(tokens)
                                
                                st.write(f"**ì…ë ¥í•œ ë¬¸ì¥:** {user_sentence}")
                                st.write("**í’ˆì‚¬ ë¶„ì„ ê²°ê³¼:**")
                                
                                # í’ˆì‚¬ë³„ ìƒ‰ìƒ í‘œì‹œ (ê°„ë‹¨í•œ ë°©ì‹)
                                st.write("**í’ˆì‚¬ë³„ ë¶„ì„ ê²°ê³¼:**")
                                
                                # ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í‘œì‹œ
                                cols = st.columns(4)
                                col_idx = 0
                                
                                for word, pos in pos_demo:
                                    # í’ˆì‚¬ë³„ ì´ëª¨ì§€ì™€ ìƒ‰ìƒ
                                    if pos.startswith('NN'):  # ëª…ì‚¬
                                        emoji = "ğŸŸ¦"
                                        category = "ëª…ì‚¬"
                                    elif pos.startswith('VB') or pos == 'MD':  # ë™ì‚¬
                                        emoji = "ğŸŸ©"
                                        category = "ë™ì‚¬"
                                    elif pos.startswith('JJ'):  # í˜•ìš©ì‚¬
                                        emoji = "ğŸŸ¨"
                                        category = "í˜•ìš©ì‚¬"
                                    elif pos.startswith('RB'):  # ë¶€ì‚¬
                                        emoji = "ğŸŸª"
                                        category = "ë¶€ì‚¬"
                                    elif pos in ['PRP', 'PRP$']:  # ëŒ€ëª…ì‚¬
                                        emoji = "ğŸŸ¡"
                                        category = "ëŒ€ëª…ì‚¬"
                                    elif pos in ['IN', 'TO']:  # ì „ì¹˜ì‚¬
                                        emoji = "ğŸŸ£"
                                        category = "ì „ì¹˜ì‚¬"
                                    elif pos == 'DT':  # ê´€ì‚¬
                                        emoji = "ğŸŸ¢"
                                        category = "ê´€ì‚¬"
                                    elif pos == 'CC':  # ì ‘ì†ì‚¬
                                        emoji = "ğŸŸ "
                                        category = "ì ‘ì†ì‚¬"
                                    else:
                                        emoji = "âšª"
                                        category = "ê¸°íƒ€"
                                    
                                    with cols[col_idx % 4]:
                                        st.write(f"{emoji} **{word}**")
                                        st.caption(f"{pos} ({category})")
                                    
                                    col_idx += 1
                                
                                # í’ˆì‚¬ í†µê³„ ì¶”ê°€
                                pos_stats = {}
                                for word, pos in pos_demo:
                                    category = 'ê¸°íƒ€'
                                    if pos.startswith('NN'):
                                        category = 'ëª…ì‚¬'
                                    elif pos.startswith('VB') or pos == 'MD':
                                        category = 'ë™ì‚¬/ì¡°ë™ì‚¬'
                                    elif pos.startswith('JJ'):
                                        category = 'í˜•ìš©ì‚¬'
                                    elif pos.startswith('RB'):
                                        category = 'ë¶€ì‚¬'
                                    elif pos in ['PRP', 'PRP$']:
                                        category = 'ëŒ€ëª…ì‚¬'
                                    elif pos in ['DT', 'IN', 'CC']:
                                        category = 'ê¸°ëŠ¥ì–´'
                                    
                                    pos_stats[category] = pos_stats.get(category, 0) + 1
                                
                                if pos_stats:
                                    st.markdown("**ğŸ“Š í’ˆì‚¬ ë¶„í¬:**")
                                    col1, col2 = st.columns(2)
                                    
                                    with col1:
                                        for category, count in pos_stats.items():
                                            st.write(f"â€¢ {category}: {count}ê°œ")
                                    
                                    with col2:
                                        total_words = sum(pos_stats.values())
                                        most_common = max(pos_stats.items(), key=lambda x: x[1])
                                        st.metric("ì´ ë‹¨ì–´ ìˆ˜", f"{total_words}ê°œ")
                                        st.metric("ê°€ì¥ ë§ì€ í’ˆì‚¬", f"{most_common[0]} ({most_common[1]}ê°œ)")
                                
                                # í’ˆì‚¬ ì„¤ëª… ì¶”ê°€
                                st.markdown("---")
                                st.info("""
                                **í’ˆì‚¬ ë¶„ë¥˜ ê¸°ì¤€:**
                                ğŸŸ¦ ëª…ì‚¬(NN) | ğŸŸ© ë™ì‚¬(VB) | ğŸŸ¨ í˜•ìš©ì‚¬(JJ) | ğŸŸª ë¶€ì‚¬(RB) | ğŸŸ¡ ëŒ€ëª…ì‚¬(PRP) | ğŸŸ£ ì „ì¹˜ì‚¬(IN) | ğŸŸ¢ ê´€ì‚¬(DT) | ğŸŸ  ì ‘ì†ì‚¬(CC)
                                """)
                                
                            except Exception as e:
                                st.error(f"ë¬¸ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                                st.warning("ì˜ì–´ ë¬¸ì¥ì„ ì •í™•íˆ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.warning("2ë‹¨ê³„ ë¶„ì„ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
            st.markdown("---")
            
            # 3ë‹¨ê³„: íŒ¨í„´ ë°œê²¬
            st.markdown("## ğŸ¯ 3ë‹¨ê³„: íŒ¨í„´ ë°œê²¬ ë° ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„")
            st.markdown("""
            **ğŸ” ì›ë¦¬ ì„¤ëª…:**
            - **í’ˆì‚¬ íŒ¨í„´**ê³¼ **ì–¸ì–´ì  íŠ¹ì„±** ë¶„ì„
            - **ë¬¸ì¥ êµ¬ì¡°**ì™€ **ì–´íœ˜ ë‹¤ì–‘ì„±** ì¸¡ì •
            - **ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼** íŠ¹ì„± ë°œê²¬
            """)
            
            if pos_method3 and 'error' not in pos_method3:
                st.success("âœ… 3ë‹¨ê³„ íŒ¨í„´ ë°œê²¬ ë° ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„ ì™„ë£Œ!")
                
                # íŒ¨í„´ ë¶„ì„ ê²°ê³¼
                patterns = pos_method3.get('common_patterns', [])
                if patterns:
                    st.write("**ğŸ” ë°œê²¬ëœ í’ˆì‚¬ íŒ¨í„´ (ìƒìœ„ 5ê°œ):**")
                    for i, pattern in enumerate(patterns[:5], 1):
                        st.write(f"{i}. `{pattern}`")
                
                # ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„
                linguistic_features = pos_method3.get('linguistic_features', {})
                if linguistic_features:
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ë¬¸ì¥ ë³µì¡ë„", f"{linguistic_features.get('complexity_score', 0):.2f}")
                    with col2:
                        st.metric("ì–´íœ˜ ë‹¤ì–‘ì„±", f"{linguistic_features.get('lexical_diversity', 0):.2f}")
                    with col3:
                        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{linguistic_features.get('avg_sentence_length', 0):.1f}")
                
                # ì²´í—˜í˜• íŒ¨í„´ ë¶„ì„ ìš”ì†Œ ì¶”ê°€
                st.markdown("**ğŸ¯ ì²´í—˜í•´ë³´ê¸°: ì§ì ‘ ì…ë ¥í•œ ë¬¸ì¥ì˜ ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„**")
                
                user_pattern_text = st.text_area("ë¶„ì„í•  ë¬¸ì¥ë“¤ì„ ì˜ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš” (ì—¬ëŸ¬ ë¬¸ì¥ ê°€ëŠ¥):", 
                                               value="I love reading books. They help me learn new things. Sometimes I write stories about my dreams.",
                                               height=100,
                                               placeholder="ì—¬ëŸ¬ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ê° ë¬¸ì¥ì˜ íŠ¹ì„±ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.",
                                               key="user_pattern_input")
                
                if st.button("ğŸ” ë‚´ ë¬¸ì¥ì˜ ì–¸ì–´ì  íŒ¨í„´ ë¶„ì„í•˜ê¸°", key="pattern_analysis_user") and user_pattern_text.strip():
                    with st.spinner("ì–¸ì–´ì  íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                        try:
                            import nltk
                            
                            # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ë³„ë¡œ ë¶„ë¦¬
                            sentences = nltk.sent_tokenize(user_pattern_text)
                            
                            st.markdown(f"**ğŸ“ ì´ {len(sentences)}ê°œ ë¬¸ì¥ì˜ ìƒì„¸ ë¶„ì„:**")
                            
                            sentence_analyses = []
                            for i, sentence in enumerate(sentences[:5], 1):  # ìµœëŒ€ 5ê°œ ë¬¸ì¥ê¹Œì§€
                                words = nltk.word_tokenize(sentence)
                                pos_tags = nltk.pos_tag(words)
                                
                                # í’ˆì‚¬ë³„ ì¹´ìš´íŠ¸
                                nouns = sum(1 for _, pos in pos_tags if pos.startswith('NN'))
                                verbs = sum(1 for _, pos in pos_tags if pos.startswith('VB'))
                                adjectives = sum(1 for _, pos in pos_tags if pos.startswith('JJ'))
                                total_content_words = nouns + verbs + adjectives
                                
                                # ë¹„ìœ¨ ê³„ì‚°
                                if total_content_words > 0:
                                    noun_ratio = (nouns / total_content_words) * 100
                                    verb_ratio = (verbs / total_content_words) * 100
                                    adj_ratio = (adjectives / total_content_words) * 100
                                else:
                                    noun_ratio = verb_ratio = adj_ratio = 0
                                
                                # ë¬¸ì¥ ë³µì¡ë„ íŒë‹¨ (15ë‹¨ì–´ ì´ìƒ ë˜ëŠ” ë³µì¡í•œ ì ‘ì†ì‚¬ í¬í•¨)
                                complex_words = ['because', 'although', 'since', 'while', 'if', 'when', 'where', 'which', 'that']
                                is_complex = len(words) > 15 or any(word.lower() in complex_words for word in words)
                                complexity = "Complex" if is_complex else "Simple"
                                
                                analysis = {
                                    'sentence': sentence,
                                    'word_count': len(words),
                                    'nouns': nouns,
                                    'verbs': verbs,
                                    'adjectives': adjectives,
                                    'content_words': total_content_words,
                                    'noun_ratio': noun_ratio,
                                    'verb_ratio': verb_ratio,
                                    'adj_ratio': adj_ratio,
                                    'complexity': complexity,
                                    'pos_tags': pos_tags
                                }
                                
                                sentence_analyses.append(analysis)
                                
                                with st.expander(f"ğŸ“„ {i}ë²ˆì§¸ ë¬¸ì¥ ìƒì„¸ ë¶„ì„"):
                                    st.write(f"**ë¬¸ì¥:** {sentence}")
                                    
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.metric("ë‹¨ì–´ ê°œìˆ˜", f"{len(words)}ê°œ")
                                        st.metric("ëª…ì‚¬ ê°œìˆ˜", f"{nouns}ê°œ")
                                    with col2:
                                        st.metric("ë™ì‚¬ ê°œìˆ˜", f"{verbs}ê°œ")
                                        st.metric("í˜•ìš©ì‚¬ ê°œìˆ˜", f"{adjectives}ê°œ")
                                    with col3:
                                        st.metric("ë¬¸ì¥ ë³µì¡ë„", complexity)
                                        st.metric("ë‚´ìš©ì–´ ì´ê³„", f"{total_content_words}ê°œ")
                                    
                                    # í’ˆì‚¬ ë¹„ìœ¨ ì‹œê°í™”
                                    if total_content_words > 0:
                                        fig = go.Figure(data=[go.Bar(
                                            x=['ëª…ì‚¬', 'ë™ì‚¬', 'í˜•ìš©ì‚¬'],
                                            y=[noun_ratio, verb_ratio, adj_ratio],
                                            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
                                            text=[f"{noun_ratio:.1f}%", f"{verb_ratio:.1f}%", f"{adj_ratio:.1f}%"],
                                            textposition='auto'
                                        )])
                                        fig.update_layout(
                                            title=f"{i}ë²ˆì§¸ ë¬¸ì¥ì˜ í’ˆì‚¬ ë¹„ìœ¨",
                                            yaxis_title="ë¹„ìœ¨ (%)",
                                            height=300
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                    
                                    # í’ˆì‚¬ íƒœê·¸ ìƒì„¸ ë³´ê¸°
                                    if st.checkbox(f"í’ˆì‚¬ íƒœê·¸ ìƒì„¸ ë³´ê¸°", key=f"pos_detail_{i}"):
                                        st.write("**í’ˆì‚¬ë³„ ë‹¨ì–´:**")
                                        pos_groups = {}
                                        for word, pos in pos_tags:
                                            if pos not in pos_groups:
                                                pos_groups[pos] = []
                                            pos_groups[pos].append(word)
                                        
                                        for pos, words_list in pos_groups.items():
                                            st.write(f"â€¢ **{pos}**: {', '.join(words_list)}")
                            
                            # ì „ì²´ í…ìŠ¤íŠ¸ ì¢…í•© ë¶„ì„
                            if len(sentence_analyses) > 1:
                                st.markdown("---")
                                st.markdown("### ğŸ“Š ì „ì²´ í…ìŠ¤íŠ¸ ì¢…í•© ë¶„ì„")
                                
                                # í‰ê·  í†µê³„ ê³„ì‚°
                                avg_noun_ratio = sum(s['noun_ratio'] for s in sentence_analyses) / len(sentence_analyses)
                                avg_verb_ratio = sum(s['verb_ratio'] for s in sentence_analyses) / len(sentence_analyses)
                                avg_adj_ratio = sum(s['adj_ratio'] for s in sentence_analyses) / len(sentence_analyses)
                                avg_sentence_length = sum(s['word_count'] for s in sentence_analyses) / len(sentence_analyses)
                                complex_count = sum(1 for s in sentence_analyses if s['complexity'] == 'Complex')
                                complexity_ratio = (complex_count / len(sentence_analyses)) * 100
                                
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("í‰ê·  ëª…ì‚¬ ë¹„ìœ¨", f"{avg_noun_ratio:.1f}%")
                                    st.metric("í‰ê·  ë™ì‚¬ ë¹„ìœ¨", f"{avg_verb_ratio:.1f}%")
                                with col2:
                                    st.metric("í‰ê·  í˜•ìš©ì‚¬ ë¹„ìœ¨", f"{avg_adj_ratio:.1f}%")
                                    st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{avg_sentence_length:.1f}ë‹¨ì–´")
                                with col3:
                                    st.metric("ë³µì¡í•œ ë¬¸ì¥ ë¹„ìœ¨", f"{complexity_ratio:.1f}%")
                                    st.metric("ì´ ë¬¸ì¥ ìˆ˜", f"{len(sentence_analyses)}ê°œ")
                                
                                # ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ íŒì •
                                st.markdown("**âœ¨ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ë¶„ì„:**")
                                if avg_noun_ratio > 50:
                                    st.info("ğŸ“Š **ì •ë³´ ì „ë‹¬í˜•**: ëª…ì‚¬ê°€ ë§ì•„ ê°ê´€ì ì´ê³  ì •ë³´ ì¤‘ì‹¬ì ì¸ ê¸€ì“°ê¸°ì…ë‹ˆë‹¤.")
                                elif avg_verb_ratio > 30:
                                    st.success("ğŸƒ **ë™ì  í‘œí˜„í˜•**: ë™ì‚¬ê°€ ë§ì•„ í™œë™ì ì´ê³  ìƒë™ê° ìˆëŠ” ê¸€ì“°ê¸°ì…ë‹ˆë‹¤.")
                                elif avg_adj_ratio > 20:
                                    st.warning("ğŸ¨ **ë¬˜ì‚¬ì **: í˜•ìš©ì‚¬ê°€ í’ë¶€í•´ ê°ì •ì ì´ê³  ë¬˜ì‚¬ì ì¸ ê¸€ì“°ê¸°ì…ë‹ˆë‹¤.")
                                else:
                                    st.info("âš–ï¸ **ê· í˜•ì¡íŒ**: ê° í’ˆì‚¬ê°€ ê³ ë¥´ê²Œ ì‚¬ìš©ëœ ê· í˜•ì¡íŒ ê¸€ì“°ê¸°ì…ë‹ˆë‹¤.")
                                
                                # ë³µì¡ë„ í‰ê°€
                                if complexity_ratio > 60:
                                    st.success("ğŸ“ **ê³ ê¸‰ ìˆ˜ì¤€**: ë³µì¡í•˜ê³  ì •êµí•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!")
                                elif complexity_ratio > 30:
                                    st.info("ğŸ“ **ì¤‘ê¸‰ ìˆ˜ì¤€**: ì ì ˆí•œ ë³µì¡ì„±ì„ ê°€ì§„ ê¸€ì“°ê¸°ì…ë‹ˆë‹¤.")
                                else:
                                    st.warning("ğŸ“š **ê¸°ì´ˆ ìˆ˜ì¤€**: ë” ë‹¤ì–‘í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                            
                        except Exception as e:
                            st.error(f"íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                            st.warning("ì˜ì–´ ë¬¸ì¥ì„ ì •í™•íˆ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                
                # ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ì²´í—˜ ìš”ì†Œ
                writing_style = pos_method3.get('writing_style', {})
                if writing_style:
                    st.markdown("**âœ¨ ë°œê²¬ëœ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼:**")
                    for style_key, style_desc in writing_style.items():
                        if style_key == 'noun_heavy':
                            st.info(f"ğŸ“Š **ì •ë³´ ì „ë‹¬í˜•**: {style_desc}")
                        elif style_key == 'verb_heavy':
                            st.success(f"ğŸƒ **ë™ì  í‘œí˜„í˜•**: {style_desc}")
                        elif style_key == 'descriptive':
                            st.warning(f"ğŸ¨ **ë¬˜ì‚¬ì **: {style_desc}")
                        else:
                            st.info(f"âš–ï¸ **ê· í˜•ì¡íŒ ìŠ¤íƒ€ì¼**: {style_desc}")
                
                # ì¸í„°ë™í‹°ë¸Œ íŒ¨í„´ ì²´í—˜
                st.markdown("**ğŸ” íŒ¨í„´ ë°œê²¬ ì²´í—˜:**")
                if st.button("ğŸ¯ ë‚´ ê¸€ì“°ê¸° íŒ¨í„´ ë¶„ì„í•´ë³´ê¸°", key="pattern_analysis_demo"):
                    overall_patterns = pos_method3.get('overall_patterns', {})
                    if overall_patterns:
                        st.markdown("### ğŸ“Š ì¢…í•© íŒ¨í„´ ë¶„ì„ ê²°ê³¼")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**ğŸ“ˆ í’ˆì‚¬ ë°€ë„ ë¶„ì„:**")
                            densities = {
                                'í‰ê·  ëª…ì‚¬ ë°€ë„': overall_patterns.get('avg_noun_density', 0),
                                'í‰ê·  ë™ì‚¬ ë°€ë„': overall_patterns.get('avg_verb_density', 0),
                                'í‰ê·  í˜•ìš©ì‚¬ ë°€ë„': overall_patterns.get('avg_adj_density', 0)
                            }
                            
                            for name, value in densities.items():
                                st.write(f"â€¢ {name}: {value:.1f}%")
                        
                        with col2:
                            st.markdown("**ğŸ—ï¸ ë¬¸ì¥ êµ¬ì¡° ë¶„ì„:**")
                            complexity_ratio = overall_patterns.get('complexity_ratio', 0)
                            total_sentences = overall_patterns.get('total_sentences', 0)
                            
                            st.write(f"â€¢ ë³µì¡í•œ ë¬¸ì¥ ë¹„ìœ¨: {complexity_ratio:.1f}%")
                            st.write(f"â€¢ ë¶„ì„ëœ ë¬¸ì¥ ìˆ˜: {total_sentences}ê°œ")
                        
                        # ì¢…í•© í‰ê°€
                        if complexity_ratio > 60:
                            st.success("ğŸ“ **ê³ ê¸‰ ìˆ˜ì¤€**: ë³µì¡í•˜ê³  ì •êµí•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!")
                        elif complexity_ratio > 30:
                            st.info("ğŸ“ **ì¤‘ê¸‰ ìˆ˜ì¤€**: ì ì ˆí•œ ë³µì¡ì„±ì„ ê°€ì§„ ê¸€ì“°ê¸°ì…ë‹ˆë‹¤.")
                        else:
                            st.warning("ğŸ“š **ê¸°ì´ˆ ìˆ˜ì¤€**: ë” ë‹¤ì–‘í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
            else:
                st.warning("3ë‹¨ê³„ ë¶„ì„ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
            st.markdown("---")
            
            # 3ê°€ì§€ ë°©ë²• ë¹„êµ ë¶„ì„ ì„¹ì…˜ ì¶”ê°€
            st.markdown("## ğŸ“Š 3ê°€ì§€ ë°©ë²• ë¹„êµ ë¶„ì„")
            st.markdown("ë™ì¼í•œ í…ìŠ¤íŠ¸ë¥¼ 3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”!")
            
            if (pos_method1 and 'error' not in pos_method1 and 
                pos_method2 and 'error' not in pos_method2 and 
                pos_method3 and 'error' not in pos_method3):
                
                # ë¹„êµ í…Œì´ë¸”
                comparison_data = {
                    "ë¶„ì„ ë°©ë²•": ["1ë‹¨ê³„: ìˆ˜ë™ ê·œì¹™", "2ë‹¨ê³„: NLTK ë¨¸ì‹ ëŸ¬ë‹", "3ë‹¨ê³„: íŒ¨í„´ ë°œê²¬"],
                    "ì¥ì ": [
                        "ë¹ ë¥¸ ì²˜ë¦¬, ì˜ˆì¸¡ ê°€ëŠ¥",
                        "ë†’ì€ ì •í™•ë„, ë¬¸ë§¥ ê³ ë ¤", 
                        "ì–¸ì–´ì  íŠ¹ì„± ë°œê²¬, ìŠ¤íƒ€ì¼ ë¶„ì„"
                    ],
                    "ë‹¨ì ": [
                        "ì˜ˆì™¸ ì²˜ë¦¬ ì–´ë ¤ì›€",
                        "ê³„ì‚° ë¹„ìš© ë†’ìŒ",
                        "ë³µì¡í•œ í•´ì„ í•„ìš”"
                    ],
                    "ì ìš© ë¶„ì•¼": [
                        "ì‹¤ì‹œê°„ ì²˜ë¦¬, ê·œì¹™ì  í…ìŠ¤íŠ¸",
                        "ì •í™•í•œ í’ˆì‚¬ íƒœê¹… í•„ìš”í•œ ë¶„ì•¼",
                        "ë¬¸ì²´ ë¶„ì„, ê¸€ì“°ê¸° í‰ê°€"
                    ]
                }
                
                st.dataframe(comparison_data, use_container_width=True)
                
                # ì§„ì§œ 3ê°€ì§€ ë°©ë²• ì¢…í•© ë¹„êµ
                st.markdown("**ğŸ” 3ê°€ì§€ ì ‘ê·¼ë²•ì˜ íŠ¹ì„± ë¹„êµ:**")
                
                # 3ë‹¨ê³„ ëª¨ë‘ í¬í•¨í•œ ë¹„êµ ì°¨íŠ¸ - ê°ìì˜ ê°•ì  í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown("**ğŸ”´ 1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜**")
                    method1_pos = pos_method1.get('pos_counts', {})
                    if method1_pos:
                        total_words = sum(method1_pos.values())
                        st.metric("ë¶„ì„ ë‹¨ì–´ ìˆ˜", f"{total_words}ê°œ")
                        st.metric("ì²˜ë¦¬ ì†ë„", "âš¡ ë¹ ë¦„")
                        pos_categories = len(method1_pos.keys())
                        st.metric("í’ˆì‚¬ ë¶„ë¥˜ ì¢…ë¥˜", f"{pos_categories}ê°œ")
                    
                with col2:
                    st.markdown("**ğŸ”µ 2ë‹¨ê³„: NLTK ê¸°ë°˜**")
                    pos_tagged = pos_method2.get('pos_tagged_words', [])
                    if pos_tagged:
                        st.metric("ë¶„ì„ ë‹¨ì–´ ìˆ˜", f"{len(pos_tagged)}ê°œ")
                        st.metric("ì •í™•ë„", "ğŸ¯ ë†’ìŒ")
                        pos_types = len(set(pos for word, pos in pos_tagged))
                        st.metric("í’ˆì‚¬ íƒœê·¸ ì¢…ë¥˜", f"{pos_types}ê°œ")
                
                with col3:
                    st.markdown("**ğŸŸ¢ 3ë‹¨ê³„: íŒ¨í„´ ë°œê²¬**")
                    linguistic_features = pos_method3.get('linguistic_features', {})
                    overall_patterns = pos_method3.get('overall_patterns', {})
                    if linguistic_features and overall_patterns:
                        st.metric("ë³µì¡ë„ ì ìˆ˜", f"{linguistic_features.get('complexity_score', 0):.2f}")
                        st.metric("ë¶„ì„ ê¹Šì´", "ğŸ”¬ ì‹¬í™”")
                        st.metric("íŒ¨í„´ ê°œìˆ˜", f"{len(pos_method3.get('common_patterns', []))}ê°œ")
                
                # 3ê°€ì§€ ë°©ë²•ì˜ ìƒí˜¸ ë³´ì™„ì  íŠ¹ì„± ë³´ì—¬ì£¼ê¸°
                st.markdown("**ğŸ“Š ì¢…í•© ë¹„êµ ë¶„ì„:**")
                
                # ê° ë°©ë²•ì´ ë°œê²¬í•œ ê²ƒë“¤ì„ ë¹„êµ
                method1_total = sum(pos_method1.get('pos_counts', {}).values()) if pos_method1.get('pos_counts') else 0
                method2_total = len(pos_method2.get('pos_tagged_words', [])) if pos_method2.get('pos_tagged_words') else 0
                method3_patterns = len(pos_method3.get('common_patterns', [])) if pos_method3.get('common_patterns') else 0
                
                comparison_metrics = {
                    "ë¶„ì„ ì¸¡ë©´": ["ê¸°ë³¸ í’ˆì‚¬ ì¸ì‹", "ì •ë°€ í’ˆì‚¬ íƒœê¹…", "ì–¸ì–´ íŒ¨í„´ ë°œê²¬"],
                    "1ë‹¨ê³„ ê²°ê³¼": [f"{method1_total}ê°œ ë‹¨ì–´ ë¶„ë¥˜", "ê¸°ë³¸ ìˆ˜ì¤€", "ì—†ìŒ"],
                    "2ë‹¨ê³„ ê²°ê³¼": [f"{method2_total}ê°œ ë‹¨ì–´ ë¶„ë¥˜", "ìƒì„¸ íƒœê·¸", "ì—†ìŒ"], 
                    "3ë‹¨ê³„ ê²°ê³¼": ["í’ˆì‚¬ ë°€ë„ ë¶„ì„", "ë¬¸ì¥ë³„ íŠ¹ì„±", f"{method3_patterns}ê°œ íŒ¨í„´"]
                }
                
                st.dataframe(comparison_metrics, use_container_width=True)
                
                # ìƒí˜¸ ë³´ì™„ì  ê´€ê³„ ì„¤ëª…
                st.info("""
                **ğŸ”„ 3ê°€ì§€ ë°©ë²•ì˜ ìƒí˜¸ ë³´ì™„ ê´€ê³„:**
                - **1ë‹¨ê³„ â†’ 2ë‹¨ê³„**: ê·œì¹™ ê¸°ë°˜ì˜ ë¹ ë¥¸ ë¶„ë¥˜ â†’ ê¸°ê³„í•™ìŠµì˜ ì •í™•í•œ íƒœê¹…  
                - **2ë‹¨ê³„ â†’ 3ë‹¨ê³„**: ê°œë³„ ë‹¨ì–´ íƒœê¹… â†’ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ì  íŒ¨í„´ ë°œê²¬
                - **ì¢…í•© í™œìš©**: ë¹ ë¥¸ 1ì°¨ ë¶„ë¥˜ + ì •í™•í•œ íƒœê¹… + ê³ ì°¨ì› íŒ¨í„´ ë¶„ì„
                """)
                
                # ì²´í—˜í˜• ë¹„êµ ìš”ì†Œ
                st.markdown("**ğŸ¯ ì§ì ‘ ë¹„êµí•´ë³´ê¸°:**")
                sample_text = st.text_input("ë¹„êµí•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                          value="I am writing a beautiful essay about my dreams.",
                                          key="pos_comparison_input")
                
                # ë²„íŠ¼ê³¼ ê²°ê³¼ë¥¼ í•¨ê»˜ ì²˜ë¦¬
                if st.button("ğŸ” 3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë™ì‹œ ë¶„ì„í•˜ê¸°", key="pos_compare_demo") and sample_text:
                        with st.spinner("3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë™ì‹œ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                            try:
                                # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
                                words = sample_text.lower().split()
                                rule_based = []
                                for word in words:
                                    if word.endswith('ing'):
                                        rule_based.append((word, 'VBG'))
                                    elif word in ['i', 'you', 'he', 'she', 'it', 'we', 'they']:
                                        rule_based.append((word, 'PRP'))
                                    elif word in ['a', 'an', 'the']:
                                        rule_based.append((word, 'DT'))
                                    elif word in ['am', 'is', 'are', 'was', 'were', 'be', 'been', 'being']:
                                        rule_based.append((word, 'VB'))
                                    elif word.endswith('ly'):
                                        rule_based.append((word, 'RB'))
                                    elif word in ['beautiful', 'good', 'bad', 'great', 'amazing', 'wonderful']:
                                        rule_based.append((word, 'JJ'))
                                    else:
                                        rule_based.append((word, 'NN'))
                                
                                # NLTK ê¸°ë°˜ ë¶„ì„
                                import nltk
                                tokens = nltk.word_tokenize(sample_text)
                                nltk_based = nltk.pos_tag(tokens)
                                
                                # ê²°ê³¼ ë¹„êµ í‘œì‹œ
                                st.markdown("### ğŸ” 3ê°€ì§€ ë°©ë²• ë¹„êµ ê²°ê³¼")
                                
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.markdown("**ğŸ”´ 1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜**")
                                    st.markdown("---")
                                    for word, pos in rule_based:
                                        if pos == 'NN':
                                            st.markdown(f"ğŸŸ¦ {word} â†’ **{pos}** (ëª…ì‚¬)")
                                        elif pos.startswith('VB'):
                                            st.markdown(f"ğŸŸ© {word} â†’ **{pos}** (ë™ì‚¬)")
                                        elif pos == 'JJ':
                                            st.markdown(f"ğŸŸ¨ {word} â†’ **{pos}** (í˜•ìš©ì‚¬)")
                                        elif pos == 'RB':
                                            st.markdown(f"ğŸŸª {word} â†’ **{pos}** (ë¶€ì‚¬)")
                                        else:
                                            st.markdown(f"âšª {word} â†’ **{pos}** (ê¸°íƒ€)")
                                
                                with col2:
                                    st.markdown("**ğŸ”µ 2ë‹¨ê³„: NLTK ê¸°ë°˜**")
                                    st.markdown("---")
                                    for word, pos in nltk_based:
                                        if pos.startswith('NN'):
                                            st.markdown(f"ğŸŸ¦ {word} â†’ **{pos}** (ëª…ì‚¬)")
                                        elif pos.startswith('VB'):
                                            st.markdown(f"ğŸŸ© {word} â†’ **{pos}** (ë™ì‚¬)")
                                        elif pos.startswith('JJ'):
                                            st.markdown(f"ğŸŸ¨ {word} â†’ **{pos}** (í˜•ìš©ì‚¬)")
                                        elif pos.startswith('RB'):
                                            st.markdown(f"ğŸŸª {word} â†’ **{pos}** (ë¶€ì‚¬)")
                                        elif pos in ['DT', 'IN', 'TO', 'PRP']:
                                            st.markdown(f"ğŸŸ« {word} â†’ **{pos}** (ê¸°ëŠ¥ì–´)")
                                        else:
                                            st.markdown(f"âšª {word} â†’ **{pos}** (ê¸°íƒ€)")
                                
                                with col3:
                                    st.markdown("**ğŸŸ¢ ì°¨ì´ì  ë¶„ì„**")
                                    st.markdown("---")
                                    differences = []
                                    same_count = 0
                                    
                                    # ë‹¨ì–´ë³„ ë¹„êµ (ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬)
                                    rule_dict = {w.lower(): p for w, p in rule_based}
                                    nltk_dict = {w.lower(): p for w, p in nltk_based}
                                    
                                    all_words = set(rule_dict.keys()) | set(nltk_dict.keys())
                                    
                                    for word in all_words:
                                        rule_pos = rule_dict.get(word, 'N/A')
                                        nltk_pos = nltk_dict.get(word, 'N/A')
                                        
                                        if rule_pos != nltk_pos:
                                            differences.append(f"â€¢ **{word}**: {rule_pos} â†’ {nltk_pos}")
                                        else:
                                            same_count += 1
                                    
                                    if differences:
                                        st.markdown("**ì£¼ìš” ì°¨ì´ì :**")
                                        for diff in differences[:8]:  # ìµœëŒ€ 8ê°œë§Œ í‘œì‹œ
                                            st.markdown(diff)
                                        if len(differences) > 8:
                                            st.markdown(f"... ì™¸ {len(differences)-8}ê°œ ë”")
                                    
                                    st.markdown(f"**ğŸ“Š ë¹„êµ í†µê³„:**")
                                    st.markdown(f"â€¢ ë™ì¼: {same_count}ê°œ")
                                    st.markdown(f"â€¢ ë‹¤ë¦„: {len(differences)}ê°œ")
                                    
                                    accuracy = same_count / len(all_words) * 100 if all_words else 0
                                    st.markdown(f"â€¢ ì¼ì¹˜ìœ¨: {accuracy:.1f}%")
                                
                                st.success("âœ… 3ê°€ì§€ ë°©ë²• ë¹„êµ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                
                                # í•™ìŠµ í¬ì¸íŠ¸
                                st.info("""
                                **ğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:**
                                - **ê·œì¹™ ê¸°ë°˜**: ë¹ ë¥´ì§€ë§Œ ë‹¨ìˆœí•œ ë¶„ë¥˜
                                - **NLTK**: ë” ì •í™•í•˜ê³  ì„¸ë°€í•œ í’ˆì‚¬ íƒœê¹…
                                - **ì°¨ì´ì **: ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì •í™•ë„ ì°¨ì´ í™•ì¸ ê°€ëŠ¥
                                """)
                            
                            except Exception as e:
                                st.error(f"ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                                import traceback
                                st.code(traceback.format_exc())
            
            else:
                st.warning("3ê°€ì§€ ë°©ë²• ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            
            # ì¢…í•© ì •ë¦¬
            st.markdown("---")
            st.subheader("ğŸ“ í’ˆì‚¬ ë¶„ì„ í•™ìŠµ ì •ë¦¬")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.info("""
                **ğŸ” ë°°ìš´ ë‚´ìš©:**
                - ìˆ˜ë™ ê·œì¹™ ê¸°ë°˜ì˜ íŒ¨í„´ ë§¤ì¹­
                - NLTK ê¸°ê³„í•™ìŠµ ëª¨ë¸ì˜ í™œìš©
                - í’ˆì‚¬ íŒ¨í„´ì„ í†µí•œ ì–¸ì–´ì  íŠ¹ì„± ë°œê²¬
                - ê° ë°©ë²•ì˜ ì¥ë‹¨ì ê³¼ ì ìš© ë¶„ì•¼
                """)
            
            with col2:
                st.success("""
                **ğŸ’¡ ì‹¤ë¬´ ì ìš©:**
                - ë¹ ë¥¸ ë¶„ì„: ê·œì¹™ ê¸°ë°˜
                - ì •í™•í•œ íƒœê¹…: NLTK ê¸°ê³„í•™ìŠµ
                - ë¬¸ì²´ ë¶„ì„: íŒ¨í„´ ë°œê²¬
                - ìì—°ì–´ì²˜ë¦¬: ì¢…í•© ì ‘ê·¼ë²• í™œìš©
                """)
            
            st.success("âœ… í…ìŠ¤íŠ¸ ë§ˆì´ë‹ í’ˆì‚¬ ë¶„ì„ ì›ë¦¬ ì²´í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
        else:
            st.error("í’ˆì‚¬ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 3. ê¸€ì“°ê¸° ìˆ˜ì¤€ ì¢…í•© ì§„ë‹¨ íƒ­
    with analysis_tabs[2]:
        st.subheader("ğŸ† ê¸€ì“°ê¸° ìˆ˜ì¤€ ì¢…í•© ì§„ë‹¨ ì›ë¦¬ ì²´í—˜")
        st.markdown("""
        **ğŸ¯ í•™ìŠµ ëª©í‘œ:**
        AIê°€ ì–´ë–»ê²Œ ê¸€ì“°ê¸°ë¥¼ í‰ê°€í•˜ëŠ”ì§€ 4ë‹¨ê³„ë¡œ ì²´í—˜í•´ë³´ì„¸ìš”!
        - **1ë‹¨ê³„**: í†µê³„ì  í…ìŠ¤íŠ¸ ë¶„ì„ - ë¬¸ì¥ êµ¬ì¡°ì™€ ì–´íœ˜ ë‹¤ì–‘ì„± ì¸¡ì •
        - **2ë‹¨ê³„**: ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„ - ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš© í‰ê°€
        - **3ë‹¨ê³„**: ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ - ì •í™•ì„± ë° ë¬¸ë²• ê·œì¹™ ê²€ì‚¬
        - **4ë‹¨ê³„**: ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ - ë…¼ë¦¬ì  ì—°ê²°ì„±ê³¼ ì£¼ì œ ì¼ê´€ì„± í‰ê°€
        """)
        
        st.info("ğŸ’¡ **ì²´í—˜ í¬ì¸íŠ¸**: ì‹¤ì œ AI ê¸€ì“°ê¸° í‰ê°€ ì‹œìŠ¤í…œì˜ ì‘ë™ ì›ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•´ë³´ì„¸ìš”!")
        
        # ìë™ìœ¼ë¡œ ì¢…í•© ë¶„ì„ ì‹¤í–‰
        if all_essays_text.strip():
            try:
                # ì¢…í•© ë¶„ì„ ì‹¤í–‰
                with st.spinner("ğŸ“Š í†µí•© ì—ì„¸ì´ í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘..."):
                    result = preprocessor.comprehensive_writing_analysis(all_essays_text)
                
                if result and 'error' not in result:
                    st.success("âœ… í†µí•© ì—ì„¸ì´ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
                    
                    # 1ë‹¨ê³„: í†µê³„ì  í…ìŠ¤íŠ¸ ë¶„ì„
                    st.markdown("## ğŸ“Š 1ë‹¨ê³„: í†µê³„ì  í…ìŠ¤íŠ¸ ë¶„ì„")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **ë¬¸ì¥ ê¸¸ì´ ë¶„í¬**: í‰ê·  ë¬¸ì¥ ê¸¸ì´ë¡œ ê¸€ì“°ê¸° ë³µì¡ë„ ì¸¡ì •
                    - **ì–´íœ˜ ë‹¤ì–‘ì„±**: TTR(Type-Token Ratio) ê³„ì‚°ìœ¼ë¡œ ì–´íœ˜ í’ë¶€í•¨ í‰ê°€
                    - **í’ˆì‚¬ íŒ¨í„´**: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë¹„ìœ¨ë¡œ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ë¶„ì„
                    """)
                    
                    # í•™ìƒ í†µí•© ì—ì„¸ì´ í†µê³„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                    st.markdown("### ğŸ“Š í•™ìƒ í†µí•© ì—ì„¸ì´ í†µê³„ ë¶„ì„ ê²°ê³¼")
                    statistical_analysis = result.get('step1_stats', {})
                    if statistical_analysis:
                        user_stats = statistical_analysis.get('user_statistics', {})
                        if user_stats:
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                avg_sentence_length = user_stats.get('avg_sentence_length', 0)
                                st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{avg_sentence_length:.1f} ë‹¨ì–´")
                                if avg_sentence_length > 15:
                                    st.caption("ğŸŸ¢ ë³µí•©ë¬¸ í™œìš©")
                                elif avg_sentence_length > 10:
                                    st.caption("ğŸŸ¡ ì ì • ìˆ˜ì¤€")
                                else:
                                    st.caption("ğŸŸ  ë‹¨ìˆœë¬¸ ìœ„ì£¼")
                            
                            with col2:
                                lexical_diversity = user_stats.get('vocabulary_diversity', 0)
                                st.metric("ì–´íœ˜ ë‹¤ì–‘ì„±", f"{lexical_diversity:.3f}")
                                if lexical_diversity > 0.7:
                                    st.caption("ğŸŸ¢ ë§¤ìš° ë‹¤ì–‘í•¨")
                                elif lexical_diversity > 0.5:
                                    st.caption("ğŸŸ¡ ì ì •í•¨")
                                else:
                                    st.caption("ğŸŸ  ë‹¨ì¡°ë¡œì›€")
                            
                            with col3:
                                total_sentences = user_stats.get('total_sentences', 0)
                                st.metric("ì´ ë¬¸ì¥ ìˆ˜", f"{total_sentences}ê°œ")
                            
                            with col4:
                                total_words = user_stats.get('total_words', 0)
                                st.metric("ì´ ë‹¨ì–´ ìˆ˜", f"{total_words}ê°œ")
                            
                            # ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ê²°ê³¼ í‘œì‹œ
                            st.markdown("### ğŸ† ì „ë¬¸ê°€ ê¸°ì¤€ê³¼ì˜ ë¹„êµ")
                            best_match = statistical_analysis.get('best_match', None)
                            if best_match:
                                match_name = best_match[1]['benchmark_name']
                                similarity_score = best_match[1]['similarity_score']
                                
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("ê°€ì¥ ìœ ì‚¬í•œ ê¸€ ìœ í˜•", match_name)
                                with col2:
                                    st.metric("ìœ ì‚¬ë„ ì ìˆ˜", f"{similarity_score:.1f}%")
                                    if similarity_score > 80:
                                        st.caption("ğŸŸ¢ ë§¤ìš° ìš°ìˆ˜")
                                    elif similarity_score > 60:
                                        st.caption("ğŸŸ¡ ì–‘í˜¸")
                                    else:
                                        st.caption("ğŸŸ  ê°œì„  í•„ìš”")
                                
                                # ì„¸ë¶€ ë¹„êµ ì ìˆ˜
                                individual_scores = best_match[1]['individual_scores']
                                st.markdown("**ğŸ“‹ ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜:**")
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.write(f"â€¢ ëª…ì‚¬ ë¹„ìœ¨: {individual_scores.get('noun_score', 0):.1f}%")
                                    st.write(f"â€¢ ë™ì‚¬ ë¹„ìœ¨: {individual_scores.get('verb_score', 0):.1f}%")
                                
                                with col2:
                                    st.write(f"â€¢ í˜•ìš©ì‚¬ ë¹„ìœ¨: {individual_scores.get('adj_score', 0):.1f}%")
                                    st.write(f"â€¢ ë³µì¡ë„: {individual_scores.get('complexity_score', 0):.1f}%")
                                
                                with col3:
                                    st.write(f"â€¢ ì–´íœ˜ ë‹¤ì–‘ì„±: {individual_scores.get('diversity_score', 0):.1f}%")
                                    st.write(f"â€¢ ë¬¸ì¥ ê¸¸ì´: {individual_scores.get('length_score', 0):.1f}%")
                            
                            # í†µì°° ë° ê°œì„ ì 
                            insights = statistical_analysis.get('insights', [])
                            if insights:
                                st.markdown("### ğŸ’¡ ë¶„ì„ í†µì°°")
                                for insight in insights[:3]:
                                    st.info(f"â€¢ {insight}")
                    else:
                        st.warning("í†µí•© ì—ì„¸ì´ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ì²´í—˜í˜• ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ - 1ë‹¨ê³„
                    st.markdown("**ğŸ¯ ì²´í—˜í•´ë³´ê¸°: ë‚´ ê¸€ì˜ í†µê³„ì  íŠ¹ì„± ë¶„ì„**")
                    user_text_stats = st.text_area("ë¶„ì„í•  ì˜ì–´ ì—ì„¸ì´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", 
                                                   value="Education is very important for everyone. Students should study hard to achieve their goals. Learning helps us grow and become better people. We need to practice reading and writing every day.",
                                                   height=100,
                                                   key="user_stats_input")
                    
                    if st.button("ğŸ” ë‚´ ê¸€ í†µê³„ ë¶„ì„í•˜ê¸°", key="stats_analysis") and user_text_stats.strip():
                        with st.spinner("í†µê³„ì  íŠ¹ì„± ë¶„ì„ ì¤‘..."):
                            import nltk
                            sentences = nltk.sent_tokenize(user_text_stats)
                            words = nltk.word_tokenize(user_text_stats)
                            word_count = len([w for w in words if w.isalnum()])
                            unique_words = len(set(w.lower() for w in words if w.isalnum()))
                            
                            user_avg_length = word_count / len(sentences) if sentences else 0
                            user_diversity = unique_words / word_count if word_count > 0 else 0
                            
                            st.write(f"**ì…ë ¥í•œ ê¸€:** {user_text_stats}")
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{user_avg_length:.1f} ë‹¨ì–´")
                            with col2:
                                st.metric("ì–´íœ˜ ë‹¤ì–‘ì„±", f"{user_diversity:.3f}")
                            with col3:
                                st.metric("ë¬¸ì¥ ìˆ˜", f"{len(sentences)}ê°œ")
                            
                            # í‰ê°€ ì½”ë©˜íŠ¸
                            if user_avg_length > 12 and user_diversity > 0.6:
                                st.success("ğŸ‰ í†µê³„ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ê¸€ì“°ê¸° íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤!")
                            elif user_avg_length > 8 and user_diversity > 0.4:
                                st.info("ğŸ“ ì ì ˆí•œ ìˆ˜ì¤€ì˜ ê¸€ì“°ê¸°ì…ë‹ˆë‹¤. ì–´íœ˜ë¥¼ ë” ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
                            else:
                                st.warning("ğŸ’¡ ë¬¸ì¥ì„ ì¢€ ë” ê¸¸ê³  ë³µí•©ì ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
                    
                    st.markdown("---")
                    
                    # 2ë‹¨ê³„: ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„
                    st.markdown("## ğŸ“š 2ë‹¨ê³„: ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„ - ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš© í‰ê°€")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **ê³ ê¸‰ ì–´íœ˜ ì‚¬ì „**: í•™ìˆ ì , ê³ ê¸‰ ì–´íœ˜ì˜ ì‚¬ìš© ë¹ˆë„ ì¸¡ì •
                    - **ìˆ˜ì¤€ë³„ ë¶„ë¥˜**: ê¸°ì´ˆ/ì¤‘ê¸‰/ê³ ê¸‰/í•™ìˆ  ì–´íœ˜ë¡œ ë¶„ë¥˜í•˜ì—¬ ê¸€ì“°ê¸° ìˆ˜ì¤€ í‰ê°€
                    - **í†µì°° ë°œê²¬**: ì–´íœ˜ ë‹¤ì–‘ì„±ê³¼ ì •êµì„±ì„ í†µí•œ ê¸€ì“°ê¸° ì„±ìˆ™ë„ ì¸¡ì •
                    """)
                    
                    # í•™ìƒ í†µí•© ì—ì„¸ì´ ì›Œë“œ ì„ë² ë”© ë¶„ì„ ê²°ê³¼
                    st.markdown("### ğŸ§  í•™ìƒ í†µí•© ì—ì„¸ì´ ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„ ê²°ê³¼")
                    vocabulary_analysis = result.get('step2_vocabulary', {})
                    if vocabulary_analysis:
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            vocab_level = vocabulary_analysis.get('overall_level', 'Unknown')
                            st.metric("ì „ì²´ ì–´íœ˜ ìˆ˜ì¤€", vocab_level)
                        
                        with col2:
                            advanced_ratio = vocabulary_analysis.get('advanced_vocabulary_ratio', 0)
                            st.metric("ê³ ê¸‰ ì–´íœ˜ ë¹„ìœ¨", f"{advanced_ratio:.1f}%")
                            if advanced_ratio > 20:
                                st.caption("ğŸŸ¢ ë†’ì€ ìˆ˜ì¤€")
                            elif advanced_ratio > 10:
                                st.caption("ğŸŸ¡ ë³´í†µ ìˆ˜ì¤€")
                            else:
                                st.caption("ğŸŸ  ê¸°ì´ˆ ìˆ˜ì¤€")
                        
                        with col3:
                            academic_score = vocabulary_analysis.get('academic_vocabulary_score', 0)
                            st.metric("í•™ìˆ  ì–´íœ˜ ì ìˆ˜", f"{academic_score:.1f}ì ")
                        
                        # ì–´íœ˜ ìˆ˜ì¤€ë³„ ë¶„ì„
                        level_analysis = vocabulary_analysis.get('level_analysis', {})
                        if level_analysis:
                            st.markdown("### ğŸ“š ìˆ˜ì¤€ë³„ ì–´íœ˜ ë¶„í¬")
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                basic_count = level_analysis.get('basic_words', 0)
                                st.metric("ê¸°ì´ˆ ì–´íœ˜", f"{basic_count}ê°œ")
                            
                            with col2:
                                intermediate_count = level_analysis.get('intermediate_words', 0) 
                                st.metric("ì¤‘ê¸‰ ì–´íœ˜", f"{intermediate_count}ê°œ")
                            
                            with col3:
                                advanced_count = level_analysis.get('advanced_words', 0)
                                st.metric("ê³ ê¸‰ ì–´íœ˜", f"{advanced_count}ê°œ")
                            
                            with col4:
                                academic_count = level_analysis.get('academic_words', 0)
                                st.metric("í•™ìˆ  ì–´íœ˜", f"{academic_count}ê°œ")
                        
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ë° ë³µì¡ë„
                        complexity_analysis = vocabulary_analysis.get('complexity_analysis', {})
                        if complexity_analysis:
                            st.markdown("### ğŸ”¬ ì–´íœ˜ ë³µì¡ë„ ë¶„ì„")
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                avg_word_length = complexity_analysis.get('avg_word_length', 0)
                                st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", f"{avg_word_length:.1f} ê¸€ì")
                                
                                unique_ratio = complexity_analysis.get('unique_word_ratio', 0)
                                st.metric("ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨", f"{unique_ratio:.1f}%")
                            
                            with col2:
                                sophistication_score = complexity_analysis.get('sophistication_score', 0)
                                st.metric("ì–´íœ˜ ì •êµì„±", f"{sophistication_score:.1f}ì ")
                                
                                st.write(f"**í‰ê°€:** {complexity_analysis.get('level_description', 'ë³´í†µ ìˆ˜ì¤€')}")
                        
                        # ê°œì„  ì¶”ì²œ ì‚¬í•­
                        recommendations = vocabulary_analysis.get('vocabulary_recommendations', [])
                        if recommendations:
                            st.markdown("### ğŸ’¡ ì–´íœ˜ ê°œì„  ë°©í–¥")
                            for rec in recommendations[:3]:
                                st.info(f"â€¢ {rec}")
                    else:
                        st.warning("í†µí•© ì—ì„¸ì´ ì–´íœ˜ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ì²´í—˜í˜• ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ - 2ë‹¨ê³„
                    st.markdown("**ğŸ¯ ì²´í—˜í•´ë³´ê¸°: ë‚´ ê¸€ì˜ ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„**")
                    
                    user_vocab_text = st.text_area("ì–´íœ˜ ìˆ˜ì¤€ì„ ë¶„ì„í•  ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", 
                                                   value="I think education is important because it helps students develop critical thinking skills and prepare for future challenges in their professional careers.",
                                                   height=100,
                                                   key="user_vocab_input")
                    
                    if st.button("ğŸ” ë‚´ ê¸€ ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„í•˜ê¸°", key="vocab_analysis") and user_vocab_text.strip():
                        with st.spinner("ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„ ì¤‘..."):
                            # ë‹¨ì–´ ì „ì²˜ë¦¬ (êµ¬ë‘ì  ì œê±°)
                            import re
                            words = re.findall(r'\b[a-zA-Z]+\b', user_vocab_text.lower())
                            total_words = len(words)
                            
                            # ìë™ ì–´íœ˜ ìˆ˜ì¤€ ë¶„ë¥˜
                            basic_words = []
                            intermediate_words = []
                            advanced_words = []
                            
                            # ê³ ê¸‰ ì–´ë¯¸ íŒ¨í„´ë“¤
                            advanced_suffixes = ['tion', 'sion', 'ment', 'ness', 'ity', 'ous', 'ive', 'able', 'ible', 'ful', 'less', 'ence', 'ance', 'ize', 'ise', 'ate']
                            
                            for word in words:
                                word_len = len(word)
                                has_advanced_suffix = any(word.endswith(suffix) for suffix in advanced_suffixes)
                                
                                # ë¶„ë¥˜ ë¡œì§
                                if word_len <= 4 and not has_advanced_suffix:
                                    basic_words.append(word)
                                elif word_len >= 8 or has_advanced_suffix:
                                    advanced_words.append(word)
                                else:
                                    intermediate_words.append(word)
                            
                            # ê³ ê¸‰ ì–´íœ˜ ë¹„ìœ¨ ê³„ì‚°
                            advanced_count = len(advanced_words)
                            advanced_ratio = (advanced_count / total_words * 100) if total_words > 0 else 0
                            
                            # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ (ì²´í—˜ìš© ê°„ëµí™”)
                            category_results = {
                                'length_based': [w for w in advanced_words if len(w) >= 8],
                                'suffix_based': [w for w in advanced_words if any(w.endswith(s) for s in advanced_suffixes)]
                            }
                            
                            st.write(f"**ë¶„ì„í•œ í…ìŠ¤íŠ¸:** {user_vocab_text}")
                            
                            # ë¶„ë¥˜ ê²°ê³¼ í‘œì‹œ
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("ì´ ë‹¨ì–´ ìˆ˜", f"{total_words}ê°œ")
                            with col2:
                                st.metric("ê¸°ì´ˆ ì–´íœ˜", f"{len(basic_words)}ê°œ")
                            with col3:
                                st.metric("ì¤‘ê¸‰ ì–´íœ˜", f"{len(intermediate_words)}ê°œ")
                            with col4:
                                st.metric("ê³ ê¸‰ ì–´íœ˜", f"{len(advanced_words)}ê°œ")
                            
                            # ê³ ê¸‰ ì–´íœ˜ ë¹„ìœ¨ ë° í‰ê°€
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("ê³ ê¸‰ ì–´íœ˜ ë¹„ìœ¨", f"{advanced_ratio:.1f}%")
                            with col2:
                                if advanced_ratio > 20:
                                    st.metric("ì–´íœ˜ ìˆ˜ì¤€", "ğŸŸ¢ ê³ ê¸‰")
                                elif advanced_ratio > 10:
                                    st.metric("ì–´íœ˜ ìˆ˜ì¤€", "ğŸŸ¡ ì¤‘ê¸‰")
                                else:
                                    st.metric("ì–´íœ˜ ìˆ˜ì¤€", "ğŸŸ  ê¸°ì´ˆ")
                            
                            # ë°œê²¬ëœ ê³ ê¸‰ ì–´íœ˜ ì˜ˆì‹œ (ìµœëŒ€ 10ê°œ)
                            if advanced_words:
                                st.markdown("**ğŸ“š ë°œê²¬ëœ ê³ ê¸‰ ì–´íœ˜ (ì˜ˆì‹œ):**")
                                st.write("â€¢ " + ", ".join(advanced_words[:10]))
                                if len(advanced_words) > 10:
                                    st.caption(f"ì™¸ {len(advanced_words)-10}ê°œ ë”...")
                            else:
                                st.info("ğŸ’¡ ë” ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ì–´íœ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”: development, significant, challenging, professional ë“±")
                            
                            # ë¶„ë¥˜ ê¸°ì¤€ ì„¤ëª…
                            st.markdown("**ğŸ” ì´ ì²´í—˜ì—ì„œ ì‚¬ìš©í•œ ìë™ ë¶„ë¥˜ ê¸°ì¤€:**")
                            st.write("â€¢ ê¸°ì´ˆ: 4ê¸€ì ì´í•˜ ë‹¨ì–´ (ì˜ˆ: good, use, make)")
                            st.write("â€¢ ê³ ê¸‰: 8ê¸€ì ì´ìƒ ë˜ëŠ” ê³ ê¸‰ ì–´ë¯¸(-tion, -ment, -ness, -ous, -ive ë“±)")
                            st.write("â€¢ ì¤‘ê¸‰: ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤")
                            
                            st.info("""
                            ğŸ’¡ **ì‹¤ì œ AI ì‹œìŠ¤í…œì—ì„œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš©:**
                            - **CEFR ë ˆë²¨ ë°ì´í„°** (A1~C2 ìˆ˜ì¤€ë³„ ì–´íœ˜ ë¶„ë¥˜)
                            - **Academic Word List** (í•™ìˆ  ë…¼ë¬¸ í•„ìˆ˜ ì–´íœ˜)
                            - **General Service List** (ì¼ìƒ ìƒí™œ ê¸°ì´ˆ ì–´íœ˜)
                            - **Word2Vec ì„ë² ë”©** (ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ë¶„ë¥˜)
                            """)
                    
                    st.markdown("---")
                    
                    # 3ë‹¨ê³„: ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
                    st.markdown("## ğŸ“ 3ë‹¨ê³„: ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ - ì •í™•ì„± í‰ê°€")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜**: I am, He is, They are ë“± ê¸°ë³¸ ë¬¸ë²• ê·œì¹™ ê²€ì‚¬
                    - **ì‹œì œ ì¼ê´€ì„±**: í•œ ë¬¸ì¥ ë‚´ì—ì„œ ê³¼ê±°/í˜„ì¬ ì‹œì œê°€ ì„ì´ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                    - **ê´€ì‚¬ ì‚¬ìš©**: a, an, the ë“± ê´€ì‚¬ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
                    - **ì „ì¹˜ì‚¬ í™œìš©**: in, on, at ë“± ì „ì¹˜ì‚¬ ì‚¬ìš©ì˜ ì ì ˆì„± ê²€í† 
                    """)
                    
                    # í•™ìƒ í†µí•© ì—ì„¸ì´ ë¬¸ë²• ë¶„ì„ ê²°ê³¼
                    st.markdown("### ğŸ“ í•™ìƒ í†µí•© ì—ì„¸ì´ ë¬¸ë²• ë¶„ì„ ê²°ê³¼")
                    
                    if 'step3_grammar' not in result:
                        with st.spinner("ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ì¤‘..."):
                            result['step3_grammar'] = preprocessor.analyze_grammar_patterns(essay_data['combined_text'])
                    
                    grammar_analysis = result.get('step3_grammar', {})
                    if grammar_analysis and 'error' not in grammar_analysis:
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            grammar_score = grammar_analysis.get('grammar_score', 100)
                            st.metric("ë¬¸ë²• ì •í™•ë„", f"{grammar_score:.1f}ì ")
                        
                        with col2:
                            total_sentences = grammar_analysis.get('total_sentences', 0)
                            sentences_with_issues = len(grammar_analysis.get('sentences_with_issues', []))
                            issue_rate = (sentences_with_issues / total_sentences * 100) if total_sentences > 0 else 0
                            st.metric("ë¬¸ì œ ë¬¸ì¥ ë¹„ìœ¨", f"{issue_rate:.1f}%")
                        
                        with col3:
                            error_types = len(grammar_analysis.get('error_count_by_type', {}))
                            st.metric("ë°œê²¬ëœ ì˜¤ë¥˜ ìœ í˜•", f"{error_types}ê°œ")
                        
                        # ì£¼ìš” ê°œì„  ì˜ì—­
                        improvement_areas = grammar_analysis.get('improvement_areas', [])
                        if improvement_areas:
                            st.markdown("### ğŸ¯ ì£¼ìš” ê°œì„  ì˜ì—­")
                            for i, area in enumerate(improvement_areas[:3], 1):
                                st.write(f"{i}. {area}")
                        
                        # ì˜¤ë¥˜ íŒ¨í„´ë³„ ìƒì„¸ ë¶„ì„
                        error_patterns = grammar_analysis.get('error_patterns', {})
                        if error_patterns:
                            st.markdown("### ğŸ“Š ë°œê²¬ëœ ë¬¸ë²• íŒ¨í„´")
                            
                            for error_type, examples in error_patterns.items():
                                error_type_korean = {
                                    'subject_verb_agreement': 'ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜',
                                    'tense_consistency': 'ì‹œì œ ì¼ê´€ì„±',
                                    'article_usage': 'ê´€ì‚¬ ì‚¬ìš©',
                                    'preposition_usage': 'ì „ì¹˜ì‚¬ ì‚¬ìš©',
                                    'sentence_structure': 'ë¬¸ì¥ êµ¬ì¡°',
                                    'sentence_length': 'ë¬¸ì¥ ê¸¸ì´'
                                }.get(error_type, error_type)
                                
                                with st.expander(f"ğŸ“Œ {error_type_korean} ({len(examples)}ê°œ ë°œê²¬)", expanded=False):
                                    for i, example in enumerate(examples[:3], 1):  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œë§Œ
                                        st.write(f"**ì˜ˆì‹œ {i}:**")
                                        st.write(f"â€¢ ë¬¸ì¥: \"{example['sentence']}\"")
                                        st.write(f"â€¢ ë¬¸ì œì : {example['description']}")
                                        if example['suggestion']:
                                            st.write(f"â€¢ ê°œì„  ì œì•ˆ: {example['suggestion']}")
                                        st.write("")
                    else:
                        error_msg = grammar_analysis.get('error', 'ë¬¸ë²• ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.')
                        st.warning(f"ë¬¸ë²• ë¶„ì„ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}")
                    
                    # ì²´í—˜í˜• ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ - ë¬¸ë²• ê²€ì‚¬
                    st.markdown("**ğŸ¯ ì²´í—˜í•´ë³´ê¸°: í…ìŠ¤íŠ¸ ë§ˆì´ë‹ìœ¼ë¡œ ë¬¸ë²• íŒ¨í„´ íƒì§€í•˜ê¸°**")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown("**ğŸ“ ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:**")
                        sample_texts = [
                            "I love reading books. Books are amazing. I read books every day.",
                            "She go to school. He are happy. They was playing games.",
                            "The students study hard. A teacher explains well. An apple is red.",
                            "Yesterday I walk to school. Today I will walked home."
                        ]
                        selected_text = st.selectbox("í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:", sample_texts, key="pattern_text")
                    
                    with col2:
                        st.markdown("**ğŸ” ì°¾ì„ íŒ¨í„´ì„ ì„ íƒí•˜ì„¸ìš”:**")
                        pattern_options = [
                            "ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜ ì˜¤ë¥˜",
                            "ì‹œì œ ì¼ê´€ì„± ë¬¸ì œ", 
                            "ê´€ì‚¬ ì‚¬ìš© íŒ¨í„´",
                            "ë°˜ë³µ ë‹¨ì–´ ë¹ˆë„"
                        ]
                        selected_pattern = st.selectbox("íŒ¨í„´ì„ ì„ íƒí•˜ì„¸ìš”:", pattern_options, key="pattern_type")
                    
                    if st.button("ğŸ”¬ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰", key="analyze_pattern"):
                        st.markdown("### ğŸ“Š í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë¶„ì„ ê²°ê³¼")
                        
                        import re
                        from collections import Counter
                        
                        if selected_pattern == "ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜ ì˜¤ë¥˜":
                            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì˜¤ë¥˜ íŒ¨í„´ íƒì§€
                            error_patterns = [
                                (r'\bI are\b', 'I are â†’ I am'),
                                (r'\bHe are\b|\bShe are\b', 'He/She are â†’ He/She is'),
                                (r'\bThey is\b', 'They is â†’ They are'),
                                (r'\bWe was\b', 'We was â†’ We were')
                            ]
                            
                            found_errors = []
                            for pattern, correction in error_patterns:
                                matches = re.findall(pattern, selected_text, re.IGNORECASE)
                                if matches:
                                    found_errors.extend([(match, correction) for match in matches])
                            
                            if found_errors:
                                st.error("âŒ ë°œê²¬ëœ ìˆ˜ì¼ì¹˜ ì˜¤ë¥˜:")
                                for i, (error, correction) in enumerate(found_errors, 1):
                                    st.write(f"{i}. **{error}** â†’ {correction}")
                            else:
                                st.success("âœ… ì£¼ì–´-ë™ì‚¬ ìˆ˜ì¼ì¹˜ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                                
                        elif selected_pattern == "ì‹œì œ ì¼ê´€ì„± ë¬¸ì œ":
                            # ì‹œì œ ê´€ë ¨ ë‹¨ì–´ íŒ¨í„´ ë¶„ì„
                            past_indicators = re.findall(r'\b(yesterday|ago|last|was|were|went|did)\b', selected_text.lower())
                            present_indicators = re.findall(r'\b(today|now|is|are|do|does|go|walk)\b', selected_text.lower())
                            future_indicators = re.findall(r'\b(tomorrow|will|going to)\b', selected_text.lower())
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ê³¼ê±°í˜• í‘œí˜„", len(past_indicators))
                                if past_indicators:
                                    st.write("ë°œê²¬ëœ ë‹¨ì–´:", ', '.join(past_indicators[:3]))
                            with col2:
                                st.metric("í˜„ì¬í˜• í‘œí˜„", len(present_indicators))
                                if present_indicators:
                                    st.write("ë°œê²¬ëœ ë‹¨ì–´:", ', '.join(present_indicators[:3]))
                            with col3:
                                st.metric("ë¯¸ë˜í˜• í‘œí˜„", len(future_indicators))
                                if future_indicators:
                                    st.write("ë°œê²¬ëœ ë‹¨ì–´:", ', '.join(future_indicators[:3]))
                                    
                            # ì‹œì œ ì¼ê´€ì„± ë¶„ì„
                            if len([x for x in [past_indicators, present_indicators, future_indicators] if x]) > 1:
                                st.warning("âš ï¸ ì—¬ëŸ¬ ì‹œì œê°€ í˜¼ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¬¸ë§¥ìƒ ì¼ê´€ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
                            else:
                                st.success("âœ… ì‹œì œ ì‚¬ìš©ì´ ì¼ê´€ì ì…ë‹ˆë‹¤!")
                                
                        elif selected_pattern == "ê´€ì‚¬ ì‚¬ìš© íŒ¨í„´":
                            # ê´€ì‚¬ íŒ¨í„´ ë¶„ì„
                            articles = re.findall(r'\b(a|an|the)\b', selected_text.lower())
                            article_count = Counter(articles)
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("'a' ì‚¬ìš©", article_count.get('a', 0))
                            with col2:
                                st.metric("'an' ì‚¬ìš©", article_count.get('an', 0))
                            with col3:
                                st.metric("'the' ì‚¬ìš©", article_count.get('the', 0))
                                
                            # ê´€ì‚¬ ë‹¤ìŒ ë‹¨ì–´ ë¶„ì„
                            article_patterns = re.findall(r'\b(a|an|the)\s+(\w+)', selected_text.lower())
                            if article_patterns:
                                st.write("**ê´€ì‚¬ + ëª…ì‚¬ íŒ¨í„´:**")
                                for article, noun in article_patterns[:5]:
                                    st.write(f"â€¢ {article} {noun}")
                                    
                        elif selected_pattern == "ë°˜ë³µ ë‹¨ì–´ ë¹ˆë„":
                            # ë‹¨ì–´ ë¹ˆë„ ë¶„ì„
                            words = re.findall(r'\b[a-zA-Z]+\b', selected_text.lower())
                            word_count = Counter(words)
                            most_common = word_count.most_common(5)
                            
                            st.write("**ìƒìœ„ 5ê°œ ë¹ˆì¶œ ë‹¨ì–´:**")
                            for i, (word, count) in enumerate(most_common, 1):
                                st.write(f"{i}. **{word}** ({count}íšŒ)")
                                
                            # ë°˜ë³µë„ ë¶„ì„
                            total_words = len(words)
                            unique_words = len(set(words))
                            repetition_rate = (total_words - unique_words) / total_words * 100 if total_words > 0 else 0
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ì´ ë‹¨ì–´ ìˆ˜", total_words)
                            with col2:
                                st.metric("ê³ ìœ  ë‹¨ì–´ ìˆ˜", unique_words)
                            with col3:
                                st.metric("ë°˜ë³µë¥ ", f"{repetition_rate:.1f}%")
                    
                    st.info("""
                    ğŸ’¡ **ì´ ì²´í—˜ì˜ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë°©ë²•:**
                    - **ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­**: íŠ¹ì • ë¬¸ë²• ì˜¤ë¥˜ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ íƒì§€
                    - **ë‹¨ì–´ ë¹ˆë„ ë¶„ì„**: Counterë¥¼ ì‚¬ìš©í•œ ë‹¨ì–´ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
                    - **íŒ¨í„´ ë¶„ë¥˜**: ì‹œì œë³„, ê´€ì‚¬ë³„ í‘œí˜„ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¥˜ ë° ì§‘ê³„
                    - **í†µê³„ì  ë¶„ì„**: ì–´íœ˜ ë‹¤ì–‘ì„±, ë°˜ë³µë¥  ë“± ìˆ˜ì¹˜ì  ì§€í‘œ ì‚°ì¶œ
                    
                    **ì‹¤ì œ í…ìŠ¤íŠ¸ ë§ˆì´ë‹**: ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ ì–¸ì–´ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ë¶„ë¥˜í•˜ëŠ” ìë™í™” ê¸°ìˆ 
                    """)
                    
                    st.markdown("---")
                    
                    # 4ë‹¨ê³„: ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ (ê¸°ì¡´ 3ë‹¨ê³„ì—ì„œ ë³€ê²½)
                    st.markdown("## ğŸ”— 4ë‹¨ê³„: ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ - ë…¼ë¦¬ì  ì—°ê²°ì„± í‰ê°€")
                    st.markdown("""
                    **ğŸ” ì›ë¦¬ ì„¤ëª…:**
                    - **ë¬¸ì¥ ê°„ ì—°ê²°ì„±**: ì¸ì ‘ ë¬¸ì¥ë“¤ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¸¡ì •
                    - **ì£¼ì œ ì¼ê´€ì„±**: ê¸€ ì „ì²´ì˜ ì£¼ì œ ì§‘ì¤‘ë„ì™€ ì´íƒˆ ì •ë„ ë¶„ì„
                    - **ë…¼ë¦¬ì  íë¦„**: ì—°ê²°ì–´ì™€ ì „í™˜ í‘œí˜„ì„ í†µí•œ ë…¼ë¦¬ì  êµ¬ì¡° í‰ê°€
                    """)
                    
                    # í•™ìƒ í†µí•© ì—ì„¸ì´ ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼
                    st.markdown("### ğŸ”— í•™ìƒ í†µí•© ì—ì„¸ì´ ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼")
                    
                    if 'step4_similarity' not in result:
                        with st.spinner("ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ ì¤‘..."):
                            result['step4_similarity'] = preprocessor.analyze_sentence_similarity(essay_data['combined_text'])
                    
                    similarity_analysis = result.get('step4_similarity', {})
                    if similarity_analysis:
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            avg_similarity = similarity_analysis.get('average_similarity', 0)
                            st.metric("í‰ê·  ë¬¸ì¥ ìœ ì‚¬ë„", f"{avg_similarity:.3f}")
                            if avg_similarity > 0.6:
                                st.caption("ğŸŸ¢ ë†’ì€ ì—°ê²°ì„±")
                            elif avg_similarity > 0.3:
                                st.caption("ğŸŸ¡ ë³´í†µ ì—°ê²°ì„±")
                            else:
                                st.caption("ğŸŸ  ë‚®ì€ ì—°ê²°ì„±")
                        
                        with col2:
                            coherence_score = similarity_analysis.get('coherence_score', 0)
                            st.metric("ê¸€ ì¼ê´€ì„± ì ìˆ˜", f"{coherence_score:.1f}/100")
                        
                        with col3:
                            logical_flow = similarity_analysis.get('logical_flow_level', 'Unknown')
                            st.metric("ë…¼ë¦¬ì  íë¦„", logical_flow)
                        
                        # ë¬¸ì¥ë³„ ì—°ê²°ì„± ë¶„ì„
                        sentence_pairs = similarity_analysis.get('sentence_pair_analysis', [])
                        if sentence_pairs:
                            st.markdown("### ğŸ“ ë¬¸ì¥ ê°„ ì—°ê²°ì„± ë¶„ì„ (ìƒìœ„ 5ê°œ ìŒ)")
                            for i, pair in enumerate(sentence_pairs[:5], 1):
                                similarity = pair.get('similarity', 0)
                                sent1_preview = pair.get('sentence1_preview', 'ë¬¸ì¥ 1')[:50] + "..."
                                sent2_preview = pair.get('sentence2_preview', 'ë¬¸ì¥ 2')[:50] + "..."
                                
                                if similarity > 0.5:
                                    icon = "ğŸŸ¢"
                                elif similarity > 0.3:
                                    icon = "ğŸŸ¡"
                                else:
                                    icon = "ğŸŸ "
                                
                                st.write(f"{i}. {icon} **ìœ ì‚¬ë„ {similarity:.3f}**")
                                st.write(f"   â€¢ {sent1_preview}")
                                st.write(f"   â€¢ {sent2_preview}")
                        
                        # ì£¼ì œ ì¼ê´€ì„± ë¶„ì„
                        topic_consistency = similarity_analysis.get('topic_consistency', {})
                        if topic_consistency:
                            st.markdown("### ğŸ“‹ ì£¼ì œ ì¼ê´€ì„± ë¶„ì„")
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                topic_drift_score = topic_consistency.get('topic_drift_score', 0)
                                st.metric("ì£¼ì œ ì´íƒˆë„", f"{topic_drift_score:.1f}%")
                                
                            with col2:
                                main_theme_strength = topic_consistency.get('main_theme_strength', 0)
                                st.metric("ì£¼ì œ ì§‘ì¤‘ë„", f"{main_theme_strength:.1f}%")
                    else:
                        st.warning("í†µí•© ì—ì„¸ì´ ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ì²´í—˜í˜• ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ - 3ë‹¨ê³„
                    st.markdown("**ğŸ¯ ì²´í—˜í•´ë³´ê¸°: ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¸¡ì •**")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        sentence1 = st.text_input("ì²« ë²ˆì§¸ ë¬¸ì¥:", 
                                                 value="Education plays a crucial role in personal development.",
                                                 key="similarity_sentence1")
                    with col2:
                        sentence2 = st.text_input("ë‘ ë²ˆì§¸ ë¬¸ì¥:", 
                                                 value="Learning is essential for individual growth and success.",
                                                 key="similarity_sentence2")
                    
                    if st.button("ğŸ” ë¬¸ì¥ ìœ ì‚¬ë„ ì¸¡ì •í•˜ê¸°", key="similarity_check") and sentence1.strip() and sentence2.strip():
                        with st.spinner("ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì¤‘..."):
                            # ë” ì •êµí•œ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¤ì¸µì  ì ‘ê·¼)
                            import re
                            
                            # 1. ì „ì²˜ë¦¬: ë‹¨ì–´ ì¶”ì¶œ ë° ì •ê·œí™”
                            words1 = re.findall(r'\b[a-zA-Z]+\b', sentence1.lower())
                            words2 = re.findall(r'\b[a-zA-Z]+\b', sentence2.lower())
                            
                            # 2. ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„
                            semantic_clusters = {
                                'education': ['education', 'learning', 'study', 'school', 'knowledge', 'teach', 'learn', 'academic', 'student'],
                                'development': ['development', 'growth', 'progress', 'improve', 'advance', 'evolve', 'enhance', 'success'],
                                'importance': ['crucial', 'important', 'essential', 'vital', 'significant', 'necessary', 'key', 'critical'],
                                'personal': ['personal', 'individual', 'self', 'own', 'private', 'person', 'people', 'human'],
                                'process': ['process', 'method', 'way', 'approach', 'system', 'procedure', 'technique', 'strategy']
                            }
                            
                            # 3. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
                            semantic_matches = {}
                            for cluster_name, cluster_words in semantic_clusters.items():
                                words1_in_cluster = [w for w in words1 if w in cluster_words]
                                words2_in_cluster = [w for w in words2 if w in cluster_words]
                                if words1_in_cluster or words2_in_cluster:
                                    semantic_matches[cluster_name] = {
                                        'sentence1': words1_in_cluster,
                                        'sentence2': words2_in_cluster,
                                        'match': bool(words1_in_cluster and words2_in_cluster)
                                    }
                            
                            # 4. êµ¬ì¡°ì  ìœ ì‚¬ì„± ë¶„ì„
                            structure_score = 0
                            # ë¬¸ì¥ ê¸¸ì´ ìœ ì‚¬ì„±
                            len_diff = abs(len(words1) - len(words2))
                            length_similarity = max(0, 1 - len_diff / max(len(words1), len(words2), 1))
                            structure_score += length_similarity * 30
                            
                            # í’ˆì‚¬ íŒ¨í„´ ìœ ì‚¬ì„± (ê°„ë‹¨ ì¶”ì •)
                            def estimate_pos_pattern(words):
                                pattern = []
                                for word in words:
                                    if word.endswith('ing') or word.endswith('ed'):
                                        pattern.append('V')  # ë™ì‚¬
                                    elif word.endswith('tion') or word.endswith('ment'):
                                        pattern.append('N')  # ëª…ì‚¬
                                    elif word.endswith('ly'):
                                        pattern.append('R')  # ë¶€ì‚¬
                                    elif word in ['a', 'an', 'the']:
                                        pattern.append('D')  # ê´€ì‚¬
                                    else:
                                        pattern.append('N')  # ê¸°ë³¸ ëª…ì‚¬
                                return pattern[:5]  # ì²« 5ê°œ íŒ¨í„´ë§Œ
                            
                            pattern1 = estimate_pos_pattern(words1)
                            pattern2 = estimate_pos_pattern(words2)
                            pattern_similarity = sum(1 for a, b in zip(pattern1, pattern2) if a == b) / max(len(pattern1), len(pattern2), 1)
                            structure_score += pattern_similarity * 20
                            
                            # 5. ì¢…í•© ìœ ì‚¬ë„ ì ìˆ˜
                            semantic_score = len([m for m in semantic_matches.values() if m['match']]) * 20
                            word_overlap_score = len(set(words1).intersection(set(words2))) / len(set(words1).union(set(words2))) * 30 if words1 or words2 else 0
                            
                            total_similarity = (semantic_score + structure_score + word_overlap_score) / 100
                            total_similarity = min(1.0, total_similarity)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
                            
                            # 6. ê²°ê³¼ í‘œì‹œ
                            st.write(f"**ë¬¸ì¥ 1:** {sentence1}")
                            st.write(f"**ë¬¸ì¥ 2:** {sentence2}")
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("ì˜ë¯¸ì  ìœ ì‚¬ë„", f"{total_similarity:.3f}")
                            with col2:
                                if total_similarity > 0.6:
                                    st.metric("ì—°ê²°ì„± íŒì •", "ğŸŸ¢ ê°•í•¨")
                                elif total_similarity > 0.3:
                                    st.metric("ì—°ê²°ì„± íŒì •", "ğŸŸ¡ ë³´í†µ")
                                else:
                                    st.metric("ì—°ê²°ì„± íŒì •", "ğŸŸ  ì•½í•¨")
                            with col3:
                                common_themes = len([m for m in semantic_matches.values() if m['match']])
                                st.metric("ê³µí†µ ì£¼ì œ", f"{common_themes}ê°œ")
                            
                            # 7. ìƒì„¸ ë¶„ì„ ê²°ê³¼
                            if semantic_matches:
                                st.markdown("**ğŸ¯ ì˜ë¯¸ì  ì—°ê²° ë¶„ì„:**")
                                for theme, data in semantic_matches.items():
                                    if data['match']:
                                        st.write(f"âœ… **{theme.capitalize()}** ì£¼ì œ ì—°ê²°: {', '.join(data['sentence1'])} â†” {', '.join(data['sentence2'])}")
                                    elif data['sentence1'] or data['sentence2']:
                                        words = data['sentence1'] or data['sentence2']
                                        st.write(f"âš ï¸ **{theme.capitalize()}** ì£¼ì œ (í•œìª½ë§Œ): {', '.join(words)}")
                            
                            # 8. êµ¬ì¡°ì  ë¶„ì„
                            st.markdown("**ğŸ“‹ êµ¬ì¡°ì  ë¶„ì„:**")
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write(f"â€¢ ê¸¸ì´ ìœ ì‚¬ì„±: {length_similarity:.2f}")
                                st.write(f"â€¢ íŒ¨í„´ ìœ ì‚¬ì„±: {pattern_similarity:.2f}")
                            with col2:
                                st.write(f"â€¢ ì–´íœ˜ ì¤‘ë³µë„: {word_overlap_score/30:.2f}")
                                st.write(f"â€¢ ì˜ë¯¸ í´ëŸ¬ìŠ¤í„°: {semantic_score/20:.0f}ê°œ ì¼ì¹˜")
                            
                            st.info("""
                            ğŸ’¡ **ì´ ì²´í—˜ì˜ ë¶„ì„ ë°©ë²•:**
                            - **ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°**: ê´€ë ¨ ë‹¨ì–´ë“¤ì„ ì£¼ì œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì˜ë¯¸ ì—°ê²°ì„± ì¸¡ì •
                            - **êµ¬ì¡°ì  ìœ ì‚¬ì„±**: ë¬¸ì¥ ê¸¸ì´, í’ˆì‚¬ íŒ¨í„´ ë“± êµ¬ì¡°ì  íŠ¹ì§• ë¹„êµ
                            - **ì¢…í•© ì ìˆ˜**: ì—¬ëŸ¬ ì°¨ì›ì˜ ìœ ì‚¬ì„±ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì ìˆ˜ ì‚°ì¶œ
                            
                            **ì‹¤ì œ AI ì‹œìŠ¤í…œ**: BERT, Sentence-BERT ë“±ì„ ì‚¬ìš©í•˜ì—¬ 768ì°¨ì› ë²¡í„° ê³µê°„ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                            """)
                    
                    st.markdown("---")
                    
                    # ì¢…í•© ì§„ë‹¨ ê²°ê³¼
                    comprehensive_results = result.get('step5_comprehensive', {})
                    if comprehensive_results:
                        st.success("ğŸŠ **ì¢…í•© ì§„ë‹¨ ì™„ë£Œ!**")
                        
                        # ì¢…í•© ì ìˆ˜ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            overall_score = comprehensive_results.get('overall_score', 0)
                            st.metric("ì¢…í•© ì ìˆ˜", f"{overall_score}/100")
                        
                        with col2:
                            final_level = comprehensive_results.get('final_level', 'Unknown')
                            st.metric("ë“±ê¸‰", final_level)
                        
                        with col3:
                            level_description = comprehensive_results.get('level_description', '')
                            st.write(f"**í‰ê°€:** {level_description}")
                        
                        # ìƒì„¸ ë¶„ì„ ê²°ê³¼
                        st.markdown("### ğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
                        
                        # ê°•ì ê³¼ ì•½ì  ë¶„ì„
                        strengths = comprehensive_results.get('strengths', [])
                        weaknesses = comprehensive_results.get('weaknesses', [])
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**ğŸŸ¢ ê°•ì  ë¶„ì„:**")
                            if strengths:
                                for strength in strengths[:3]:
                                    st.write(f"â€¢ {strength}")
                            else:
                                st.write("â€¢ ë¬¸ì¥ êµ¬ì„± ëŠ¥ë ¥")
                                st.write("â€¢ ê¸°ë³¸ì ì¸ ì–´íœ˜ ì‚¬ìš©")
                        
                        with col2:
                            st.markdown("**ğŸŸ  ê°œì„ ì :**")
                            if weaknesses:
                                for weakness in weaknesses[:3]:
                                    st.write(f"â€¢ {weakness}")
                            else:
                                st.write("â€¢ ì–´íœ˜ ë‹¤ì–‘ì„± í–¥ìƒ í•„ìš”")
                                st.write("â€¢ ë³µí•©ë¬¸ êµ¬ì¡° ì—°ìŠµ")
                                st.write("â€¢ ë…¼ë¦¬ì  ì—°ê²°ì„± ê°•í™”")
                        
                        # ë§ì¶¤í˜• í•™ìŠµ ê³„íš
                        st.markdown("### ğŸ¯ ë§ì¶¤í˜• í•™ìŠµ ê³„íš")
                        recommendations = comprehensive_results.get('recommendations', [])
                        
                        if recommendations:
                            for i, rec in enumerate(recommendations[:4], 1):
                                st.write(f"{i}. **{rec}**")
                        else:
                            st.write("1. **ì–´íœ˜ë ¥ í–¥ìƒ**: ë‹¤ì–‘í•œ ë™ì˜ì–´ì™€ ì „ë¬¸ ìš©ì–´ í•™ìŠµ")
                            st.write("2. **ë¬¸ì¥ êµ¬ì¡° ë‹¤ì–‘í™”**: ë³µí•©ë¬¸ê³¼ ë³µë¬¸ ì‚¬ìš© ì—°ìŠµ") 
                            st.write("3. **ë…¼ë¦¬ì  ì—°ê²°**: ì ‘ì†ì‚¬ë¥¼ í™œìš©í•œ ë¬¸ì¥ ê°„ ì—°ê²°ì„± ê°•í™”")
                            st.write("4. **ë…ì„œ í™•ëŒ€**: ë‹¤ì–‘í•œ ì¥ë¥´ì˜ ì˜ë¬¸ í…ìŠ¤íŠ¸ ì½ê¸°")
                    
                    # í•™ìƒ ì£¼ë„í˜• í•™ìŠµ ê³„íš ìƒì„± í™œë™
                    st.markdown("### ğŸ¯ ë‚˜ë§Œì˜ ë§ì¶¤í˜• í•™ìŠµ ê³„íš ë§Œë“¤ê¸°")
                    st.info("ğŸ’¡ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ ê³„íšì„ ì„¸ì›Œë³´ì„¸ìš”. ë‹¨ê³„ì  ì•ˆë‚´ë¥¼ í†µí•´ ë‚˜ë§Œì˜ ëª©í‘œë¥¼ ì°¾ì•„ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    # 1ë‹¨ê³„: ìê¸° ë°˜ì„± ë° í˜„ì¬ ìƒíƒœ íŒŒì•…
                    st.markdown("#### ğŸ“‹ 1ë‹¨ê³„: ë‚´ ê¸€ì“°ê¸° í˜„ì¬ ìƒíƒœ ë¶„ì„í•˜ê¸°")
                    
                    # ìê¸° ë°˜ì„± ì§ˆë¬¸
                    st.markdown("**ğŸ¤” ìŠ¤ìŠ¤ë¡œì—ê²Œ ë¬¼ì–´ë³´ê¸°:**")
                    reflection_questions = [
                        "ë‚´ ê¸€ì“°ê¸°ì—ì„œ ê°€ì¥ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                        "ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì–´ë ¤ì›€ì„ ëŠë¼ë‚˜ìš”?",
                        "ê¸€ì“°ê¸° ì‹¤ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ì‹¶ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
                    ]
                    
                    reflection_answers = {}
                    for i, question in enumerate(reflection_questions):
                        reflection_answers[f"q{i+1}"] = st.text_area(
                            question, 
                            placeholder="ì†”ì§í•˜ê²Œ ìƒê°í•´ë³´ê³  ì ì–´ì£¼ì„¸ìš”...",
                            key=f"reflection_{i+1}",
                            height=80
                        )
                    
                    # 2ë‹¨ê³„: ëª©í‘œ ì„¤ì •
                    st.markdown("#### ğŸ¯ 2ë‹¨ê³„: êµ¬ì²´ì ì¸ ëª©í‘œ ì •í•˜ê¸°")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**ìš°ì„ ìˆœìœ„ ì •í•˜ê¸°:**")
                        priority_areas = st.multiselect(
                            "ê°€ì¥ ì§‘ì¤‘í•˜ê³  ì‹¶ì€ ì˜ì—­ì„ 1-2ê°œ ì„ íƒí•˜ì„¸ìš”:",
                            ["ì–´íœ˜ ë‹¤ì–‘ì„± ëŠ˜ë¦¬ê¸°", "ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©í•˜ê¸°", "ë¬¸ì¥ ì—°ê²°ì„± ê°•í™”í•˜ê¸°", 
                             "ë¬¸ë²• ì •í™•ì„± ë†’ì´ê¸°", "ì°½ì˜ì  í‘œí˜„ ëŠ˜ë¦¬ê¸°", "ë…¼ë¦¬ì  êµ¬ì¡° ë§Œë“¤ê¸°"],
                            key="priority_areas"
                        )
                    
                    with col2:
                        st.markdown("**í•™ìŠµ ìŠ¤íƒ€ì¼:**")
                        learning_style = st.radio(
                            "ë‚˜ì—ê²Œ ë§ëŠ” í•™ìŠµ ë°©ì‹ì€:",
                            ["ë§¤ì¼ ì¡°ê¸ˆì”© ê¾¸ì¤€íˆ", "ì£¼ë§ì— ì§‘ì¤‘ì ìœ¼ë¡œ", "ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜", "í˜¼ì ì°¨ê·¼ì°¨ê·¼"],
                            key="learning_style"
                        )
                    
                    # ëª©í‘œ êµ¬ì²´í™”
                    target_period = st.selectbox(
                        "ì–¸ì œê¹Œì§€ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê³  ì‹¶ë‚˜ìš”?",
                        ["2ì£¼ í›„", "4ì£¼ í›„", "8ì£¼ í›„", "í•™ê¸°ë§ê¹Œì§€"],
                        key="target_period"
                    )
                    
                    success_measure = st.text_input(
                        "ëª©í‘œ ë‹¬ì„±ì„ ì–´ë–»ê²Œ í™•ì¸í•  ê±´ê°€ìš”? (ì˜ˆ: ì—ì„¸ì´ ì ìˆ˜ 10ì  í–¥ìƒ, ìƒˆë¡œìš´ ë‹¨ì–´ 100ê°œ ì‚¬ìš©)",
                        key="success_measure",
                        placeholder="êµ¬ì²´ì ì¸ ì„±ê³µ ê¸°ì¤€ì„ ì ì–´ë³´ì„¸ìš”..."
                    )
                    
                    # 3ë‹¨ê³„: ì‹¤í–‰ ê³„íš ë§Œë“¤ê¸°
                    if priority_areas and st.button("ğŸ“ ë‚˜ë§Œì˜ í•™ìŠµ ê³„íš ì™„ì„±í•˜ê¸°", key="generate_personal_plan"):
                        st.markdown("#### âœ¨ 3ë‹¨ê³„: ì™„ì„±ëœ ë‚˜ë§Œì˜ í•™ìŠµ ê³„íš")
                        
                        # ë²„íŠ¼ í´ë¦­ ì‹œì—ë§Œ í’ì„  í‘œì‹œ (ì„¸ì…˜ë³„ ê³ ìœ  í‚¤ ì‚¬ìš©)
                        balloon_key = f"learning_plan_balloons_{hash(str(priority_areas) + target_period + learning_style)}"
                        if balloon_key not in st.session_state:
                            st.balloons()
                            st.session_state[balloon_key] = True
                        
                        # í•™ìƒì´ ì„ íƒí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì¸í™”ëœ ê³„íš ì œì‹œ
                        st.success("ğŸŠ ì¶•í•˜í•©ë‹ˆë‹¤! ë‚˜ë§Œì˜ ë§ì¶¤í˜• í•™ìŠµ ê³„íšì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        
                        plan_container = st.container()
                        with plan_container:
                            st.markdown("---")
                            st.markdown(f"### ğŸ“‹ {st.session_state.get('user_name', 'ë‚˜')}ì˜ ê¸€ì“°ê¸° í–¥ìƒ ê³„íš")
                            
                            col1, col2 = st.columns([2, 1])
                            with col1:
                                st.write(f"**ğŸ¯ ì£¼ìš” ëª©í‘œ:** {', '.join(priority_areas)}")
                                st.write(f"**â° ëª©í‘œ ë‹¬ì„± ê¸°í•œ:** {target_period}")
                                st.write(f"**ğŸ“ ì„±ê³µ ê¸°ì¤€:** {success_measure if success_measure else 'ê¾¸ì¤€í•œ ì—°ìŠµê³¼ ìê¸° í‰ê°€'}")
                            with col2:
                                st.write(f"**ğŸ“š í•™ìŠµ ë°©ì‹:** {learning_style}")
                                st.write(f"**ğŸ’ª ì§‘ì¤‘ ì˜ì—­:** {len(priority_areas)}ê°œ ë¶„ì•¼")
                            
                            # êµ¬ì²´ì  ì‹¤í–‰ ë°©ë²• ì œì•ˆ (ì„ íƒí•œ ì˜ì—­ë³„ë¡œ)
                            st.markdown("### ğŸ“ ì£¼ê°„ë³„ ì‹¤í–‰ ê³„íš")
                            
                            # ì¤‘í•™êµ 1í•™ë…„ ìˆ˜ì¤€ ë§ì¶¤ í•™ìŠµ ê³„íš
                            action_plans = {
                                "ì–´íœ˜ ë‹¤ì–‘ì„± ëŠ˜ë¦¬ê¸°": {
                                    "ì£¼ê°„ ëª©í‘œ": "ìƒˆë¡œìš´ ì˜ì–´ ë‹¨ì–´ 15ê°œ í™•ì‹¤íˆ ì™¸ìš°ê³  ì‚¬ìš©í•´ë³´ê¸°",
                                    "ì¼ì¼ í™œë™": "ğŸ“š **ë§¤ì¼ 10ë¶„ ì–´íœ˜ í•™ìŠµ**\nâ€¢ EBS ì¤‘í•™ì˜ì–´ë‚˜ ëŠ¥ë¥ êµìœ¡ ì‚¬ì´íŠ¸ì—ì„œ ì¤‘1 ìˆ˜ì¤€ ì˜ì–´ë™í™” 1ê°œ ì½ê¸°\nâ€¢ ëª¨ë¥´ëŠ” ë‹¨ì–´ 3ê°œë¥¼ ê³µì±…ì— í¬ê²Œ ì¨ì„œ ëœ»ê³¼ ë°œìŒê¸°í˜¸ ì ê¸°\nâ€¢ ìƒˆë¡œ ë°°ìš´ ë‹¨ì–´ë¡œ 'ë‚˜ëŠ” ___ì´ë‹¤' í˜•ì‹ì˜ ì‰¬ìš´ ë¬¸ì¥ ë§Œë“¤ê¸°\nâ€¢ ìŠ¤ë§ˆíŠ¸í° ë‹¨ì–´ì¥ ì•±(ì˜ˆ: ë„¤ì´ë²„ì‚¬ì „)ì— ìƒˆ ë‹¨ì–´ ì €ì¥í•˜ê³  ì†Œë¦¬ë‚´ì–´ ì½ê¸°",
                                    "ì²´í¬í¬ì¸íŠ¸": "ğŸ¯ **ì£¼ê°„ í™•ì¸ í™œë™**\nâ€¢ ìˆ˜ìš”ì¼: ì´ë²ˆ ì£¼ ë°°ìš´ ë‹¨ì–´ 10ê°œë¡œ ë‚´ ì¼ìƒ ì†Œê°œí•˜ëŠ” 5ë¬¸ì¥ ë§Œë“¤ê¸°\nâ€¢ ê¸ˆìš”ì¼: ë¶€ëª¨ë‹˜/í˜•ì œì—ê²Œ ìƒˆë¡œ ë°°ìš´ ë‹¨ì–´ 3ê°œ ì˜ì–´ë¡œ ì„¤ëª…í•´ë“œë¦¬ê¸°\nâ€¢ ì¼ìš”ì¼: ì§€ë‚œ ì£¼ ë‹¨ì–´ 5ê°œë¥¼ ì¹œêµ¬ì—ê²Œ í€´ì¦ˆë¡œ ë‚´ê³  ë§ì¶”ê¸° ê²Œì„"
                                },
                                "ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©í•˜ê¸°": {
                                    "ì£¼ê°„ ëª©í‘œ": "simpleí•œ ë‹¨ì–´ë¥¼ ì¢€ ë” ë©‹ì§„ ë‹¨ì–´ë¡œ ë°”ê¿” ì“°ê¸°",
                                    "ì¼ì¼ í™œë™": "âœ¨ **ë§¤ì¼ 10ë¶„ ë‹¨ì–´ ì—…ê·¸ë ˆì´ë“œ**\nâ€¢ ë‚´ê°€ ì“´ ì˜ì–´ ì¼ê¸°ë‚˜ ìˆ™ì œì—ì„œ easy, good, bad ê°™ì€ ì‰¬ìš´ ë‹¨ì–´ ì°¾ê¸°\nâ€¢ ì¤‘í•™ì˜ì–´ êµê³¼ì„œ ë’·í¸ ë‹¨ì–´ì¥ì—ì„œ ë¹„ìŠ·í•œ ëœ»ì˜ ì–´ë ¤ìš´ ë‹¨ì–´ ì°¾ê¸°\nâ€¢ ë„¤ì´ë²„ ì˜ì–´ì‚¬ì „ì—ì„œ ì˜ˆë¬¸ 2ê°œ ì½ê³  ë”°ë¼ ì¨ë³´ê¸°\nâ€¢ ë°”ë€ ë¬¸ì¥ì„ í° ì†Œë¦¬ë¡œ ì½ìœ¼ë©´ì„œ ì–´ìƒ‰í•œ ê³³ ì—†ëŠ”ì§€ í™•ì¸í•˜ê¸°",
                                    "ì²´í¬í¬ì¸íŠ¸": "ğŸ¯ **ì£¼ê°„ í™•ì¸ í™œë™**\nâ€¢ í™”ìš”ì¼: 'My hobby is...' ë¬¸ì¥ì„ basic ë‹¨ì–´ì™€ advanced ë‹¨ì–´ë¡œ ê°ê° ì¨ë³´ê¸°\nâ€¢ ê¸ˆìš”ì¼: ì˜ì–´ì„ ìƒë‹˜ê»˜ ë‚´ê°€ ë°”ê¾¼ ë¬¸ì¥ì´ ìì—°ìŠ¤ëŸ¬ìš´ì§€ ë¬¼ì–´ë³´ê¸°\nâ€¢ ì¼ìš”ì¼: ì´ë²ˆ ì£¼ ë°°ìš´ ë©‹ì§„ ë‹¨ì–´ 10ê°œë¡œ ë‹¨ì–´ì¹´ë“œ ë§Œë“¤ì–´ ë²½ì— ë¶™ì´ê¸°"
                                },
                                "ë¬¸ì¥ ì—°ê²°ì„± ê°•í™”í•˜ê¸°": {
                                    "ì£¼ê°„ ëª©í‘œ": "ì§§ì€ ë¬¸ì¥ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ê¸´ ë¬¸ì¥ ë§Œë“¤ê¸°",
                                    "ì¼ì¼ í™œë™": "ğŸ”— **ë§¤ì¼ 10ë¶„ ë¬¸ì¥ ì—°ê²° ì—°ìŠµ**\nâ€¢ and, but, so, because ê°™ì€ ì—°ê²°ì–´ í•˜ë‚˜ì”© ì§‘ì¤‘ í•™ìŠµ\nâ€¢ ì¤‘í•™êµ ì˜ì–´êµê³¼ì„œì—ì„œ ì—°ê²°ì–´ê°€ ë“¤ì–´ê°„ ë¬¸ì¥ 5ê°œ ì°¾ì•„ ê³µì±…ì— ì“°ê¸°\nâ€¢ 'I like pizza.' + 'Pizza is delicious.' â†’ 'I like pizza because it is delicious.' ì—°ìŠµ\nâ€¢ ë§Œë“  ë¬¸ì¥ì„ ë…¹ìŒê¸°ë¡œ ë…¹ìŒí•´ì„œ ë“¤ì–´ë³´ê³  ìì—°ìŠ¤ëŸ¬ìš´ì§€ í™•ì¸",
                                    "ì²´í¬í¬ì¸íŠ¸": "ğŸ¯ **ì£¼ê°„ í™•ì¸ í™œë™**\nâ€¢ ëª©ìš”ì¼: ë‚´ í•˜ë£¨ ì¼ê³¼ë¥¼ 5ê°œ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ì“´ ë‹¤ìŒ ì—°ê²°ì–´ë¡œ 3ë¬¸ì¥ìœ¼ë¡œ ì¤„ì´ê¸°\nâ€¢ í† ìš”ì¼: ì¹œêµ¬ì™€ ì„œë¡œ ì“´ ë¬¸ì¥ ì½ì–´ì£¼ê³  ì´ìƒí•œ ê³³ ì°¾ì•„ì£¼ê¸°\nâ€¢ ì¼ìš”ì¼: ì¢‹ì•„í•˜ëŠ” ì—°ì˜ˆì¸/ìš´ë™ì„ ìˆ˜ ì†Œê°œë¥¼ ì—°ê²°ì–´ ì‚¬ìš©í•´ì„œ 100ìë¡œ ì“°ê¸°"
                                },
                                "ë¬¸ë²• ì •í™•ì„± ë†’ì´ê¸°": {
                                    "ì£¼ê°„ ëª©í‘œ": "beë™ì‚¬, ì¼ë°˜ë™ì‚¬, ë³µìˆ˜í˜• ì‹¤ìˆ˜ ì—†ì´ ì“°ê¸°",
                                    "ì¼ì¼ í™œë™": "ğŸ“ **ë§¤ì¼ 10ë¶„ ë¬¸ë²• ì ê²€**\nâ€¢ EBS ì¤‘í•™ì˜ì–´ ë˜ëŠ” 'ë¬¸ë²•ì´ ì‰¬ì›Œì§€ëŠ” ì˜ì–´' ì•±ì—ì„œ ì˜¤ëŠ˜ì˜ ë¬¸ë²• 1ê°œ í•™ìŠµ\nâ€¢ í•™ìŠµí•œ ë¬¸ë²•ìœ¼ë¡œ ë‚´ ê²½í—˜ ë¬¸ì¥ 3ê°œ ë§Œë“¤ê¸° (ì˜ˆ: am/is/are êµ¬ë¶„í•´ì„œ)\nâ€¢ ì–´ì œ ì“´ ì˜ì–´ ë¬¸ì¥ì—ì„œ í‹€ë¦° ë¶€ë¶„ ë¹¨ê°„íœìœ¼ë¡œ ê³ ì¹˜ê¸°\nâ€¢ ê³ ì¹œ ë¶€ë¶„ì„ ë…¸íŠ¸ì— 'ìì£¼ í‹€ë¦¬ëŠ” ì‹¤ìˆ˜' ëª©ë¡ìœ¼ë¡œ ë§Œë“¤ê¸°",
                                    "ì²´í¬í¬ì¸íŠ¸": "ğŸ¯ **ì£¼ê°„ í™•ì¸ í™œë™**\nâ€¢ ìˆ˜ìš”ì¼: ë‚´ ì†Œê°œ 5ë¬¸ì¥ ì“°ê³  beë™ì‚¬/ì¼ë°˜ë™ì‚¬ ìƒ‰ê¹”íœìœ¼ë¡œ êµ¬ë¶„í•˜ê¸°\nâ€¢ ê¸ˆìš”ì¼: ì§ê¿ê³¼ ì„œë¡œ ì“´ ë¬¸ì¥ ë¬¸ë²• ê²€ì‚¬í•´ì£¼ê³  í‹€ë¦° ê³³ ì„¤ëª…í•´ì£¼ê¸°\nâ€¢ ì¼ìš”ì¼: ì´ë²ˆ ì£¼ ê°€ì¥ ë§ì´ í‹€ë¦° ë¬¸ë²• 1ê°œ ê³¨ë¼ì„œ ì˜ˆë¬¸ 10ê°œ ë§Œë“¤ê¸°"
                                },
                                "ì°½ì˜ì  í‘œí˜„ ëŠ˜ë¦¬ê¸°": {
                                    "ì£¼ê°„ ëª©í‘œ": "ë”±ë”±í•œ ë¬¸ì¥ ëŒ€ì‹  ì¬ë¯¸ìˆê³  ìƒë™ê° ìˆëŠ” í‘œí˜„ ì“°ê¸°",
                                    "ì¼ì¼ í™œë™": "ğŸ¨ **ë§¤ì¼ 10ë¶„ í‘œí˜„ ì—°ìŠµ**\nâ€¢ ë””ì¦ˆë‹ˆ ì˜í™”ë‚˜ ìŠ¤íŠœë””ì˜¤ ì§€ë¸Œë¦¬ ì˜í™” ëª…ëŒ€ì‚¬ 1ê°œ ì°¾ì•„ ë”°ë¼ ì“°ê¸°\nâ€¢ 'ë¹„ê°€ ì˜¨ë‹¤' ëŒ€ì‹  'Rain is dancing'ì²˜ëŸ¼ ì˜ì¸ë²•ìœ¼ë¡œ í‘œí˜„í•´ë³´ê¸°\nâ€¢ ê°ì •ì„ ìƒ‰ê¹”ë¡œ í‘œí˜„í•˜ê¸° (I'm feeling blue = ìŠ¬í”„ë‹¤)\nâ€¢ ë‚´ê°€ ë§Œë“  ì°½ì˜ì  ë¬¸ì¥ì„ ê°€ì¡±ì—ê²Œ ì½ì–´ì£¼ê³  ë°˜ì‘ ë³´ê¸°",
                                    "ì²´í¬í¬ì¸íŠ¸": "ğŸ¯ **ì£¼ê°„ í™•ì¸ í™œë™**\nâ€¢ í™”ìš”ì¼: 'í•™êµì— ê°„ë‹¤'ë¥¼ 3ê°€ì§€ ë‹¤ë¥¸ ì¬ë¯¸ìˆëŠ” ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê¸°\nâ€¢ ê¸ˆìš”ì¼: ì¢‹ì•„í•˜ëŠ” ìŒì‹ì„ ì˜ì¸ë²•ì´ë‚˜ ë¹„ìœ ë²•ìœ¼ë¡œ ì†Œê°œí•˜ëŠ” ë¬¸ì¥ ì“°ê¸°\nâ€¢ ì¼ìš”ì¼: ì´ë²ˆ ì£¼ ë‚ ì”¨ë¥¼ ì°½ì˜ì  í‘œí˜„ìœ¼ë¡œ ì¼ê¸° 3ì¤„ ì“°ê¸°"
                                },
                                "ë…¼ë¦¬ì  êµ¬ì¡° ë§Œë“¤ê¸°": {
                                    "ì£¼ê°„ ëª©í‘œ": "First, Next, Finally ìˆœì„œë¡œ ì°¨ê·¼ì°¨ê·¼ ì„¤ëª…í•˜ëŠ” ê¸€ì“°ê¸°",
                                    "ì¼ì¼ í™œë™": "ğŸ“Š **ë§¤ì¼ 10ë¶„ ìˆœì„œ ì •ë¦¬ ì—°ìŠµ**\nâ€¢ ë¼ë©´ ë“ì´ëŠ” ë°©ë²•, í•™êµ ê°€ëŠ” ê¸¸ ë“± ì¼ìƒì ì¸ ìˆœì„œë¥¼ First, Next, Finallyë¡œ ì“°ê¸°\nâ€¢ ì¤‘í•™êµ ì˜ì–´êµê³¼ì„œ reading ë¶€ë¶„ì—ì„œ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ ì°¾ê¸°\nâ€¢ ë‚´ ì˜ê²¬ì„ I think â†’ because â†’ For example ìˆœì„œë¡œ ì„¤ëª…í•˜ëŠ” ì—°ìŠµ\nâ€¢ ì“´ ê¸€ì„ ì†Œë¦¬ë‚´ì–´ ì½ìœ¼ë©° ìˆœì„œê°€ ë…¼ë¦¬ì ì¸ì§€ í™•ì¸í•˜ê¸°",
                                    "ì²´í¬í¬ì¸íŠ¸": "ğŸ¯ **ì£¼ê°„ í™•ì¸ í™œë™**\nâ€¢ ìˆ˜ìš”ì¼: ì¢‹ì•„í•˜ëŠ” ê²Œì„/ì·¨ë¯¸ ë°©ë²•ì„ ìˆœì„œëŒ€ë¡œ ì˜ì–´ 5ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ê¸°\nâ€¢ í† ìš”ì¼: ì¹œêµ¬ì—ê²Œ ë‚´ ì„¤ëª…ì„ ì½ì–´ì£¼ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ì§€ ë¬¼ì–´ë³´ê¸°\nâ€¢ ì¼ìš”ì¼: ìš°ë¦¬ í•™êµ ìë‘ê±°ë¦¬ë¥¼ First, Second, Thirdë¡œ ë‚˜ëˆ ì„œ ì†Œê°œí•˜ê¸°"
                                }
                            }
                            
                            weeks_mapping = {"2ì£¼ í›„": 2, "4ì£¼ í›„": 4, "8ì£¼ í›„": 8, "í•™ê¸°ë§ê¹Œì§€": 12}
                            weeks = weeks_mapping.get(target_period, 4)
                            
                            for i, area in enumerate(priority_areas, 1):
                                if area in action_plans:
                                    plan = action_plans[area]
                                    with st.expander(f"ğŸ“Œ ì˜ì—­ {i}: {area}", expanded=True):
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            st.write(f"**ì£¼ê°„ ëª©í‘œ:**")
                                            st.write(plan["ì£¼ê°„ ëª©í‘œ"])
                                        with col2:
                                            st.write(f"**ì¼ì¼ í™œë™:**")
                                            st.write(plan["ì¼ì¼ í™œë™"])
                                        with col3:
                                            st.write(f"**ì²´í¬í¬ì¸íŠ¸:**")
                                            st.write(plan["ì²´í¬í¬ì¸íŠ¸"])
                            
                            # í•™ìŠµ ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ë§ì¶¤ ì¡°ì–¸
                            st.markdown("### ğŸ’¡ ë‚˜ë§Œì˜ í•™ìŠµ ì „ëµ")
                            style_advice = {
                                "ë§¤ì¼ ì¡°ê¸ˆì”© ê¾¸ì¤€íˆ": "â° ë§¤ì¼ 15ë¶„ì”© ì •í•´ì§„ ì‹œê°„ì— í•™ìŠµí•˜ê³ , ìŠ¤ë§ˆíŠ¸í° ì•Œë¦¼ ì„¤ì •í•˜ê¸°",
                                "ì£¼ë§ì— ì§‘ì¤‘ì ìœ¼ë¡œ": "ğŸ“… í† ìš”ì¼/ì¼ìš”ì¼ 2ì‹œê°„ì”© ì§‘ì¤‘ í•™ìŠµ ì‹œê°„ í™•ë³´í•˜ê³ , ì£¼ì¤‘ì—” ë³µìŠµë§Œ",
                                "ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜": "ğŸ‘¥ ìŠ¤í„°ë”” ê·¸ë£¹ ë§Œë“¤ì–´ ì„œë¡œ ê¸€ ê²€í† í•´ì£¼ê³ , ì˜ì–´ ëŒ€í™” ì—°ìŠµí•˜ê¸°",
                                "í˜¼ì ì°¨ê·¼ì°¨ê·¼": "ğŸ“š ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ì§‘ì¤‘í•˜ê³ , í•™ìŠµ ì¼ì§€ ì‘ì„±í•´ ìŠ¤ìŠ¤ë¡œ ì ê²€í•˜ê¸°"
                            }
                            
                            st.info(f"**{learning_style} ìŠ¤íƒ€ì¼ ë§ì¶¤ ì¡°ì–¸:** {style_advice.get(learning_style, 'ìì‹ ë§Œì˜ í˜ì´ìŠ¤ë¡œ ê¾¸ì¤€íˆ í•™ìŠµí•˜ì„¸ìš”!')}")
                            
                            # ë™ê¸°ë¶€ì—¬ ë° ì ê²€ ì‹œìŠ¤í…œ
                            st.markdown("### ğŸ† ì„±ê³µì„ ìœ„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸")
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.markdown("**ğŸ“ˆ ì£¼ê°„ ì ê²€ ì§ˆë¬¸:**")
                                st.write("â–¡ ì´ë²ˆ ì£¼ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆë‚˜ìš”?")
                                st.write("â–¡ ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ ì–´ë ¤ì› ë‚˜ìš”?")
                                st.write("â–¡ ì‹¤ì œë¡œ ì‹¤ë ¥ì´ ëŠ˜ì—ˆë‹¤ê³  ëŠë¼ë‚˜ìš”?")
                                st.write("â–¡ ë‹¤ìŒ ì£¼ ê³„íšì„ ì¡°ì •í•  ë¶€ë¶„ì´ ìˆë‚˜ìš”?")
                            
                            with col2:
                                st.markdown("**ğŸ¯ ë™ê¸°ë¶€ì—¬ ë°©ë²•:**")
                                st.write("â€¢ ì‘ì€ ì„±ì·¨ë„ ê¸°ë¡í•˜ê³  ì¶•í•˜í•˜ê¸°")
                                st.write("â€¢ í•™ìŠµ ì „í›„ ê¸€ì“°ê¸° ë¹„êµí•´ë³´ê¸°")
                                st.write("â€¢ í˜ë“¤ ë•Œ í•™ìŠµ ëª©ì  ë‹¤ì‹œ ë– ì˜¬ë¦¬ê¸°")
                                st.write(f"â€¢ {target_period} í›„ ì„±ì·¨ê° ë¯¸ë¦¬ ìƒìƒí•˜ê¸°")
                            
                            # ì‘ê¸‰ì²˜ì¹˜ ê³„íš
                            st.markdown("### ğŸ†˜ í•™ìŠµì´ ì–´ë ¤ìš¸ ë•Œ ëŒ€ì²˜ë²•")
                            emergency_tips = [
                                "**ë™ê¸°ê°€ ë–¨ì–´ì§ˆ ë•Œ**: ì²˜ìŒì— ì„¸ìš´ ëª©í‘œì™€ ì´ìœ ë¥¼ ë‹¤ì‹œ ì½ì–´ë³´ê¸°",
                                "**ì‹œê°„ì´ ë¶€ì¡±í•  ë•Œ**: í•˜ë£¨ 5ë¶„ì´ë¼ë„ ì˜ì–´ ë¬¸ì¥ í•˜ë‚˜ ì½ê³  ë¶„ì„í•˜ê¸°",
                                "**ë„ˆë¬´ ì–´ë ¤ìš¸ ë•Œ**: ëª©í‘œë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë‹¬ì„± ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¡°ì •í•˜ê¸°",
                                "**í˜¼ì í•˜ê¸° í˜ë“¤ ë•Œ**: ì„ ìƒë‹˜ê»˜ ì¡°ì–¸ êµ¬í•˜ê±°ë‚˜ ì˜¨ë¼ì¸ í•™ìŠµ ì»¤ë®¤ë‹ˆí‹° í™œìš©í•˜ê¸°"
                            ]
                            
                            for tip in emergency_tips:
                                st.write(f"â€¢ {tip}")
                            
                            st.markdown("---")
                            st.success("ğŸŒŸ **ê¸°ì–µí•˜ì„¸ìš”**: ì™„ë²½í•œ ê³„íšë³´ë‹¤ ê¾¸ì¤€í•œ ì‹¤í–‰ì´ ë” ì¤‘ìš”í•©ë‹ˆë‹¤. ë‚˜ë§Œì˜ ì†ë„ë¡œ ì°¨ê·¼ì°¨ê·¼ ì„±ì¥í•´ë‚˜ê°€ì„¸ìš”!")
                    
                else:
                    st.error("ì¢…í•© ê¸€ì“°ê¸° ì§„ë‹¨ì— í•„ìš”í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"âŒ ì¢…í•© ê¸€ì“°ê¸° ì§„ë‹¨ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
        else:
            st.warning("ğŸ“š ë¨¼ì € 'ì—ì„¸ì´ ìˆ˜ì§‘' íƒ­ì—ì„œ ì—ì„¸ì´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì£¼ì„¸ìš”.")
    
    # ì™„ë£Œ ì•ˆë‚´
    if 'educational_sentiment_results' in st.session_state or 'educational_pos_results' in st.session_state:
        st.markdown("---")
        st.info("ğŸŠ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì›ë¦¬ ì²´í—˜ì„ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤! ê° ë¶„ì„ì€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
.main-header {
    text-align: center;
    color: #2E86AB;
    margin-bottom: 1rem;
}
</style>
""", unsafe_allow_html=True)

# ë©”ì¸ ì‹¤í–‰
def main():
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'logged_in' not in st.session_state:
        st.session_state.logged_in = False
    
    # ë¡œê·¸ì¸ ìƒíƒœì— ë”°ë¼ í˜ì´ì§€ í‘œì‹œ
    if st.session_state.logged_in:
        main_app()
    else:
        login_page()

if __name__ == "__main__":
    main()